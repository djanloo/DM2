{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:33.191372Z",
     "start_time": "2023-05-30T13:19:33.188437Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.dtypes.common import is_numeric_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import default_style\n",
    "\n",
    "# This is for reloading custom modules in case they are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:33.194825Z",
     "start_time": "2023-05-30T13:19:33.192067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocal_channel</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotional_intensity</th>\n",
       "      <th>statement</th>\n",
       "      <th>repetition</th>\n",
       "      <th>actor</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>1st</td>\n",
       "      <td>actor_1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>2nd</td>\n",
       "      <td>actor_1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>1st</td>\n",
       "      <td>actor_1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speech</td>\n",
       "      <td>neutral</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>2nd</td>\n",
       "      <td>actor_1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speech</td>\n",
       "      <td>calm</td>\n",
       "      <td>normal</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>1st</td>\n",
       "      <td>actor_1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>song</td>\n",
       "      <td>fearful</td>\n",
       "      <td>normal</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>2nd</td>\n",
       "      <td>actor_24</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>song</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>1st</td>\n",
       "      <td>actor_24</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>song</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Kids are talking by the door</td>\n",
       "      <td>2nd</td>\n",
       "      <td>actor_24</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>song</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>1st</td>\n",
       "      <td>actor_24</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>song</td>\n",
       "      <td>fearful</td>\n",
       "      <td>strong</td>\n",
       "      <td>Dogs are sitting by the door</td>\n",
       "      <td>2nd</td>\n",
       "      <td>actor_24</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2429 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     vocal_channel  emotion emotional_intensity                     statement  \\\n",
       "0           speech  neutral              normal  Kids are talking by the door   \n",
       "1           speech  neutral              normal  Kids are talking by the door   \n",
       "2           speech  neutral              normal  Dogs are sitting by the door   \n",
       "3           speech  neutral              normal  Dogs are sitting by the door   \n",
       "4           speech     calm              normal  Kids are talking by the door   \n",
       "...            ...      ...                 ...                           ...   \n",
       "2424          song  fearful              normal  Dogs are sitting by the door   \n",
       "2425          song  fearful              strong  Kids are talking by the door   \n",
       "2426          song  fearful              strong  Kids are talking by the door   \n",
       "2427          song  fearful              strong  Dogs are sitting by the door   \n",
       "2428          song  fearful              strong  Dogs are sitting by the door   \n",
       "\n",
       "     repetition     actor sex  \n",
       "0           1st   actor_1   M  \n",
       "1           2nd   actor_1   M  \n",
       "2           1st   actor_1   M  \n",
       "3           2nd   actor_1   M  \n",
       "4           1st   actor_1   M  \n",
       "...         ...       ...  ..  \n",
       "2424        2nd  actor_24   F  \n",
       "2425        1st  actor_24   F  \n",
       "2426        2nd  actor_24   F  \n",
       "2427        1st  actor_24   F  \n",
       "2428        2nd  actor_24   F  \n",
       "\n",
       "[2429 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER = 'dataset'\n",
    "DATASET = os.path.join(DATA_FOLDER, 'outliers_removed.csv')\n",
    "\n",
    "df = pd.read_csv(DATASET)\n",
    "numerical_attr_list = [col for col in df.columns if is_numeric_dtype(df[col])]\n",
    "\n",
    "possible_targets = [col for col in df.columns if not is_numeric_dtype(df[col])]\n",
    "df[possible_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:33.304718Z",
     "start_time": "2023-05-30T13:19:33.303682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_COLUMN = 'emotion'\n",
    "values = df[TARGET_COLUMN].unique().tolist()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:43.895220Z",
     "start_time": "2023-05-30T13:19:43.866505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zc_sum\n",
      "zc_mean\n",
      "zc_std\n",
      "zc_kur\n",
      "zc_skew\n",
      "zc_mean_w1\n",
      "zc_sum_w2\n",
      "zc_skew_w2\n",
      "zc_sum_w3\n",
      "zc_kur_w3\n",
      "zc_sum_w4\n",
      "zc_mean_w4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    excluded_prefixes = ['zc']\n",
    "    columns_to_remove = []\n",
    "    for column in df[numerical_attr_list].columns:\n",
    "        if column.startswith(tuple(excluded_prefixes)) or column == TARGET_COLUMN:\n",
    "            columns_to_remove.append(column)\n",
    "\n",
    "    columns_to_remove;\n",
    "    df = df.drop(columns=columns_to_remove, axis=1)\n",
    "    \n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "    \n",
    "for col in columns_to_remove:\n",
    "    try:\n",
    "        print(col)\n",
    "        numerical_attr_list.remove(col)\n",
    "    except ValueError as e:\n",
    "        print(e, f\"raised by column '{col}'\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:43.895376Z",
     "start_time": "2023-05-30T13:19:43.875940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target variable: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([373, 369, 189, 374, 373, 185, 374, 192]))\n",
      "X has shape (2429, 231)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df[numerical_attr_list])\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "y = label_enc.fit_transform(df[TARGET_COLUMN])\n",
    "\n",
    "# print(f\"numerical_attr_list is {numerical_attr_list}\")\n",
    "print(f\"target variable: {np.unique(y, return_counts=True)}\")\n",
    "print(f\"X has shape {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1449, 231)\n",
      "Val: (363, 231)\n",
      "Test: (617, 231)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide in train, test, validation\n",
    "df[\"actor_number\"] = df.actor.apply(lambda x: int(x.split(\"_\")[1]))\n",
    "test_mask = df.actor_number >= 19\n",
    "df.drop(columns=\"actor_number\", inplace=True)\n",
    "\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[~test_mask], y[~test_mask], test_size=0.2)\n",
    "print(f\"Train: {X_train.shape}\\nVal: {X_val.shape}\\nTest: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the decision boundaries in the embedding\n",
    "def boundaries_on_embedding(reducer, predictor, \n",
    "                            embedding=None, n_pts=30, \n",
    "                            **kwargs):\n",
    "    \n",
    "    global X, y, X_train, X_test,X_val, y_train, y_test, y_val\n",
    "    cmap, title = kwargs.get(\"cmap\", \"viridis\"), kwargs.get(\"title\", \"Decision boundaries on embedding\")\n",
    "    if embedding is None:\n",
    "        embedding =  reducer.fit_transform(X)\n",
    "        \n",
    "    # Generate a grid in embedding\n",
    "    xx = np.linspace(np.min(embedding[:,0]), np.max(embedding[:,0]), n_pts)\n",
    "    yy = np.linspace(np.min(embedding[:,1]), np.max(embedding[:,1]), n_pts)\n",
    "\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    points_in_embedding = np.array(np.meshgrid(xx, yy)).T\n",
    "    old_shape = points_in_embedding.shape[:-1]\n",
    "    \n",
    "    # Maps them back in the big space\n",
    "    points_in_embedding = points_in_embedding.reshape(-1,2)\n",
    "    points_in_gigaspace = reducer.inverse_transform(points_in_embedding)\n",
    "\n",
    "    # Gets results\n",
    "    results = predictor.predict(points_in_gigaspace).reshape(old_shape)\n",
    "    plt.contourf(XX, YY, results.T, cmap=cmap, alpha=0.6, levels=len(np.unique(y))-1)\n",
    "    plt.scatter(*embedding[test_mask].T, c=y_test, marker=\"o\", edgecolor=\"k\", s=5,cmap=cmap, label=\"test\")\n",
    "    plt.scatter(*embedding[~test_mask].T, c=y[~test_mask], marker=\"+\",  s=5, cmap=cmap, label=\"train+val\")\n",
    "    plt.legend()\n",
    "    plt.axis(\"off\");\n",
    "    plt.title(title);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now prepare an  so it saves time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "\n",
    "# reducer = UMAP(n_neighbors=20)\n",
    "# embedding = reducer.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reducer = PCA(n_components=2)\n",
    "embedding = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cfm(y_true, y_pred, title=\"Confusion matrix\")\n",
    "    fig, ax = plt.subplots(figsize=default_style.SHORT_HALFSIZE_FIGURE)\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cf, annot=True, cmap='Greens', fmt=\".4g\", cbar=False, ax=ax)\n",
    "    ax.set_xlabel('True')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ticks = np.unique(y_true)\n",
    "    ax.set_xticks(ticks + 0.5, labels=label_enc.inverse_transform(ticks))\n",
    "    ax.set_yticks(ticks + 0.5, labels=label_enc.inverse_transform(ticks))\n",
    "    \n",
    "    plt.savefig(f\"images/{title.replace(\" \", \"_\")}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:56:17,783] A new study created in memory with name: no-name-7a4f0330-c0f2-4c6b-8c9b-8ca15c9aa866\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[W 2023-07-08 13:56:17,794] Trial 0 failed with parameters: {'penalty': 'l1', 'C': 7.783445997928208, 'solver': 'newton-cg'} because of the following error: ValueError(\"Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:17,809] Trial 3 failed with parameters: {'penalty': 'elasticnet', 'C': 3.3991404494622555, 'solver': 'lbfgs'} because of the following error: ValueError(\"Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:17,819] Trial 3 failed with value None.\n",
      "[W 2023-07-08 13:56:17,816] Trial 0 failed with value None.\n",
      "[W 2023-07-08 13:56:17,835] Trial 4 failed with parameters: {'penalty': 'elasticnet', 'C': 4.525997039337021, 'solver': 'liblinear'} because of the following error: ValueError(\"Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "[W 2023-07-08 13:56:17,845] Trial 4 failed with value None.\n",
      "[W 2023-07-08 13:56:17,863] Trial 6 failed with parameters: {'penalty': 'l1', 'C': 2.798105429134936, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:17,877] Trial 6 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-08 13:56:21,710] Trial 5 finished with value: 0.5371900826446281 and parameters: {'penalty': 'l2', 'C': 3.186065677837688, 'solver': 'lbfgs'}. Best is trial 5 with value: 0.5371900826446281.\n",
      "[W 2023-07-08 13:56:21,724] Trial 8 failed with parameters: {'penalty': 'elasticnet', 'C': 3.023965950561543, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:21,731] Trial 8 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-08 13:56:21,871] Trial 7 finished with value: 0.5564738292011019 and parameters: {'penalty': 'none', 'C': 5.042992438480683, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.5564738292011019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:21,879] Trial 10 failed with parameters: {'penalty': 'elasticnet', 'C': 7.5350883013042536, 'solver': 'newton-cg'} because of the following error: ValueError(\"Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:21,886] Trial 10 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:23,923] Trial 1 finished with value: 0.5647382920110193 and parameters: {'penalty': 'none', 'C': 9.502352223733316, 'solver': 'sag'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:25,018] Trial 2 finished with value: 0.5454545454545454 and parameters: {'penalty': 'none', 'C': 9.213733421165838, 'solver': 'saga'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[W 2023-07-08 13:56:25,035] Trial 13 failed with parameters: {'penalty': 'l1', 'C': 6.230077828859545, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:25,044] Trial 13 failed with value None.\n",
      "[W 2023-07-08 13:56:25,058] Trial 14 failed with parameters: {'penalty': 'elasticnet', 'C': 4.3224121311560015, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:25,086] Trial 14 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-08 13:56:26,859] Trial 15 finished with value: 0.5399449035812672 and parameters: {'penalty': 'l2', 'C': 5.509035463332134, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:28,630] Trial 12 finished with value: 0.5454545454545454 and parameters: {'penalty': 'l2', 'C': 6.32293552517131, 'solver': 'sag'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:28,752] Trial 9 finished with value: 0.5454545454545454 and parameters: {'penalty': 'none', 'C': 0.7484027291690064, 'solver': 'saga'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-08 13:56:30,362] Trial 17 finished with value: 0.5399449035812672 and parameters: {'penalty': 'l2', 'C': 5.7848864939626115, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[W 2023-07-08 13:56:30,372] Trial 19 failed with parameters: {'penalty': 'elasticnet', 'C': 1.7243977285279508, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:30,374] Trial 19 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-08 13:56:31,588] Trial 20 finished with value: 0.5564738292011019 and parameters: {'penalty': 'none', 'C': 3.930027599651206, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[W 2023-07-08 13:56:31,604] Trial 21 failed with parameters: {'penalty': 'l1', 'C': 8.912859158088247, 'solver': 'lbfgs'} because of the following error: ValueError(\"Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:31,621] Trial 21 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:34,487] Trial 18 finished with value: 0.5619834710743802 and parameters: {'penalty': 'none', 'C': 1.4880826586977358, 'solver': 'sag'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:56:39,299] Trial 22 finished with value: 0.4765840220385675 and parameters: {'penalty': 'l2', 'C': 0.19892523810522, 'solver': 'saga'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[W 2023-07-08 13:56:39,398] Trial 24 failed with parameters: {'penalty': 'elasticnet', 'C': 8.839608910358056, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,403] Trial 24 failed with value None.\n",
      "[W 2023-07-08 13:56:39,449] Trial 25 failed with parameters: {'penalty': 'elasticnet', 'C': 9.483254157369693, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,463] Trial 25 failed with value None.\n",
      "[W 2023-07-08 13:56:39,500] Trial 26 failed with parameters: {'penalty': 'elasticnet', 'C': 9.607585684081009, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,506] Trial 26 failed with value None.\n",
      "[W 2023-07-08 13:56:39,560] Trial 27 failed with parameters: {'penalty': 'l1', 'C': 9.233642500014355, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:39,567] Trial 27 failed with value None.\n",
      "[W 2023-07-08 13:56:39,595] Trial 28 failed with parameters: {'penalty': 'elasticnet', 'C': 9.258936078141268, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,602] Trial 28 failed with value None.\n",
      "[W 2023-07-08 13:56:39,637] Trial 29 failed with parameters: {'penalty': 'l1', 'C': 9.021749255167471, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:39,644] Trial 29 failed with value None.\n",
      "[W 2023-07-08 13:56:39,722] Trial 30 failed with parameters: {'penalty': 'elasticnet', 'C': 9.159636586079408, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,729] Trial 30 failed with value None.\n",
      "[W 2023-07-08 13:56:39,779] Trial 31 failed with parameters: {'penalty': 'elasticnet', 'C': 9.589404320529233, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,785] Trial 31 failed with value None.\n",
      "[W 2023-07-08 13:56:39,817] Trial 32 failed with parameters: {'penalty': 'elasticnet', 'C': 9.83671121089019, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,820] Trial 32 failed with value None.\n",
      "[W 2023-07-08 13:56:39,877] Trial 33 failed with parameters: {'penalty': 'elasticnet', 'C': 9.347413691469153, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,882] Trial 33 failed with value None.\n",
      "[W 2023-07-08 13:56:39,910] Trial 34 failed with parameters: {'penalty': 'elasticnet', 'C': 9.650349936688542, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,913] Trial 34 failed with value None.\n",
      "[W 2023-07-08 13:56:39,937] Trial 35 failed with parameters: {'penalty': 'elasticnet', 'C': 9.790843614856827, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,940] Trial 35 failed with value None.\n",
      "[W 2023-07-08 13:56:39,964] Trial 36 failed with parameters: {'penalty': 'elasticnet', 'C': 9.46329794302963, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:39,967] Trial 36 failed with value None.\n",
      "[W 2023-07-08 13:56:39,993] Trial 37 failed with parameters: {'penalty': 'elasticnet', 'C': 8.721238161365825, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:39,996] Trial 37 failed with value None.\n",
      "[W 2023-07-08 13:56:40,044] Trial 38 failed with parameters: {'penalty': 'elasticnet', 'C': 9.430068917138419, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:40,049] Trial 38 failed with value None.\n",
      "[W 2023-07-08 13:56:40,074] Trial 39 failed with parameters: {'penalty': 'l1', 'C': 9.846370276908623, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,076] Trial 39 failed with value None.\n",
      "[W 2023-07-08 13:56:40,103] Trial 40 failed with parameters: {'penalty': 'l1', 'C': 9.38114799022456, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,106] Trial 40 failed with value None.\n",
      "[W 2023-07-08 13:56:40,205] Trial 41 failed with parameters: {'penalty': 'l1', 'C': 9.892533612205106, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,212] Trial 41 failed with value None.\n",
      "[W 2023-07-08 13:56:40,293] Trial 42 failed with parameters: {'penalty': 'l1', 'C': 9.965522993054623, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,298] Trial 42 failed with value None.\n",
      "[W 2023-07-08 13:56:40,420] Trial 43 failed with parameters: {'penalty': 'l1', 'C': 9.620226421455055, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,433] Trial 43 failed with value None.\n",
      "[W 2023-07-08 13:56:40,495] Trial 44 failed with parameters: {'penalty': 'l1', 'C': 9.901411036703767, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,508] Trial 44 failed with value None.\n",
      "[W 2023-07-08 13:56:40,566] Trial 45 failed with parameters: {'penalty': 'elasticnet', 'C': 9.89890861068523, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:40,581] Trial 45 failed with value None.\n",
      "[W 2023-07-08 13:56:40,646] Trial 46 failed with parameters: {'penalty': 'l1', 'C': 9.709917224929608, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,660] Trial 46 failed with value None.\n",
      "[W 2023-07-08 13:56:40,746] Trial 47 failed with parameters: {'penalty': 'elasticnet', 'C': 9.801047895986592, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:40,750] Trial 47 failed with value None.\n",
      "[W 2023-07-08 13:56:40,826] Trial 48 failed with parameters: {'penalty': 'l1', 'C': 9.852815459475284, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,836] Trial 48 failed with value None.\n",
      "[W 2023-07-08 13:56:40,930] Trial 49 failed with parameters: {'penalty': 'l1', 'C': 9.326379498136111, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:40,943] Trial 49 failed with value None.\n",
      "[W 2023-07-08 13:56:41,024] Trial 50 failed with parameters: {'penalty': 'elasticnet', 'C': 9.818647016405759, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,042] Trial 50 failed with value None.\n",
      "[W 2023-07-08 13:56:41,103] Trial 51 failed with parameters: {'penalty': 'elasticnet', 'C': 9.932708617200847, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,116] Trial 51 failed with value None.\n",
      "[W 2023-07-08 13:56:41,151] Trial 52 failed with parameters: {'penalty': 'elasticnet', 'C': 9.33527706396155, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,158] Trial 52 failed with value None.\n",
      "[W 2023-07-08 13:56:41,200] Trial 53 failed with parameters: {'penalty': 'elasticnet', 'C': 9.84540466602548, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:41,207] Trial 53 failed with value None.\n",
      "[W 2023-07-08 13:56:41,251] Trial 54 failed with parameters: {'penalty': 'elasticnet', 'C': 9.872228614631014, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,262] Trial 54 failed with value None.\n",
      "[W 2023-07-08 13:56:41,321] Trial 55 failed with parameters: {'penalty': 'elasticnet', 'C': 9.638805037342763, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,331] Trial 55 failed with value None.\n",
      "[W 2023-07-08 13:56:41,398] Trial 56 failed with parameters: {'penalty': 'elasticnet', 'C': 9.167393107000953, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,415] Trial 56 failed with value None.\n",
      "[W 2023-07-08 13:56:41,466] Trial 57 failed with parameters: {'penalty': 'l1', 'C': 9.317161028384294, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,475] Trial 57 failed with value None.\n",
      "[W 2023-07-08 13:56:41,539] Trial 58 failed with parameters: {'penalty': 'l1', 'C': 9.227289606763332, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,544] Trial 58 failed with value None.\n",
      "[W 2023-07-08 13:56:41,599] Trial 59 failed with parameters: {'penalty': 'l1', 'C': 9.402328015431966, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,607] Trial 59 failed with value None.\n",
      "[W 2023-07-08 13:56:41,668] Trial 60 failed with parameters: {'penalty': 'elasticnet', 'C': 8.813344804180598, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,680] Trial 60 failed with value None.\n",
      "[W 2023-07-08 13:56:41,741] Trial 61 failed with parameters: {'penalty': 'elasticnet', 'C': 9.012746409577735, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:41,748] Trial 61 failed with value None.\n",
      "[W 2023-07-08 13:56:41,777] Trial 62 failed with parameters: {'penalty': 'elasticnet', 'C': 9.905893401592175, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,782] Trial 62 failed with value None.\n",
      "[W 2023-07-08 13:56:41,822] Trial 63 failed with parameters: {'penalty': 'l1', 'C': 9.544145526259893, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,829] Trial 63 failed with value None.\n",
      "[W 2023-07-08 13:56:41,861] Trial 64 failed with parameters: {'penalty': 'l1', 'C': 9.226094400225556, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,868] Trial 64 failed with value None.\n",
      "[W 2023-07-08 13:56:41,908] Trial 65 failed with parameters: {'penalty': 'elasticnet', 'C': 8.40624589466749, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,915] Trial 65 failed with value None.\n",
      "[W 2023-07-08 13:56:41,949] Trial 66 failed with parameters: {'penalty': 'l1', 'C': 9.103010841445764, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:41,954] Trial 66 failed with value None.\n",
      "[W 2023-07-08 13:56:41,986] Trial 67 failed with parameters: {'penalty': 'elasticnet', 'C': 9.079410350686388, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:41,991] Trial 67 failed with value None.\n",
      "[W 2023-07-08 13:56:42,031] Trial 68 failed with parameters: {'penalty': 'l1', 'C': 8.988854534529295, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,038] Trial 68 failed with value None.\n",
      "[W 2023-07-08 13:56:42,072] Trial 69 failed with parameters: {'penalty': 'elasticnet', 'C': 9.448165749238946, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:42,080] Trial 69 failed with value None.\n",
      "[W 2023-07-08 13:56:42,131] Trial 70 failed with parameters: {'penalty': 'elasticnet', 'C': 9.862279547947102, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,140] Trial 70 failed with value None.\n",
      "[W 2023-07-08 13:56:42,172] Trial 71 failed with parameters: {'penalty': 'elasticnet', 'C': 9.974860981388943, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,179] Trial 71 failed with value None.\n",
      "[W 2023-07-08 13:56:42,231] Trial 72 failed with parameters: {'penalty': 'l1', 'C': 9.935599761884266, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,238] Trial 72 failed with value None.\n",
      "[W 2023-07-08 13:56:42,274] Trial 73 failed with parameters: {'penalty': 'elasticnet', 'C': 8.541262337897669, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,281] Trial 73 failed with value None.\n",
      "[W 2023-07-08 13:56:42,335] Trial 74 failed with parameters: {'penalty': 'l1', 'C': 9.880194371500771, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,339] Trial 74 failed with value None.\n",
      "[W 2023-07-08 13:56:42,386] Trial 75 failed with parameters: {'penalty': 'elasticnet', 'C': 9.561991063207739, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,389] Trial 75 failed with value None.\n",
      "[W 2023-07-08 13:56:42,453] Trial 76 failed with parameters: {'penalty': 'elasticnet', 'C': 9.59639457899681, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,459] Trial 76 failed with value None.\n",
      "[W 2023-07-08 13:56:42,511] Trial 77 failed with parameters: {'penalty': 'elasticnet', 'C': 9.55112576426617, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:42,517] Trial 77 failed with value None.\n",
      "[W 2023-07-08 13:56:42,555] Trial 78 failed with parameters: {'penalty': 'l1', 'C': 9.553592145019692, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,565] Trial 78 failed with value None.\n",
      "[W 2023-07-08 13:56:42,617] Trial 79 failed with parameters: {'penalty': 'elasticnet', 'C': 9.6579918892852, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,632] Trial 79 failed with value None.\n",
      "[W 2023-07-08 13:56:42,670] Trial 80 failed with parameters: {'penalty': 'elasticnet', 'C': 9.177927488755211, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,681] Trial 80 failed with value None.\n",
      "[W 2023-07-08 13:56:42,727] Trial 81 failed with parameters: {'penalty': 'elasticnet', 'C': 9.72715950213106, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,733] Trial 81 failed with value None.\n",
      "[W 2023-07-08 13:56:42,770] Trial 82 failed with parameters: {'penalty': 'l1', 'C': 9.897265987185564, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,783] Trial 82 failed with value None.\n",
      "[W 2023-07-08 13:56:42,834] Trial 83 failed with parameters: {'penalty': 'l1', 'C': 9.612964844299857, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:42,850] Trial 83 failed with value None.\n",
      "[W 2023-07-08 13:56:42,902] Trial 84 failed with parameters: {'penalty': 'elasticnet', 'C': 9.561654903221173, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:42,908] Trial 84 failed with value None.\n",
      "[W 2023-07-08 13:56:42,994] Trial 85 failed with parameters: {'penalty': 'l1', 'C': 8.474214047828173, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:42,998] Trial 85 failed with value None.\n",
      "[W 2023-07-08 13:56:43,052] Trial 86 failed with parameters: {'penalty': 'l1', 'C': 9.542672195172646, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:43,058] Trial 86 failed with value None.\n",
      "[W 2023-07-08 13:56:43,115] Trial 87 failed with parameters: {'penalty': 'elasticnet', 'C': 9.315754439042397, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,121] Trial 87 failed with value None.\n",
      "[W 2023-07-08 13:56:43,164] Trial 88 failed with parameters: {'penalty': 'elasticnet', 'C': 9.819858841899617, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,175] Trial 88 failed with value None.\n",
      "[W 2023-07-08 13:56:43,212] Trial 89 failed with parameters: {'penalty': 'l1', 'C': 8.95958014027055, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:43,220] Trial 89 failed with value None.\n",
      "[W 2023-07-08 13:56:43,283] Trial 90 failed with parameters: {'penalty': 'elasticnet', 'C': 9.931107674412385, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,287] Trial 90 failed with value None.\n",
      "[W 2023-07-08 13:56:43,344] Trial 91 failed with parameters: {'penalty': 'elasticnet', 'C': 9.930197333247248, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,350] Trial 91 failed with value None.\n",
      "[W 2023-07-08 13:56:43,390] Trial 92 failed with parameters: {'penalty': 'l1', 'C': 9.992704775497359, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:43,408] Trial 92 failed with value None.\n",
      "[W 2023-07-08 13:56:43,465] Trial 93 failed with parameters: {'penalty': 'elasticnet', 'C': 9.906111834786588, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:43,468] Trial 93 failed with value None.\n",
      "[W 2023-07-08 13:56:43,514] Trial 94 failed with parameters: {'penalty': 'elasticnet', 'C': 9.904238018902612, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,517] Trial 94 failed with value None.\n",
      "[W 2023-07-08 13:56:43,565] Trial 95 failed with parameters: {'penalty': 'elasticnet', 'C': 9.591512195252063, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,573] Trial 95 failed with value None.\n",
      "[W 2023-07-08 13:56:43,623] Trial 96 failed with parameters: {'penalty': 'l1', 'C': 9.969614043552532, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:43,626] Trial 96 failed with value None.\n",
      "[W 2023-07-08 13:56:43,676] Trial 97 failed with parameters: {'penalty': 'elasticnet', 'C': 9.746388252040191, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,678] Trial 97 failed with value None.\n",
      "[W 2023-07-08 13:56:43,728] Trial 98 failed with parameters: {'penalty': 'elasticnet', 'C': 9.975045026223524, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,731] Trial 98 failed with value None.\n",
      "[W 2023-07-08 13:56:43,791] Trial 99 failed with parameters: {'penalty': 'l1', 'C': 9.809063633981024, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:43,794] Trial 99 failed with value None.\n",
      "[W 2023-07-08 13:56:43,836] Trial 100 failed with parameters: {'penalty': 'elasticnet', 'C': 9.729475977988969, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,843] Trial 100 failed with value None.\n",
      "[W 2023-07-08 13:56:43,885] Trial 101 failed with parameters: {'penalty': 'l1', 'C': 9.798167666845817, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:43,892] Trial 101 failed with value None.\n",
      "[W 2023-07-08 13:56:43,939] Trial 102 failed with parameters: {'penalty': 'elasticnet', 'C': 9.697402329581529, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:43,950] Trial 102 failed with value None.\n",
      "[W 2023-07-08 13:56:43,998] Trial 103 failed with parameters: {'penalty': 'l1', 'C': 8.563800222686478, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:44,003] Trial 103 failed with value None.\n",
      "[W 2023-07-08 13:56:44,055] Trial 104 failed with parameters: {'penalty': 'elasticnet', 'C': 9.89018004226694, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,060] Trial 104 failed with value None.\n",
      "[W 2023-07-08 13:56:44,120] Trial 105 failed with parameters: {'penalty': 'elasticnet', 'C': 9.063467190586312, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,134] Trial 105 failed with value None.\n",
      "[W 2023-07-08 13:56:44,224] Trial 106 failed with parameters: {'penalty': 'elasticnet', 'C': 9.652594264699227, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,240] Trial 106 failed with value None.\n",
      "[W 2023-07-08 13:56:44,300] Trial 107 failed with parameters: {'penalty': 'elasticnet', 'C': 9.950885016662102, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,306] Trial 107 failed with value None.\n",
      "[W 2023-07-08 13:56:44,360] Trial 108 failed with parameters: {'penalty': 'elasticnet', 'C': 9.723206987389421, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,365] Trial 108 failed with value None.\n",
      "[W 2023-07-08 13:56:44,425] Trial 109 failed with parameters: {'penalty': 'elasticnet', 'C': 9.237129114712474, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:44,435] Trial 109 failed with value None.\n",
      "[W 2023-07-08 13:56:44,485] Trial 110 failed with parameters: {'penalty': 'elasticnet', 'C': 9.109858994343318, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,496] Trial 110 failed with value None.\n",
      "[W 2023-07-08 13:56:44,550] Trial 111 failed with parameters: {'penalty': 'l1', 'C': 9.867308051886194, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:44,556] Trial 111 failed with value None.\n",
      "[W 2023-07-08 13:56:44,604] Trial 112 failed with parameters: {'penalty': 'elasticnet', 'C': 9.848482797431899, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,623] Trial 112 failed with value None.\n",
      "[W 2023-07-08 13:56:44,668] Trial 113 failed with parameters: {'penalty': 'l1', 'C': 9.956341361129015, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:44,676] Trial 113 failed with value None.\n",
      "[W 2023-07-08 13:56:44,739] Trial 114 failed with parameters: {'penalty': 'elasticnet', 'C': 9.019844124858112, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,750] Trial 114 failed with value None.\n",
      "[W 2023-07-08 13:56:44,810] Trial 115 failed with parameters: {'penalty': 'elasticnet', 'C': 9.790845787709706, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,820] Trial 115 failed with value None.\n",
      "[W 2023-07-08 13:56:44,871] Trial 116 failed with parameters: {'penalty': 'l1', 'C': 9.877003988796876, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:44,873] Trial 116 failed with value None.\n",
      "[W 2023-07-08 13:56:44,934] Trial 117 failed with parameters: {'penalty': 'elasticnet', 'C': 9.91858686341739, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:44,939] Trial 117 failed with value None.\n",
      "[W 2023-07-08 13:56:44,984] Trial 118 failed with parameters: {'penalty': 'elasticnet', 'C': 9.046071972016827, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:44,990] Trial 118 failed with value None.\n",
      "[W 2023-07-08 13:56:45,059] Trial 119 failed with parameters: {'penalty': 'l1', 'C': 8.978863010409286, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,068] Trial 119 failed with value None.\n",
      "[W 2023-07-08 13:56:45,115] Trial 120 failed with parameters: {'penalty': 'l1', 'C': 9.71288870569941, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,118] Trial 120 failed with value None.\n",
      "[W 2023-07-08 13:56:45,167] Trial 121 failed with parameters: {'penalty': 'elasticnet', 'C': 9.8059026716959, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,179] Trial 121 failed with value None.\n",
      "[W 2023-07-08 13:56:45,267] Trial 122 failed with parameters: {'penalty': 'l1', 'C': 9.485306633766939, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,275] Trial 122 failed with value None.\n",
      "[W 2023-07-08 13:56:45,342] Trial 123 failed with parameters: {'penalty': 'l1', 'C': 9.989411847576298, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,347] Trial 123 failed with value None.\n",
      "[W 2023-07-08 13:56:45,399] Trial 124 failed with parameters: {'penalty': 'elasticnet', 'C': 9.87368681547246, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,426] Trial 124 failed with value None.\n",
      "[W 2023-07-08 13:56:45,521] Trial 125 failed with parameters: {'penalty': 'elasticnet', 'C': 9.708238444036393, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:45,532] Trial 125 failed with value None.\n",
      "[W 2023-07-08 13:56:45,563] Trial 126 failed with parameters: {'penalty': 'elasticnet', 'C': 9.567057632267447, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,566] Trial 126 failed with value None.\n",
      "[W 2023-07-08 13:56:45,608] Trial 127 failed with parameters: {'penalty': 'elasticnet', 'C': 9.666573234381849, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,611] Trial 127 failed with value None.\n",
      "[W 2023-07-08 13:56:45,642] Trial 128 failed with parameters: {'penalty': 'l1', 'C': 9.678481255010073, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,645] Trial 128 failed with value None.\n",
      "[W 2023-07-08 13:56:45,671] Trial 129 failed with parameters: {'penalty': 'elasticnet', 'C': 9.945700237767499, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,673] Trial 129 failed with value None.\n",
      "[W 2023-07-08 13:56:45,700] Trial 130 failed with parameters: {'penalty': 'l1', 'C': 9.868845571007713, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,702] Trial 130 failed with value None.\n",
      "[W 2023-07-08 13:56:45,732] Trial 131 failed with parameters: {'penalty': 'elasticnet', 'C': 9.595790582312594, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,737] Trial 131 failed with value None.\n",
      "[W 2023-07-08 13:56:45,768] Trial 132 failed with parameters: {'penalty': 'l1', 'C': 9.641784827687305, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,770] Trial 132 failed with value None.\n",
      "[W 2023-07-08 13:56:45,797] Trial 133 failed with parameters: {'penalty': 'l1', 'C': 9.528560289906402, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:45,799] Trial 133 failed with value None.\n",
      "[W 2023-07-08 13:56:45,835] Trial 134 failed with parameters: {'penalty': 'l1', 'C': 8.110285597776974, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,841] Trial 134 failed with value None.\n",
      "[W 2023-07-08 13:56:45,900] Trial 135 failed with parameters: {'penalty': 'elasticnet', 'C': 9.764208375746467, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:45,906] Trial 135 failed with value None.\n",
      "[W 2023-07-08 13:56:45,990] Trial 136 failed with parameters: {'penalty': 'l1', 'C': 9.661068174263837, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:45,997] Trial 136 failed with value None.\n",
      "[W 2023-07-08 13:56:46,057] Trial 137 failed with parameters: {'penalty': 'l1', 'C': 9.695900301964386, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,072] Trial 137 failed with value None.\n",
      "[W 2023-07-08 13:56:46,125] Trial 138 failed with parameters: {'penalty': 'elasticnet', 'C': 9.848175826598192, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:46,139] Trial 138 failed with value None.\n",
      "[W 2023-07-08 13:56:46,219] Trial 139 failed with parameters: {'penalty': 'l1', 'C': 9.961279283848254, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,228] Trial 139 failed with value None.\n",
      "[W 2023-07-08 13:56:46,278] Trial 140 failed with parameters: {'penalty': 'elasticnet', 'C': 9.414922153363397, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:46,291] Trial 140 failed with value None.\n",
      "[W 2023-07-08 13:56:46,345] Trial 141 failed with parameters: {'penalty': 'l1', 'C': 9.812986158603785, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:46,352] Trial 141 failed with value None.\n",
      "[W 2023-07-08 13:56:46,405] Trial 142 failed with parameters: {'penalty': 'l1', 'C': 9.24728363399413, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,411] Trial 142 failed with value None.\n",
      "[W 2023-07-08 13:56:46,475] Trial 143 failed with parameters: {'penalty': 'l1', 'C': 9.576903355093004, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,481] Trial 143 failed with value None.\n",
      "[W 2023-07-08 13:56:46,538] Trial 144 failed with parameters: {'penalty': 'l1', 'C': 9.238702253496587, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,545] Trial 144 failed with value None.\n",
      "[W 2023-07-08 13:56:46,619] Trial 145 failed with parameters: {'penalty': 'l1', 'C': 9.992367388887045, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,633] Trial 145 failed with value None.\n",
      "[W 2023-07-08 13:56:46,703] Trial 146 failed with parameters: {'penalty': 'l1', 'C': 9.541164913379873, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "[W 2023-07-08 13:56:46,708] Trial 146 failed with value None.\n",
      "[W 2023-07-08 13:56:46,787] Trial 147 failed with parameters: {'penalty': 'elasticnet', 'C': 9.529315275417437, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:46,796] Trial 147 failed with value None.\n",
      "[W 2023-07-08 13:56:46,839] Trial 148 failed with parameters: {'penalty': 'elasticnet', 'C': 9.847670159449743, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "[W 2023-07-08 13:56:46,858] Trial 148 failed with value None.\n",
      "[W 2023-07-08 13:56:46,909] Trial 149 failed with parameters: {'penalty': 'elasticnet', 'C': 9.677195922759898, 'solver': 'sag'} because of the following error: ValueError(\"Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/441388116.py\", line 14, in objective_fun\n",
      "    logr.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:56:46,942] Trial 149 failed with value None.\n",
      "[I 2023-07-08 13:57:04,779] Trial 11 finished with value: 0.5426997245179064 and parameters: {'penalty': 'l1', 'C': 6.239939473108199, 'solver': 'liblinear'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[I 2023-07-08 13:57:07,476] Trial 16 finished with value: 0.5399449035812672 and parameters: {'penalty': 'l1', 'C': 6.229067639768931, 'solver': 'liblinear'}. Best is trial 1 with value: 0.5647382920110193.\n",
      "[I 2023-07-08 13:57:26,316] Trial 23 finished with value: 0.5454545454545454 and parameters: {'penalty': 'l1', 'C': 9.935306481776557, 'solver': 'liblinear'}. Best is trial 1 with value: 0.5647382920110193.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "# objective function to be minimized\n",
    "def objective_fun(trial):\n",
    "\n",
    "    penalty = trial.suggest_categorical('penalty', ['none', 'elasticnet', 'l1', 'l2'])\n",
    "    C = trial.suggest_float('C', 0.001,10)\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'newton-cg',  'sag', 'saga'])\n",
    "\n",
    "    logr = LogisticRegression(solver=solver, penalty=penalty, C=C)\n",
    "\n",
    "    logr.fit(X_train, y_train)\n",
    "    y_pred = logr.predict(X_val)\n",
    "\n",
    "    error = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 150, n_jobs = -1, catch=(ValueError, TypeError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "logr = LogisticRegression(**best_params)\n",
    "\n",
    "# Trains on test AND validation\n",
    "logr.fit(np.concatenate((X_train, X_val)), np.concatenate([y_train, y_val]))\n",
    "\n",
    "y_pred_test = logr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose liblinear solver beacause for small datasets is a good choice. \n",
    "The key difference between these l1(ridege) and l2(lasso) is that Lasso shrinks the less \n",
    "important featureâ€™s coefficient to zero thus, removing some feature altogether. \n",
    "So, this works well for feature selection in case we have a huge number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354.331x236.22 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boundaries_on_embedding(reducer, logr, embedding=embedding, \n",
    "                        title=\"Logistic Regression on PCA\", \n",
    "                        cmap=\"viridis\",\n",
    "                       n_pts=80)\n",
    "plt.savefig(\"images/LogRegUMAP.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T13:19:49.478898Z",
     "start_time": "2023-05-30T13:19:49.263397Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cfm(y_test, y_pred_test, title=\"Logistic regression confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppport Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:57:33,999] A new study created in memory with name: no-name-58ec3971-4a51-471c-92c5-8bbe107b7ccc\n",
      "[W 2023-07-08 13:57:34,018] Trial 1 failed with parameters: {'penalty': 'l1', 'C': 77.92872071775828, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:57:34,035] Trial 1 failed with value None.\n",
      "[W 2023-07-08 13:57:34,019] Trial 0 failed with parameters: {'penalty': 'l1', 'C': 91.93375941390859, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:34,043] Trial 0 failed with value None.\n",
      "[W 2023-07-08 13:57:34,047] Trial 3 failed with parameters: {'penalty': 'l1', 'C': 3.116751449542191, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:34,103] Trial 3 failed with value None.\n",
      "[W 2023-07-08 13:57:34,116] Trial 6 failed with parameters: {'penalty': 'l1', 'C': 78.1471465182607, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:34,120] Trial 6 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:44,852] Trial 5 finished with value: 0.5371900826446281 and parameters: {'penalty': 'l2', 'C': 89.71410187146715, 'loss': 'squared_hinge'}. Best is trial 5 with value: 0.5371900826446281.\n",
      "[W 2023-07-08 13:57:44,863] Trial 8 failed with parameters: {'penalty': 'l1', 'C': 49.36664841419965, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:57:44,868] Trial 8 failed with value None.\n",
      "[W 2023-07-08 13:57:44,887] Trial 9 failed with parameters: {'penalty': 'l1', 'C': 19.497483700268482, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:57:44,890] Trial 9 failed with value None.\n",
      "[W 2023-07-08 13:57:44,926] Trial 10 failed with parameters: {'penalty': 'l1', 'C': 89.37720055540943, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:44,930] Trial 10 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:45,062] Trial 7 finished with value: 0.4765840220385675 and parameters: {'penalty': 'l2', 'C': 68.2643871370509, 'loss': 'hinge'}. Best is trial 5 with value: 0.5371900826446281.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:45,407] Trial 2 finished with value: 0.5316804407713499 and parameters: {'penalty': 'l2', 'C': 55.23989293916977, 'loss': 'squared_hinge'}. Best is trial 5 with value: 0.5371900826446281.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:45,757] Trial 4 finished with value: 0.5013774104683195 and parameters: {'penalty': 'l2', 'C': 87.08291683490063, 'loss': 'squared_hinge'}. Best is trial 5 with value: 0.5371900826446281.\n",
      "[W 2023-07-08 13:57:45,822] Trial 14 failed with parameters: {'penalty': 'l1', 'C': 42.01893066037555, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:45,851] Trial 14 failed with value None.\n",
      "[W 2023-07-08 13:57:45,877] Trial 15 failed with parameters: {'penalty': 'l1', 'C': 85.81236498154055, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:45,883] Trial 15 failed with value None.\n",
      "[W 2023-07-08 13:57:45,913] Trial 16 failed with parameters: {'penalty': 'l1', 'C': 25.899800369348565, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:57:45,916] Trial 16 failed with value None.\n",
      "[W 2023-07-08 13:57:45,960] Trial 17 failed with parameters: {'penalty': 'l1', 'C': 24.629179218513784, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:57:45,965] Trial 17 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:55,950] Trial 11 finished with value: 0.5564738292011019 and parameters: {'penalty': 'l2', 'C': 13.232241053884712, 'loss': 'hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:57,618] Trial 12 finished with value: 0.5041322314049587 and parameters: {'penalty': 'l2', 'C': 40.73986711087145, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "[W 2023-07-08 13:57:57,629] Trial 20 failed with parameters: {'penalty': 'l1', 'C': 90.6475052492444, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:57,637] Trial 20 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:58,355] Trial 13 finished with value: 0.5261707988980716 and parameters: {'penalty': 'l2', 'C': 87.19302576717709, 'loss': 'hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:57:58,506] Trial 18 finished with value: 0.509641873278237 and parameters: {'penalty': 'l2', 'C': 73.75203728137645, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "[W 2023-07-08 13:57:58,515] Trial 23 failed with parameters: {'penalty': 'l1', 'C': 39.40785599220529, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:58,520] Trial 23 failed with value None.\n",
      "[W 2023-07-08 13:57:58,536] Trial 24 failed with parameters: {'penalty': 'l1', 'C': 99.56701322297067, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:57:58,538] Trial 24 failed with value None.\n",
      "[W 2023-07-08 13:57:58,550] Trial 25 failed with parameters: {'penalty': 'l1', 'C': 31.632560315242372, 'loss': 'squared_hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "[W 2023-07-08 13:57:58,552] Trial 25 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:58:08,018] Trial 19 finished with value: 0.5151515151515151 and parameters: {'penalty': 'l2', 'C': 78.1752809617262, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:58:10,273] Trial 21 finished with value: 0.5564738292011019 and parameters: {'penalty': 'l2', 'C': 57.504422403595626, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:10,298] Trial 28 failed with parameters: {'penalty': 'l1', 'C': 7.616884885468181, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,301] Trial 28 failed with value None.\n",
      "[W 2023-07-08 13:58:10,351] Trial 29 failed with parameters: {'penalty': 'l1', 'C': 6.064335324744323, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,363] Trial 29 failed with value None.\n",
      "[W 2023-07-08 13:58:10,422] Trial 30 failed with parameters: {'penalty': 'l1', 'C': 7.571029619414869, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,454] Trial 30 failed with value None.\n",
      "[W 2023-07-08 13:58:10,546] Trial 31 failed with parameters: {'penalty': 'l1', 'C': 6.902958466354771, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,566] Trial 31 failed with value None.\n",
      "[W 2023-07-08 13:58:10,611] Trial 32 failed with parameters: {'penalty': 'l1', 'C': 6.726992764195387, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,614] Trial 32 failed with value None.\n",
      "[W 2023-07-08 13:58:10,660] Trial 33 failed with parameters: {'penalty': 'l1', 'C': 9.105722732711996, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:10,667] Trial 33 failed with value None.\n",
      "[W 2023-07-08 13:58:10,712] Trial 34 failed with parameters: {'penalty': 'l1', 'C': 3.756708490176969, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,714] Trial 34 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[W 2023-07-08 13:58:10,756] Trial 35 failed with parameters: {'penalty': 'l1', 'C': 6.238044600025203, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,761] Trial 35 failed with value None.\n",
      "[W 2023-07-08 13:58:10,788] Trial 36 failed with parameters: {'penalty': 'l1', 'C': 4.069844976789907, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,795] Trial 36 failed with value None.\n",
      "[I 2023-07-08 13:58:10,799] Trial 26 finished with value: 0.4628099173553719 and parameters: {'penalty': 'l2', 'C': 52.39357770499198, 'loss': 'hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "[W 2023-07-08 13:58:10,858] Trial 37 failed with parameters: {'penalty': 'l1', 'C': 12.62097909511232, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,866] Trial 37 failed with value None.\n",
      "[W 2023-07-08 13:58:10,865] Trial 38 failed with parameters: {'penalty': 'l1', 'C': 13.988567083086338, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,935] Trial 38 failed with value None.\n",
      "[W 2023-07-08 13:58:10,977] Trial 39 failed with parameters: {'penalty': 'l1', 'C': 12.036157565900579, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:10,980] Trial 39 failed with value None.\n",
      "[W 2023-07-08 13:58:10,983] Trial 40 failed with parameters: {'penalty': 'l1', 'C': 8.721313289917255, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:10,986] Trial 40 failed with value None.\n",
      "[W 2023-07-08 13:58:11,052] Trial 42 failed with parameters: {'penalty': 'l1', 'C': 12.451729837864754, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,056] Trial 42 failed with value None.\n",
      "[W 2023-07-08 13:58:11,056] Trial 41 failed with parameters: {'penalty': 'l1', 'C': 16.909092701810202, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,069] Trial 41 failed with value None.\n",
      "[W 2023-07-08 13:58:11,114] Trial 43 failed with parameters: {'penalty': 'l1', 'C': 6.662751868089707, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,122] Trial 43 failed with value None.\n",
      "[W 2023-07-08 13:58:11,119] Trial 44 failed with parameters: {'penalty': 'l1', 'C': 12.096766833771133, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,130] Trial 44 failed with value None.\n",
      "[W 2023-07-08 13:58:11,185] Trial 45 failed with parameters: {'penalty': 'l1', 'C': 11.254601431048794, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:11,188] Trial 45 failed with value None.\n",
      "[W 2023-07-08 13:58:11,187] Trial 46 failed with parameters: {'penalty': 'l1', 'C': 8.79844046046955, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,213] Trial 46 failed with value None.\n",
      "[W 2023-07-08 13:58:11,274] Trial 48 failed with parameters: {'penalty': 'l1', 'C': 12.126118653516066, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[W 2023-07-08 13:58:11,275] Trial 47 failed with parameters: {'penalty': 'l1', 'C': 11.251410299965386, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,285] Trial 47 failed with value None.\n",
      "[W 2023-07-08 13:58:11,276] Trial 48 failed with value None.\n",
      "[I 2023-07-08 13:58:11,296] Trial 22 finished with value: 0.47107438016528924 and parameters: {'penalty': 'l2', 'C': 80.80568630245918, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n",
      "[W 2023-07-08 13:58:11,433] Trial 49 failed with parameters: {'penalty': 'l1', 'C': 11.848786615196232, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,436] Trial 49 failed with value None.\n",
      "[W 2023-07-08 13:58:11,439] Trial 50 failed with parameters: {'penalty': 'l1', 'C': 16.988713993430693, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,443] Trial 50 failed with value None.\n",
      "[W 2023-07-08 13:58:11,443] Trial 51 failed with parameters: {'penalty': 'l1', 'C': 18.50184480941931, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:11,448] Trial 51 failed with value None.\n",
      "[W 2023-07-08 13:58:11,514] Trial 53 failed with parameters: {'penalty': 'l1', 'C': 10.715718731700541, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,519] Trial 53 failed with value None.\n",
      "[W 2023-07-08 13:58:11,518] Trial 54 failed with parameters: {'penalty': 'l1', 'C': 13.250838796412745, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,543] Trial 54 failed with value None.\n",
      "[W 2023-07-08 13:58:11,543] Trial 55 failed with parameters: {'penalty': 'l1', 'C': 11.620252767538624, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,515] Trial 52 failed with parameters: {'penalty': 'l1', 'C': 12.144723421621798, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,573] Trial 56 failed with parameters: {'penalty': 'l1', 'C': 11.93799856244835, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,582] Trial 56 failed with value None.\n",
      "[W 2023-07-08 13:58:11,579] Trial 52 failed with value None.\n",
      "[W 2023-07-08 13:58:11,576] Trial 55 failed with value None.\n",
      "[W 2023-07-08 13:58:11,663] Trial 57 failed with parameters: {'penalty': 'l1', 'C': 14.396135038878683, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:11,668] Trial 57 failed with value None.\n",
      "[W 2023-07-08 13:58:11,668] Trial 58 failed with parameters: {'penalty': 'l1', 'C': 8.208554814702065, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,679] Trial 58 failed with value None.\n",
      "[W 2023-07-08 13:58:11,664] Trial 59 failed with parameters: {'penalty': 'l1', 'C': 14.984033043393971, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,683] Trial 59 failed with value None.\n",
      "[W 2023-07-08 13:58:11,742] Trial 61 failed with parameters: {'penalty': 'l1', 'C': 8.475483844293635, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,747] Trial 62 failed with parameters: {'penalty': 'l1', 'C': 16.741463031871547, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,750] Trial 62 failed with value None.\n",
      "[W 2023-07-08 13:58:11,749] Trial 60 failed with parameters: {'penalty': 'l1', 'C': 13.698112749290814, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,761] Trial 60 failed with value None.\n",
      "[W 2023-07-08 13:58:11,747] Trial 61 failed with value None.\n",
      "[W 2023-07-08 13:58:11,841] Trial 65 failed with parameters: {'penalty': 'l1', 'C': 10.84706244817028, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:11,843] Trial 63 failed with parameters: {'penalty': 'l1', 'C': 13.61052419029815, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,858] Trial 63 failed with value None.\n",
      "[W 2023-07-08 13:58:11,852] Trial 65 failed with value None.\n",
      "[W 2023-07-08 13:58:11,846] Trial 64 failed with parameters: {'penalty': 'l1', 'C': 2.6498524635420395, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,926] Trial 64 failed with value None.\n",
      "[W 2023-07-08 13:58:11,962] Trial 67 failed with parameters: {'penalty': 'l1', 'C': 12.542834357243505, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,967] Trial 67 failed with value None.\n",
      "[W 2023-07-08 13:58:11,970] Trial 66 failed with parameters: {'penalty': 'l1', 'C': 15.611455715945297, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,976] Trial 66 failed with value None.\n",
      "[W 2023-07-08 13:58:11,975] Trial 68 failed with parameters: {'penalty': 'l1', 'C': 6.871492669646173, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:11,982] Trial 68 failed with value None.\n",
      "[W 2023-07-08 13:58:12,027] Trial 69 failed with parameters: {'penalty': 'l1', 'C': 13.200786175140141, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:12,056] Trial 69 failed with value None.\n",
      "[W 2023-07-08 13:58:12,082] Trial 71 failed with parameters: {'penalty': 'l1', 'C': 14.716200706937464, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,086] Trial 71 failed with value None.\n",
      "[W 2023-07-08 13:58:12,084] Trial 70 failed with parameters: {'penalty': 'l1', 'C': 12.397822552507435, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,099] Trial 72 failed with parameters: {'penalty': 'l1', 'C': 14.166862317947377, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,114] Trial 70 failed with value None.\n",
      "[W 2023-07-08 13:58:12,124] Trial 72 failed with value None.\n",
      "[W 2023-07-08 13:58:12,130] Trial 73 failed with parameters: {'penalty': 'l1', 'C': 15.214014393951754, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,161] Trial 73 failed with value None.\n",
      "[W 2023-07-08 13:58:12,182] Trial 74 failed with parameters: {'penalty': 'l1', 'C': 5.786502099539562, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,225] Trial 74 failed with value None.\n",
      "[W 2023-07-08 13:58:12,208] Trial 75 failed with parameters: {'penalty': 'l1', 'C': 11.489320957287244, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:12,235] Trial 75 failed with value None.\n",
      "[W 2023-07-08 13:58:12,229] Trial 76 failed with parameters: {'penalty': 'l1', 'C': 8.995265012510252, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,261] Trial 76 failed with value None.\n",
      "[W 2023-07-08 13:58:12,319] Trial 77 failed with parameters: {'penalty': 'l1', 'C': 7.816902186718603, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,336] Trial 77 failed with value None.\n",
      "[W 2023-07-08 13:58:12,353] Trial 79 failed with parameters: {'penalty': 'l1', 'C': 12.717585658310817, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,331] Trial 78 failed with parameters: {'penalty': 'l1', 'C': 14.13844448842252, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,412] Trial 79 failed with value None.\n",
      "[W 2023-07-08 13:58:12,417] Trial 78 failed with value None.\n",
      "[W 2023-07-08 13:58:12,420] Trial 80 failed with parameters: {'penalty': 'l1', 'C': 12.690856376802003, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,447] Trial 80 failed with value None.\n",
      "[W 2023-07-08 13:58:12,487] Trial 81 failed with parameters: {'penalty': 'l1', 'C': 13.13645208703744, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:12,491] Trial 82 failed with parameters: {'penalty': 'l1', 'C': 2.9949589692997574, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,510] Trial 81 failed with value None.\n",
      "[W 2023-07-08 13:58:12,513] Trial 82 failed with value None.\n",
      "[W 2023-07-08 13:58:12,521] Trial 83 failed with parameters: {'penalty': 'l1', 'C': 16.22782964212706, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,569] Trial 83 failed with value None.\n",
      "[W 2023-07-08 13:58:12,574] Trial 84 failed with parameters: {'penalty': 'l1', 'C': 14.992888523421307, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,597] Trial 84 failed with value None.\n",
      "[W 2023-07-08 13:58:12,589] Trial 85 failed with parameters: {'penalty': 'l1', 'C': 15.43248086381545, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,614] Trial 85 failed with value None.\n",
      "[W 2023-07-08 13:58:12,663] Trial 88 failed with parameters: {'penalty': 'l1', 'C': 25.879081361313972, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,677] Trial 88 failed with value None.\n",
      "[W 2023-07-08 13:58:12,679] Trial 86 failed with parameters: {'penalty': 'l1', 'C': 6.267867079125483, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:12,686] Trial 87 failed with parameters: {'penalty': 'l1', 'C': 23.97531116475173, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,716] Trial 87 failed with value None.\n",
      "[W 2023-07-08 13:58:12,715] Trial 89 failed with parameters: {'penalty': 'l1', 'C': 11.5378913637687, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,688] Trial 86 failed with value None.\n",
      "[W 2023-07-08 13:58:12,745] Trial 89 failed with value None.\n",
      "[W 2023-07-08 13:58:12,755] Trial 90 failed with parameters: {'penalty': 'l1', 'C': 13.906144094298625, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,794] Trial 90 failed with value None.\n",
      "[W 2023-07-08 13:58:12,819] Trial 91 failed with parameters: {'penalty': 'l1', 'C': 13.48519204847032, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,869] Trial 91 failed with value None.\n",
      "[W 2023-07-08 13:58:12,920] Trial 93 failed with parameters: {'penalty': 'l1', 'C': 15.355119051344126, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,922] Trial 94 failed with parameters: {'penalty': 'l1', 'C': 13.150726163982876, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:12,926] Trial 94 failed with value None.\n",
      "[W 2023-07-08 13:58:12,924] Trial 93 failed with value None.\n",
      "[W 2023-07-08 13:58:12,924] Trial 92 failed with parameters: {'penalty': 'l1', 'C': 17.920165774776052, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:12,941] Trial 92 failed with value None.\n",
      "[W 2023-07-08 13:58:13,003] Trial 95 failed with parameters: {'penalty': 'l1', 'C': 23.28215733429445, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,004] Trial 97 failed with parameters: {'penalty': 'l1', 'C': 9.860766981229427, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,009] Trial 96 failed with parameters: {'penalty': 'l1', 'C': 13.425811683246756, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,014] Trial 96 failed with value None.\n",
      "[W 2023-07-08 13:58:13,012] Trial 97 failed with value None.\n",
      "[W 2023-07-08 13:58:13,009] Trial 95 failed with value None.\n",
      "[W 2023-07-08 13:58:13,100] Trial 98 failed with parameters: {'penalty': 'l1', 'C': 13.627226832221394, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,101] Trial 99 failed with parameters: {'penalty': 'l1', 'C': 11.303515626816564, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:13,103] Trial 98 failed with value None.\n",
      "[W 2023-07-08 13:58:13,104] Trial 100 failed with parameters: {'penalty': 'l1', 'C': 13.317520634047739, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,107] Trial 99 failed with value None.\n",
      "[W 2023-07-08 13:58:13,130] Trial 100 failed with value None.\n",
      "[W 2023-07-08 13:58:13,150] Trial 101 failed with parameters: {'penalty': 'l1', 'C': 12.947273082595238, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,169] Trial 101 failed with value None.\n",
      "[W 2023-07-08 13:58:13,229] Trial 103 failed with parameters: {'penalty': 'l1', 'C': 13.107251956310428, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,250] Trial 103 failed with value None.\n",
      "[W 2023-07-08 13:58:13,238] Trial 102 failed with parameters: {'penalty': 'l1', 'C': 15.303455602127073, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,275] Trial 102 failed with value None.\n",
      "[W 2023-07-08 13:58:13,267] Trial 104 failed with parameters: {'penalty': 'l1', 'C': 15.595757324674226, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,306] Trial 104 failed with value None.\n",
      "[W 2023-07-08 13:58:13,403] Trial 105 failed with parameters: {'penalty': 'l1', 'C': 10.559954122681047, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:13,414] Trial 107 failed with parameters: {'penalty': 'l1', 'C': 14.888584206397645, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,416] Trial 105 failed with value None.\n",
      "[W 2023-07-08 13:58:13,417] Trial 106 failed with parameters: {'penalty': 'l1', 'C': 14.535234900238791, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,446] Trial 106 failed with value None.\n",
      "[W 2023-07-08 13:58:13,419] Trial 107 failed with value None.\n",
      "[W 2023-07-08 13:58:13,492] Trial 108 failed with parameters: {'penalty': 'l1', 'C': 9.74759444383389, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,504] Trial 108 failed with value None.\n",
      "[W 2023-07-08 13:58:13,534] Trial 109 failed with parameters: {'penalty': 'l1', 'C': 10.677838544664901, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,536] Trial 110 failed with parameters: {'penalty': 'l1', 'C': 13.522526083177889, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,543] Trial 110 failed with value None.\n",
      "[W 2023-07-08 13:58:13,539] Trial 111 failed with parameters: {'penalty': 'l1', 'C': 13.951273740178515, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:13,537] Trial 109 failed with value None.\n",
      "[W 2023-07-08 13:58:13,551] Trial 111 failed with value None.\n",
      "[W 2023-07-08 13:58:13,618] Trial 112 failed with parameters: {'penalty': 'l1', 'C': 12.865977302199774, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,639] Trial 112 failed with value None.\n",
      "[W 2023-07-08 13:58:13,642] Trial 113 failed with parameters: {'penalty': 'l1', 'C': 15.69397367031717, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,644] Trial 113 failed with value None.\n",
      "[W 2023-07-08 13:58:13,683] Trial 114 failed with parameters: {'penalty': 'l1', 'C': 13.87964154612293, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,700] Trial 114 failed with value None.\n",
      "[W 2023-07-08 13:58:13,710] Trial 116 failed with parameters: {'penalty': 'l1', 'C': 15.13889508290694, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,715] Trial 116 failed with value None.\n",
      "[W 2023-07-08 13:58:13,712] Trial 115 failed with parameters: {'penalty': 'l1', 'C': 15.185264765512109, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,749] Trial 115 failed with value None.\n",
      "[W 2023-07-08 13:58:13,826] Trial 117 failed with parameters: {'penalty': 'l1', 'C': 13.073815204550458, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:13,841] Trial 117 failed with value None.\n",
      "[W 2023-07-08 13:58:13,919] Trial 119 failed with parameters: {'penalty': 'l1', 'C': 10.031532633608247, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,920] Trial 118 failed with parameters: {'penalty': 'l1', 'C': 12.201403250047846, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:13,933] Trial 118 failed with value None.\n",
      "[W 2023-07-08 13:58:13,931] Trial 119 failed with value None.\n",
      "[W 2023-07-08 13:58:14,012] Trial 120 failed with parameters: {'penalty': 'l1', 'C': 16.385231164309655, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,014] Trial 120 failed with value None.\n",
      "[W 2023-07-08 13:58:14,029] Trial 121 failed with parameters: {'penalty': 'l1', 'C': 14.045958950515008, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,034] Trial 122 failed with parameters: {'penalty': 'l1', 'C': 15.48642555227918, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,067] Trial 122 failed with value None.\n",
      "[W 2023-07-08 13:58:14,057] Trial 121 failed with value None.\n",
      "[W 2023-07-08 13:58:14,121] Trial 123 failed with parameters: {'penalty': 'l1', 'C': 12.076345350013192, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:14,129] Trial 123 failed with value None.\n",
      "[W 2023-07-08 13:58:14,134] Trial 124 failed with parameters: {'penalty': 'l1', 'C': 10.704484413682307, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,144] Trial 124 failed with value None.\n",
      "[W 2023-07-08 13:58:14,145] Trial 125 failed with parameters: {'penalty': 'l1', 'C': 19.99900474970218, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,181] Trial 125 failed with value None.\n",
      "[W 2023-07-08 13:58:14,171] Trial 126 failed with parameters: {'penalty': 'l1', 'C': 13.214225756279923, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,225] Trial 127 failed with parameters: {'penalty': 'l1', 'C': 7.646913628223675, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,228] Trial 126 failed with value None.\n",
      "[W 2023-07-08 13:58:14,230] Trial 128 failed with parameters: {'penalty': 'l1', 'C': 8.047498130982717, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,259] Trial 128 failed with value None.\n",
      "[W 2023-07-08 13:58:14,231] Trial 127 failed with value None.\n",
      "[W 2023-07-08 13:58:14,313] Trial 129 failed with parameters: {'penalty': 'l1', 'C': 13.678234404342078, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:14,330] Trial 130 failed with parameters: {'penalty': 'l1', 'C': 12.005242256126829, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,347] Trial 129 failed with value None.\n",
      "[W 2023-07-08 13:58:14,351] Trial 130 failed with value None.\n",
      "[W 2023-07-08 13:58:14,356] Trial 131 failed with parameters: {'penalty': 'l1', 'C': 14.817207319129707, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,380] Trial 131 failed with value None.\n",
      "[W 2023-07-08 13:58:14,421] Trial 132 failed with parameters: {'penalty': 'l1', 'C': 14.876948910720124, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,424] Trial 132 failed with value None.\n",
      "[W 2023-07-08 13:58:14,443] Trial 133 failed with parameters: {'penalty': 'l1', 'C': 12.116379159334722, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,475] Trial 133 failed with value None.\n",
      "[W 2023-07-08 13:58:14,478] Trial 134 failed with parameters: {'penalty': 'l1', 'C': 11.880683033117489, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,496] Trial 134 failed with value None.\n",
      "[W 2023-07-08 13:58:14,501] Trial 135 failed with parameters: {'penalty': 'l1', 'C': 16.355105246271005, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:14,510] Trial 135 failed with value None.\n",
      "[W 2023-07-08 13:58:14,575] Trial 136 failed with parameters: {'penalty': 'l1', 'C': 7.405467413636699, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,589] Trial 136 failed with value None.\n",
      "[W 2023-07-08 13:58:14,614] Trial 137 failed with parameters: {'penalty': 'l1', 'C': 16.495384838834184, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,635] Trial 137 failed with value None.\n",
      "[W 2023-07-08 13:58:14,638] Trial 138 failed with parameters: {'penalty': 'l1', 'C': 14.758602701250787, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,691] Trial 138 failed with value None.\n",
      "[W 2023-07-08 13:58:14,746] Trial 139 failed with parameters: {'penalty': 'l1', 'C': 10.923655850062346, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,785] Trial 139 failed with value None.\n",
      "[W 2023-07-08 13:58:14,773] Trial 141 failed with parameters: {'penalty': 'l1', 'C': 14.842213418803414, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,779] Trial 140 failed with parameters: {'penalty': 'l1', 'C': 7.371377105905105, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:14,794] Trial 141 failed with value None.\n",
      "[W 2023-07-08 13:58:14,798] Trial 140 failed with value None.\n",
      "[W 2023-07-08 13:58:14,855] Trial 144 failed with parameters: {'penalty': 'l1', 'C': 14.524585694270195, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,875] Trial 144 failed with value None.\n",
      "[W 2023-07-08 13:58:14,871] Trial 143 failed with parameters: {'penalty': 'l1', 'C': 13.000749856457752, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,879] Trial 142 failed with parameters: {'penalty': 'l1', 'C': 12.080536879708433, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,913] Trial 142 failed with value None.\n",
      "[W 2023-07-08 13:58:14,910] Trial 143 failed with value None.\n",
      "[W 2023-07-08 13:58:14,908] Trial 145 failed with parameters: {'penalty': 'l1', 'C': 10.37929819530126, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,944] Trial 146 failed with parameters: {'penalty': 'l1', 'C': 12.618864069858944, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:14,969] Trial 146 failed with value None.\n",
      "[W 2023-07-08 13:58:14,963] Trial 145 failed with value None.\n",
      "[W 2023-07-08 13:58:14,988] Trial 147 failed with parameters: {'penalty': 'l1', 'C': 15.649955356461568, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-08 13:58:15,023] Trial 147 failed with value None.\n",
      "[W 2023-07-08 13:58:15,030] Trial 149 failed with parameters: {'penalty': 'l1', 'C': 14.847422783754155, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:15,034] Trial 149 failed with value None.\n",
      "[W 2023-07-08 13:58:15,033] Trial 148 failed with parameters: {'penalty': 'l1', 'C': 11.396588744766412, 'loss': 'hinge'} because of the following error: ValueError(\"Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_118123/266359131.py\", line 10, in objective_fun\n",
      "    lin_svc.fit(X_train, y_train)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "[W 2023-07-08 13:58:15,038] Trial 148 failed with value None.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2023-07-08 13:58:17,260] Trial 27 finished with value: 0.5068870523415978 and parameters: {'penalty': 'l2', 'C': 94.805366428913, 'loss': 'squared_hinge'}. Best is trial 11 with value: 0.5564738292011019.\n"
     ]
    }
   ],
   "source": [
    "# objective function to be minimized\n",
    "def objective_fun(trial):\n",
    "\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    C = trial.suggest_float('C', 0.01,100)\n",
    "    loss = trial.suggest_categorical('loss', ['hinge', 'squared_hinge'])\n",
    "    \n",
    "    lin_svc = LinearSVC(loss=loss, penalty=penalty, C=C)\n",
    "\n",
    "    lin_svc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lin_svc.predict(X_val)\n",
    "    error = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 150, n_jobs = -1, catch=(ValueError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 13.232241053884712, 'loss': 'hinge'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62        96\n",
      "           1       0.49      0.55      0.52        94\n",
      "           2       0.43      0.53      0.48        45\n",
      "           3       0.70      0.33      0.45        95\n",
      "           4       0.35      0.44      0.39        96\n",
      "           5       0.33      0.31      0.32        48\n",
      "           6       0.40      0.26      0.32        95\n",
      "           7       0.33      0.35      0.34        48\n",
      "\n",
      "    accuracy                           0.45       617\n",
      "   macro avg       0.45      0.44      0.43       617\n",
      "weighted avg       0.47      0.45      0.44       617\n",
      "\n",
      "Accuracy 0.44894651539708263\n",
      "F1-score [0.62280702 0.51741294 0.47524752 0.44604317 0.38888889 0.32258065\n",
      " 0.31847134 0.34343434]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "lin_svc = LinearSVC(**best_params)\n",
    "lin_svc.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "\n",
    "y_pred_test = lin_svc.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAADoCAYAAACnz4zGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABT7ElEQVR4nO3dd1gU19fA8e/SBQQEpdjrgiXGbtQYe2+gxhJLTIzRqFGjMXaNBUvsvZfYNVasMaix1/yMLbao2OggvS7c9w9eNqCo1JmNuZ/n8ZFtM2dnh8PsnTvnaIQQAkmSJEkxRmoHIEmS9F8jE68kSZLCZOKVJElSmEy8kiRJCpOJV5IkSWEy8UqSJClMJl5JkiSFycQrSZKkMJl4JUmSFCYT77+Eq6srrq6uxMfHp7v/+fPnuLq60rhxY5Uigxs3btCjRw+qVq1KlSpVaNasGXPnzgVg9OjRuLq6snr16nSvmTBhAq6urixZsgSA0NBQJk+eTKNGjahUqRIff/wxQ4YMITIyUtH34uXlRcOGDSlfvjz16tXL0bJ69eqFq6srly5dyqXo8sadO3dYvHgx3t7e73xu48aNcXV15fnz5wpE9v4yUTsAKWfs7e2ZN28e+fLlU3zdOp0OY2NjBg4cSHR0NEOHDsXa2ppHjx4RGhoKgIeHB3v37sXLy4t+/foBkJCQwNGjR9FoNLi7uxMeHk7Xrl15+vQp9erVo3///kRFRXH06FHCw8PJnz+/Yu9pxYoV+Pn5MWTIEFxdXXO0rIEDB9KtWzfKli2bS9HljTt37rBkyRI8PDxo2rRphs8RQiCEYPz48cTGxmJvb69wlO8ZIf0raLVaodVqRVxcXLr7nz17JrRarWjUqJEQQojdu3cLrVYrvvzyS/HVV1+JqlWrCg8PD+Hj4yOEECIpKUmsXLlSNGvWTFSuXFm0atVK7N69W7+80aNHizp16oiKFSuKunXrilGjRonIyEghhBCLFi0SWq1WDB06VHTt2lVUrFhRhISECK1WKz7++GPx8OHD1+JOTk4WjRs3FlqtVty+fVsIIcSRI0eEVqsVvXr1EkIIsWTJEqHVakX37t1fe31SUlKG22Pv3r2iQ4cO4sMPPxS1atUSK1euFEIIERISIkaPHi3q1asnqlSpIrp06SLOnz+fblvVr19fTJ48WdSuXVt88skn4sSJE0IIIRo1aqTfzlqtVvTs2VNcvHhRaLVa8emnn2a4vZ8+fSp69uwpqlatKipVqiSaNWsmvLy8hBBC9OzZU2i1WnHx4kUhhBB37twRX375pahRo4aoXbu2GDhwoHjy5EmmPrdXjRo1Smi1WjF69GjRtm1bUaVKFTFv3jyxd+9eUbduXVG7dm2xY8cO/fO/+OILUatWLVGxYkVRv359MW3aNKHT6fTrTftv0aJF6eL54osvROXKlcWzZ8/02+jZs2fi4MGDQqvVim+++UYIIcT8+fOFVqsVc+bMyTBm6R9yqOE9dfHiRWrUqEGtWrW4ffs2y5cvB2Dt2rXMnTuXsmXLMmjQIAoUKMCYMWM4c+YMAFqtliFDhjBmzBg++ugj9u7d+9owwfHjx2nYsCE//PAD9vb2uLm5ERgYSKtWrahduzbDhg3j5s2bAPqjWoD9+/en+79jx44AXL9+HYBmzZq99j6MjF7fRX/99VdGjRpFcHAw3333Hd9++y0WFhYAjBw5kj179vDxxx8zfPhwHj58SP/+/Xn06JH+9QEBAcTFxdGpUyf8/f2ZOnUqAOPHj6dAgQL6nwcOHPjO7bxx40YuX75Mjx49+PHHH2nevDlJSUmvPS8iIoK+ffty4cIFvvzyS7p06YK3tzdff/01iYmJ+ue96XN7k4sXL/LZZ5+h0WhYsWIFGzdu5OuvvyY8PJxp06YRFxcHQJUqVRgxYoR+6Gfjxo3s3r2bmjVr0q1bNwBq1qzJvHnzaNGihX75586do0KFCowePfq1bx5t2rSha9euHD9+nMmTJ7Nq1Spq1qzJsGHD3rnd/uvkUMN7KvUr+7lz5zh58iRPnjwB4NixY0BK8jx+/Lj++adOnaJevXo8ffqUffv2ERMTo3/s9u3b6Zbdrl07BgwYoL+9adMmNm7cyJkzZ7h9+zZHjhzh1KlTeHt74+DggLu7O0uXLuXgwYP069ePM2fOYGVlle4XPCuOHDkCwLBhw+jcubP+/piYGM6dO4eFhQXTpk3DxMQEHx8fNm/ezOnTp/Vfo62trZk6dSrJycmsWbOGFy9ekJiYSOPGjbG0tOTly5c0atSIokWLvnN8tkyZMgBcuHCB2NhYKlWqRMuWLV973rVr1wgODqZevXp88803AJw8eZL79+/z4MED/fPe9Lm9Sbdu3ejevTv79+/n2rVrfPnll7Rt25Z169bh7+9PQEAAjo6OPHr0iNWrV5OQkKB/7e3bt+nSpQsffvgh27dvp2jRorRp0waAW7duAVC7dm2+//77N65/3Lhx/Pnnn2zdupUCBQowd+5cjI2N3xqzJE+uvbdSx+BMTFL+tr56FDZhwgTWr1+v//fpp59y/vx5tm7dSsGCBVm6dCkTJkwAeO2EnouLi/5nIQT58uVj8ODB7Nixg/Pnz1OsWDFiYmJ4/PgxAMWKFaNmzZoEBwczfvx4EhMTadmypX5cukqVKgDp/hCkSk5Oztb712g06f5Py9bWFmNjY0xNTd+5ntQkotPpAAgPD0/3eLdu3di6dSvNmjUjMDCQUaNGvTVRvcu7PrdX2djYAOjfS+rttHF7eXlx5MgRXF1dWblypT7xp36uGW2jVGk/64zExMTot0lsbOxr20fKmDzi/ZdZsmSJ/ut30aJFqVOnTpZe37x5c27cuMGePXvo3r070dHRnDt3jjZt2lCwYEEg5RcyJCSEo0ePvnN5MTExNG/enDZt2lC2bFmio6MJCQnBwsKC0qVL65/n4eHB5cuXOXnyJPDPMANAjx492Lt3L1euXKFv3740b96cmJgYjh49yty5cylatGi6dbZs2ZIjR46wYMECoqOjMTExISkpid69e1OvXj3Onj3L+PHjqVixIvv378fc3JwGDRpkaTulKlasGEZGRjx8+JBDhw6xb9++dI9v2bKFkJAQihYtSuXKlfn111/x9fV9bTlVq1alYMGCXLp0iZUrVxITE8P9+/cpVaoU5cqV4+7du9mKLyvi4+MJDAx87Q+cra0tkHIEfPDgQapVq5ap5QkhGD16NP7+/owYMYIlS5YwdOhQdu/ejaWlZa7H/z6RR7z/MqtWrWLFihWsWLECLy+vLL++b9++jBgxgujoaKZOnaofv3V1daVevXp07dqVyMhIVqxYwccff/zO5ZmZmVG/fn1OnTrF9OnTWbRoEaVLl2bRokXpzny3aNFC/8tYokQJatSooX/M1taWHTt28Nlnn/Hw4UOmTp3KqlWrKFSokD4ppNWyZUumT5+un9GxcOFCYmNjAZg9ezYdO3bkzJkzzJ07l9KlS7Ny5UpKlSqV5W0F4OTkxLBhwzA3N2f27Nm4ubmle9zCwoLDhw8zefJklixZQuXKlRk7duxry7GxsWHt2rXUrl2bNWvWsH37dpo0acKqVavSHXnnhQ4dOtC0aVOePn3K+vXrX5t6WK9ePerWrYuPjw8jRozg2rVrmVruunXr+P333+nevTtff/01o0eP5tGjR0yaNCkv3sZ7RSOE7EAhSZKkJHnEK0mSpDCZeCVJkhQmE68kSZLCVEu8Xbp04cCBA+kmj0uSJP0XqJZ4hwwZwpEjR2jcuDELFiwgICBArVAkSZIUpfqsBl9fX7Zv386ePXuoVq0an3/+OdWrV1czJEmSpDyl+hhvREQEwcHBGBkZ4ejoyNSpU5kyZYraYUmSJOUZ1Y54Dx48yObNm4mOjqZXr160b98eCwsLkpKSaNasGSdOnMjWcjXNir77SQqJPXpf7RD0YnRRaoegZ2FsOFc1BccZzhCXnbnhlFo0MzJXOwS9qMQItUPQK2jhnCvLUe2S4YMHDzJkyBDq1q2b7n5jY2PGjx+vUlSSJEl5T5WhhqSkJPLnz/9a0k2lZjcFSZKkvKZK4jU2NsbHx0eNVUuSJKlOtaGG2rVrM2HCBDw8PNJVMnq1CIkkSdL7RrXEe/jwYSClwn0qjUaTYU3WvGKf347js3fob1ua56O0S3EcP63CgLY9+bzZp5QrUoqOk/ux//yvisWV6onPEyaMncjLl2Hkz2/NFM8plC1XRvE4ADxafoqZqSnmFiknXXr37UnTlk0Uj2PW9J84dfI0fr5+bN+1FdfyOeuLlhVLf1rBhdMXCfALZPnWxZR1Tfksnj99wexJ8wgPC8fK2oqRPw6nZJkSisWVanC/IYQEh6AxMsLKypLvxwxXdPukZUj7bkJCAovnLOPyhcuYmZlRVluWSTPUPY+kWuLN7qyF3BQaGUbVAf90QRjRuT8NKn/Ey8gwvP93lu0nvVj3/VzV4ps62ZNOn3aig0d7fvv1NyaOm8jWnVvUi2f2ZLRu5VRbP0DT5k3p8+XnfNGrr+Lrrt+0Hl0+78x3fdMXOl/ouZjWHi1p0b4Zp73PMvvHeSzdtFDx+GbM9SS/TUp7npPevzN53FS27tmseBxgWPvu8oWr0Ghgu9cWNBoNIcEhqsSRlmrzeH19fV/7FxWl7pSnvq26sfbodgCu3PuTx/5PVYslJCSUv279RZt2rYGUhOPvF8DTJ+rFZAiq16iGk7OTKuuuXO0DCjkVTHffy9Aw7t95QNPWKSeE6zepR1BAMC+evV4MPa+lJl0g5XfpLZ0l8pIh7buxMbEc3HuI/t/203facCjooHgcr1LtiLdjx46Eh4frW5zodDosLS1xdnZmzpw5lC9fXtF46lSoTgFrWw5e9FZ0vW8S4O9PwUIF9dtHo9HgXNgZPz9/ipcorkpMU8ZNQwio8EF5Bg7tTwH7AqrEYUiCAoKwL2iPsUlKqx2NRoOjcyEC/QIpUqyw4vFMGjOZq5f/AGDh8nmKrx8Ma9998dwXG1sbNq7ZzJVLf2Bubkbfb76gRm11r45VLfF27tyZ0qVL4+HhgRACLy8v7t+/T7Vq1ZgyZQrbtm1TNJ6+Lbuz8bfdJCW/vcfVf9Xy9UtwdnFCl6hj5ZLVTB0/nXnLZqsdlvSKyTNSuj8c3H+IxfOXsnD5fJUjUldSUhL+vv6ULFOCb4b15/6d+wwb8D2b92zA3kG9C1ZUG2o4e/YsHTt2RKPRYGRkhLu7OxcuXKBp06aKDzlYWVjSpUFb1v26XdH1vo2TszPBQcH6JotCCPx9/XFxyZ0rZ7LK2SXl672JqQlde37K9f9dVyUOQ1PIqRChwaEk6VL+YAshCPQPwtHFUdW42nZowx+X/0dYmPLNJw1p33VydsTIyIjmrZsBoC2vxaWIMw8fPFI8lrRUS7wJCQnp5vL6+Pjou56mNnNUSteG7bn+6C/uPXuo6HrfxsHBnvIV3Dh0IGX2h/cxb5ycHVUZZoiNiSUyIlJ/+7cj3mjdtIrHYYgK2NtR1q0s3odTThafOX6Ogo4Oig8zREZEEhQYpL/9+/FT2NrZYGtro2gcYFj7rl0BO6rXqsal81cA8H3uh98Lf0qWUn7WSVqq1Wrw9vZm3Lhx+nm79+7dY9q0adSpU4fNmzfTv3//bC03O7Uazi3Yx+ojW9nw6079feM+G8KAtr0oZGtPZGw0cQnxVP2mBcHhoZlebk5rNfg89mHC2ImEhYVjbW3FFM/JlNNmb1ZBTmo1vHjuy5jh40lOSkIIKFK0MN+NGoJLkbe3/n6TnNRqmPajJ2dOnyUkOARbO1usLC3xOro/28vLSq2GBZ6LuXT2MqEhL7GxtcHSMh8/71/LM5/nzP5xHhHhEVhaWTJy0neUKpf15po5qdXg5+vH6OHjiI+Px0ijwc7ejqHfD8E1m38gc1qrITf33ZzWanjx3JcZk2YRHhaOkZERffp/TqOm2es6nVu1GlQtCxkSEsL16ylfWatUqZKuK212ySI5GZNFcjImi+RkTBbJydi/vkgOgIODg6zLIEnSf45qiff06dNMnz6d58+fk5SUhBACjUbDnTt31ApJkiRJEaolXk9PT8aPH0/VqlUVP5kmSZKkJtUSr5WVFfXr11dr9ZIkSapR7VCzUaNGeHt7ExUVRVhYmP6fJEnS+061WQ1VqlQhNjb2tfvv3buXo+VGJSo/YfxNaiztpnYIemcHrFc7BD0L43xqh6AXkRimdgh6umSd2iEYJEOa7WFjmjuXyas21GBnZ8e8efNwdVWnbJ0kSZJaVEu8Dg4OnD59mlWrVumvWAPYu3evWiFJkiQpQrUxXp1Ox7NnzwgMDKRHjx7Y2NhQrVo1tcKRJElSjGqJ9+XLl1y9ehU/Pz/mzJnDvXv32L7dcIrUSJIk5RXVhhpCQ0M5fPgw3333HWvWrMHW1paWLVuqFQ4A8fHxjBk5jscPH2Nubo69vT1jJo6iWPFiiqz/+JcbSNAlEpeUMvSy6spOjj+8wLzWoylrX5w4XQIhMWFMPrGEp+F+isSUylDap6j9GaV1+dwV1i7dgE6nw9zCnOHjhurbAeU1Q2pDZEixZMSQWiKlUi3xWltbkz9/ftq3b0+XLl2wtramYsWKaoWj17GzB/Xq10Wj0bBj606mTvRk1YYViq3/uyMzuBv0T8k6M2NTdt48ymmflOpKPT5sx7Rmw+i9a5RiMYFhtU9R+zOClGpgnuNmsmDtXEqVKcmN/93Ec9xM1u9arcj6DakNkSHFkhFDaomUSrWhhm7durFy5UqqVq3KwIED6dixI1999ZVa4QBgbm7Ox5/U07cI+aByJXx9lT2yfFVCUqI+6QJc97tLERtlW98YUvsUQ/mMfJ/5YWNrQ6kyJYGUtkCB/kHcv/NAkfUbUhsiQ4olI4bSEikt1Y54vby8ADh27Jj+vg0bNijaZfhdtm3eQYNGnyi6zlnNv0ejgRv+95l7bj0vY9PPS+5VtQPHH15QNCZDbZ8C6nxGAEWKFyEiPIJbf96mUpWKnPv9AjHRMfj7BqAtr05DUENqQ2RIsYBhtERK6z/dZfht1q1az7Nnz1g+aZli6+z5y0j8IoMwMTJmaN3Pmdl8BP33T9Q/3r9mV0rYFabP7jGKxQSG2z5Fjc8olXV+K36cPYHVi9cRGxtLxcoVKFG6BMbGxorHIr2bobVEUjzxvqutj7W1tUKRvNnG9Zs54X2S5WuWki+fhWLr9YtM6SCgS05i4//2crTPGv1jX1brRLOydfliz1jidPFvWkSeeFv7FLUSr1qfUVpVa1ahas0qQMrJx05Nu1GytDqNSCF9GyJjE2NV2xAZUixpte3QhplTfiIsLBw7O1vV4lA88daoUQONRkNycrJ+nC6tu3fvKh1SOpt/3sKvR46xfPWSdGNDeS2fiTkmxiZExkcD0Ma1IX8FprQi6lPVgzauDfhiz1j940pK2z6lbv2PVG+fotZn9KqQoBAcCqWMdW9avYWqNatQpHgR1eJJ24aoRftmqrUhMqRYIiMiiYuLo5BjIUDdlkhpqVarwcPD47Wr1DK6L6tyUqshwD+A1k3bUaRoEaysUrokmJqZsXFb9uocZKVWQ1EbZxa1HY+xxgiNBp6F+zP91Ap0ycmc+moTT8P8iE6MAVJOuHXd/l2WYslprYbcbJ+Sk1oNuf0Z5aRWw5wp87lx7SZJSclUrFyeIaMGYZ0/+9/YslKrIa/bEGXFf6klUm7ValA88SYkJJCQkMBnn33G1q1b9fdHRkbSp08ffv311xwtXxbJyZgskpMxWSTH8MkiOblg1apVLFmyBI1GQ/Xq6c+Km5sbTp8nSZKkvKL4PN7Bgwdz9+5dChYsyPz58ylbtixeXl506dKFgQMHKh2OJEmS4lS7gMLW1pamTZtiZGREVFQUWq02x8MMkiRJ/waqzeN99uwZQghMTU0ZOnQo1atX5/Hjx2qFI0mSpBjVjnitrKyIiYmhfPnyREZG8scff2Bhoc58TEmSJCWplnjNzMxISkoiMjKS2bNnM3/+fKysrNQKR5IkSTGqDTUkJibSqlUrSpYsSbNmzQgMDCQkJOcVr4LiAnIhutzx57e71Q5B79DT/WqHoNeqWDu1Q9AzpKltFmaWaoeglyyS1A5Bz0jz/l2GrVriLViwIJs2bcLa2hqNRoO1tTUuLi5qhSNJkqQY1RJvcnIyBw4cQKtNuYLk3r17ssCIJEn/CaolXp1Ox/Tp0zE3Nyc5OZnY2FhKlcrbSxslSZIMgWqJ98iRI4SHh/Pnn38CKSfb5DxeSZL+C1RLvJByEUWDBv8UWpk1a5ai6182eyUXT18m0C+QpVsWUsa19FvvV9oTnydMGDuRly/DyJ/fmimeUyhbLu97eiUmJLJ9xi4CnwZhamaKtZ0VHb5tg0Phf7pOPPzzEevGbqJ1v+bU86iT5zGlmjX9J06dPI2frx/bd21VvXeWR8tPMTM1xdwi5XL33n170rRlE8XjMKTtYkh98Qxpu6SlWuJNW/4xOTmZ69evo9MpWySkfpN6fNq7EyO+GpWp+5U2dbInnT7tRAeP9vz2629MHDeRrTu3KLLuWq2qo61ZDo1GwwWvS+yZ70W/2V8AEBcdx6/rvHGtqXynhabNm9Lny8/5oldfxdf9JlNnT0brpk7XiVSGtl0MoS8eGN52SaVa4k1bl8HExIQSJUowc+ZMRWP4oFqlLN2vpJCQUP669RcrVqd0V2javCkzps3i6ZOnFC+Rt8W2Tc1Mca31T9m8Ym5FObPrvP6219LDNOr+CbfP3cnTODJSvUY1xdf5b2BI2yW1L16qDypXYtMGZQ4YXmVI2yWtTCfeffv2Zep57u7umXqeobf+UVuAvz8FCxXExCTlI9JoNDgXdsbPzz/PE++rzu+7RPk6bgDcPHMbjZGG8nXcVEm8hmjKuGkIARU+KM/Aof0pYJ87pQPfF2r1xTNkmU68o0ePzrBjxKsym3h9fV/vNmpjY2MQrX+kf/y+/TQhvqH0ndmbyNBIft92mq9+6qN2WAZj+folOLs4oUvUsXLJaqaOn868ZbPVDstgqNkXz5BlOvEWLpy+ZYe/vz9GRkbY2dkRFhZGUlLSa895m44dOxIeHq4/otPpdFhaWuLs7MycOXMoX758ppf1PnJydiY4KBidToeJiQlCCPx9/XFxcVYshjO7znH73B2+nNEbMwszHl33ISI0isUDU8bqYiJiuHPxHtHhMTTvo/wJJUPg7OIEgImpCV17fkrXdp+pHJHhMIS+eIYq04k37dDAhg0b2LVrFxs3bsTe3p7Q0FB69eqFh4dHplfcuXNnSpcujYeHB0IIvLy8uH//PtWqVWPKlCls27Yta+/kPePgYE/5Cm4cOnCYDh7t8T7mjZOzo2LDDGd3n+f677foO6M3+axTLqt1q61l3PaR+ufsmrMXlzLOis5qMCSxMbHodDp937ffjnijzWZLmfeNofTFM1TZav3z8ccfU79+fWbMmKG/b8yYMZw5c4azZ89mahnu7u6vjRun9lxr164dBw4cyGpYADyOvJ/p5y70XMKVc1f1vaLyWeZj/b5Vb7w/q1wsi2b5NWn5PPZhwtiJhIWFY21txRTPyZTTZu/seVZqNYQHhTOr13zsXQpgns8MAGNTEwYu7JfuedlNvDmp1TDtR0/OnD5LSHAItna2WFla4nU0+3Uo4pJisv3aF899GTN8PMlJSQgBRYoW5rtRQ3Apkr1L3y2Ms1+rIbe3S05qNeR2X7yc1GrI7e1iaZI7Q6HZSrx169YlJiaGIUOGUKpUKR4+fMjixYuxsrLi/Pnzb31tarLeu3cvTZo0wcYmpdtnREQEx48f5/Lly3To0IH9+7O3cbKSePNaThNvbpJFcjKWk8Sb23KSeHObLJKTsdxKvNmaTtahQwfWr1/P7Nn/nEQQQvDZZ+8e38qfP+VrR7169Th8+DAFCxZECEFISAiNGzcmOjqa1q1bZycsSZKkf4VsHfEmJSWxdu1adu/ejb+/P87OznTs2JG+ffvqT5ZlRmhoqP6S4SpVqmBvn/NuovKIN2PyiDdj8og3Y/KIN2OqDjXkhpkzZ5IvXz6MjY3x8/MjKCgINzc3hg8fnqPlysSbMZl4MyYTb8Zk4s1YbiXebHegOHXqFF9//TUtWrQgICCAJUuWcOPGjUy/3svLi+joaIKCgjh8+DBXrlzBy8sru+FIkiT9a2Qr8f72228MGDCA06dP8/TpUxwcHNi6dStr167N9DKcnJwYO3YsFSpUoF+/fpw4cYKIiIjshCNJkvSvkq2TaytWrMDGxoYyZcpw7do1TExMqFatWpaOeBMSEkhISODcuXN8/vnnFChQgMTExOyEk87z6Kc5XkZuKWDu8O4nKaRRYcO5wGHZLcO5iqlxMcO5lLWsjWFUzgKI0UWrHYKeIQ17qDqr4dGjR7Rr1w4LCwuuXbsGgIODA6GhoZleRpEiRahRowYODg7cuHGDtWvXYmZmlp1wJEmS/lWyNdRQoEABHj9+rL+dmJjItWvXKFiwYKaX4eDgQLNmzahduzYPHz6kYsWK7NixIzvhSJIk/atk64i3du3a7Nu3j3v37gHQokUL/Pz8MnXJ8MyZMxk9ejRVq1alcuXK6R5Tuh6vJEmSGrKVeIcPH86lS5f0FcZ8fX1xcnJi6NCh73ztL7/8wujRo5kyZQqOjo7pHtNoNBw/fjw7IUmSJP1rZCvxFipUiEOHDnHs2DH8/PxwdnamWbNmWFlZvfO1JUuWpFevXpibm1O69D8tdYQQmSo7mZu2LtzJ9fM3CfEPZeKa0RQvl9Ka5Nal2+xdexBdog5zCzN6jehOsbLqzMk9sPcg0yZO56cFM2jQpMG7X5BH1Gxxc9jzCLFhsaDRYJrPlLp9PqJgqYKcX3+BJ388JSooio6z3HEombcnMzfM28wfZ68R7B/MjA1TKKktAaS0Stq8eBs3Lt3C1MyU4mWLMfjHAXkaS0YG9xtCSHAIGiMjrKws+X7McNVa3Vw8c4k1S9chkgVJSUl0+7wLLdu3UCWWy+eusHbpBnQ6HeYW5gwfN5SyrnnfQuttspV4e/fuTcuWLdNdIuzt7c2VK1cYM2bMW1/77NkzdDodsbGxnD17Vt/SPSkpCUtLZSeQ12hYlZbdmzHr23n6+6IjY1g97Wd+WDSMIqUKc//636yetoEpG8YrGhuA7ws/9u/2olLlioqvOyNqtbhpMqwx5lYpCf/xZR9OLTtDp9kelKpdksrtK3Ng0kFF4qjdqAbterbmxwGe6e7ftnwnaDTM2zELjUZDWEiYIvG8asZcT30lsJPevzN53FS27tmseBxCCKaNm8HCNXMpoy2D3wt/env04ZMm9bG0UvZ3PDIiEs9xM1mwdi6lypTkxv9u4jluJut3rVY0jldl6+Ta5cuXefLkSbr7Ll68yMaNGzP12itXrtCoUSOmTJnCzZs3uXnzJlOnTqVHjx7ZCSfbtB+Ww94xfbeAoBdBWNtYUaRU4f9/TllCA17y5L6y09SSk5OZPmkGI8YM/8/P9khNugCJMQnw/1+MXCq4YO3w7m9ZuaV8VTccHNNf1h4XG8/vB07TtX9n/Tc2Owc7xWJKK235xaioKFD4G2RaGg1ERaZMSYuJjsbG1gZTM1PF4/B95oeNrQ2lypQEoHK1Dwj0D+L+nQeKx5JWlo54lyxZov/5+vXr+ttCCE6cOIG5ufmbXpqOsbExZ8+eZdmyf+Zzfvrpp7i7uzNixIishJTrHIs6EhURzd+3HlG2Umn+PHeDuJg4gv1DKaFVruXO1o3bqVy1MuUruim2zndRs8XNySWn8PvLD4CWo5srtt53CXgRgJWNNft/PsDNq7cxMzejc193KtVQ51vKpDGTuXr5DwAWLp/3jmfnDY1Gw6RZE5gwYhIW+SyIjIhi6twfMTVVPvEWKV6EiPAIbv15m0pVKnLu9wvERMfg7xuAtrx6DUqznHg1Gg0ajYbr169z/fp1/WNCCGrWrJnpZZmZmeHl5UX79u0BePz4MQkJCVkJJ09YWufjm8lfsWfVfuJi4ylTsRSFSzpjbJztq6uz7OGDh5z87SQrNyxXbJ3vonaLm0aDU8a37596wOUtV2g5Rp3xwlclJyUT7B9MkVKF6T6wC4/vPWH6sJ+YvWU6dva2isczecYkAA7uP8Ti+UtZuHy+4jHodElsWr2FqXMn82H1yty5dZexwyaw/pc12BVQdptY57fix9kTWL14HbGxsVSsXIESpUvohzjVkqXE6+7ujkajYe/evZQpU0Y/HczIyAgXFxe6du2a6WVZW1szcuRIJkyYgJGREfHx8VlqHZSX3KppcauW0kkgMSGRER5jcCmhXMudP/93HT9ffzq37QJASHAoM6bMIjg4hE5dOyoWR1qG0uJG26AcZ1efIy4yDov86reTcXByQGOk4ePmdQEo5VoCR5dCPHv4XJXEm6pthzbMnPITYWHh2NkpG8ff9/4mOCiYD6un5Ifyldwo5FiIB3cfULNODUVjAahaswpVa1YBUq6Y7dS0GyVLK9sw9lVZSryp7defP39Oy5YtczQm+9NPPxEREcHff/8NQNmyZfVF0dUWFhKOnUPKznpw41HcqrniVNTxHa/KPZ26dkyXYL/5YhDdenZRbVaDmi1u4qPj0cXrsLJPGcv1ueKDRX5zzK0zN6yV12zs8lOpegWuX7pJ1bofEugbRKBfEEVKZq8LRXZFRkQSFxdHIcdCAPx+/BS2djbY2ir/O+XoXIiQ4FB8Hj2hZOkSPH/6At/nvhQvWUzxWABCgkJwKJQy42XT6i1UrVmFIsWLqBJLqmzNavjhhx8IDAwkKSkJY2NjkpKSOHXqFI6OjlSqVClTy6hVqxYATZs2zU4IuWLjnK3cvHib8NAI5o9cioWlOTO2Tmb/2oM8uPmQpKQkylQoRZ8flD3pZ2hCQ1++1uJmouc4RdadEJPA8fkn0CUkodFosLCxoMWo5mg0Gs6sOsuza8+ICYvlyPSjmFqY0nVRlzyLZc2s9Vw7f52w0HBmfDeHfJYWLPhlNn1/6MOqGWvZtmwnGiMNX/3QB/tCOa8tnRVRUVGMHj6O+Ph4jDQa7OztmLd0ruJTNAHsHez5fsJwfvxhKkZGGpKTBUNHf4vT/39rUtr65Ru5ce0mSUnJVKxcnh9+zFnp2dyQrXq8bdq0wdraOt0lvj169CAiIiLTvdLi4uJYvXo1Pj4+xMfH6+9PewIvO874e+fo9bnpA/uqaoegZ0iFRtbd2aB2CHqySE7GZJGcjBW2LJEry8nWGaNnz57h6pp+JylTpgxPn2Z+ytX48eNZvXo1J06coGTJkvj6+hrMGK8kSVJeylbitbW15datW/rbQghu3bqVpTHae/fuce7cOaysrPjtt98wNTXlzJkzJCUZzl83SZKkvJCtxFuxYkXu3LlDly5d8PT0pGvXrty5cyfT47sA5ubm5M+fn0KFCrF3714GDRrEkydPaNiwIcuXLycmxnBaskiSJOWmbJ1cGzJkCOfPn+fGjRvcvHkTIQSmpqaZKpKTytbWlvDwcD766CPatWtHaGgo+fPnZ8KECVy6dIm+ffuybdu27IQnSZJk0LKVeCtUqMC2bdvYsmUL/v7+uLi40KNHD9zcMn+V1apVq5g8eTLHjx9Hq9XSrl07vvrqK6ytrWnevDmtWrXKTmiSJEkGT7UuwwCLFi3igw8+oFGjRiQmJiKE0NclCAwMfK1sZGZcDDyV22FmW8n86lZASsvaJP+7n6SQZJLVDkGv19Ef1A5Bb1PLn9QOQS8hKf7dT1JIQKyf2iHoVSyQOzOVMn3Em7YiWe/evTN8jkaj4eeff87U8o4ePcrevXvZtWsXERER+Pn5ce7cOTZt2gSQraQrSZL0b5DpxHv58mXKly+v/zkjWZmsvWrVKvr378+UKVM4duwYAFevXuXQoUO0adMm08uRJEn6t8l04p0xYwZlypTR/5xTRkZGbNiwgZIlS7J06VIAWrduzeLFi2XilSTpvZbpxJu2n1pmequ9i5WVFfHx8ZiYpIRw4cIFChUqRERERI6XLUmSZMgynXjf1VkCUoYapk+f/tbnPHjwgHLlyvH9998zYMAAIiIi6NSpE35+fjRv3hx7e/uUIs6kVDDLS5sXbOfauesE+4cwZd0ESvx/65/rF26ye/U+hBAkJSXTuntzPm5VN09jeZUhtU6Jj49nzMhxPH74GHNzc+zt7RkzcRTFiqtT9MQQWtw0KVaXIVW+YPqVpVzy/5MhH/bBzb4sCUkJxCXFs+bWDv4O91E0JkPYLqkSEhJYPGcZly9cxszMjLLaskyakfddXNbM3cCVM1cJ8g9m7saZlNKWTPf48YO/s3TaCkbNGkHtBpkvY5vbMp149+7di0ajIXUSROp4bmqvtNT/35V4f/jhB/bu3cv06dMJCQkB4Pbt2wBs374dgGXLlqHRaLhz507W31EW1GhYjdaftcBz0D9nk4UQrJy6ltGLvqd42aIE+QUzpudEqjeoRj5LZcoQGlLrlFQdO3tQr35dNBoNO7buZOpET1ZtWKFKLGq3uHHM50Dz4vW5G/pQf99F/2ssubGRZJFMDcfK/FCjP18ff/fBSm5Se7uktXzhKjQa2O61BY1GQ0hwiCLrrdO4Nu692jHu60mvPRboG4j3/uNoK6lXAD1VphNvai1eSPlrduTIEbRaLWXKlOHhw4fcu3ePli1bvnM58fHxHD58mKCgIJYtW8ars9maNFGmgSKAW5U3lDbUaIiJSrlyLjY6Disba0xNszXlOdsMpXUKpFxl+PEn9fS3P6hciU0btqgSC6jb4kaDhsEf9mbVrW18UeGfSmiXA/5pCnAv7BEOFnYYaYxIFspNnTOU1j+xMbEc3HuIfcd26XOGQ8G8bUSaqmLV8hnen5yczLLpq/hqxBdsWLhJkVjeJtPZJLUWL6QUuPnoo49Yt26d/r4vvvgCC4t3HxGOGDGC7du3ExISwvr164mPjycyMhIAGxsbRRNvRjQaDYMm92Px+OWYW5gTHRnDEM8BmCiYeA2pdUpGtm3eQYNG6lb1UqvFTYfSzbgT+pCH4W8uCNWuVBP+CLylaNJNZQitf14898XG1oaNazZz5dIfmJub0febL6hRu7oq8QB4bTuE24eulHEr/e4nKyBbtRoOHz5Mvnz50t2XL18+fv3113e+tkmTJqxevZpOnTrRpUsXnjx5QunSpSldujRPnjzh0KFD2Qkp1yTpkvD6+TDfTvuGebtmMmrBcFZOXUdkWKRiMaRtnbLzyDbmrZyN5/iZhL0MVyyGN1m3aj3Pnj1j8LBBqsYxecYkDh334psh/Vk8f6ki6yyevzB1XKqx88Gb99EGRWrzceEaLL3+7saveUGN7fKqpKQk/H39KVmmBOu2reK7UUOY+MNkQkNCVYnnycNnXDx5mc5f5HxSQG7J1mGcvb09J06coH///pQuXZqHDx9y5swZihYtmulljB8/npYtW7Jr1y6KFUs5SfP8+XO++uorVaeTPf37GS+Dw/TDEKXLl8TesQBPHjyjUs0KisRgaK1TUm1cv5kT3idZvmYp+fKp33YHlG1xU8G+HI6WDixvPA2AAua2FMvfiwLmthx9coqPC9egm7YdEy7OIzxBuT/UGVGz9Y+TsyNGRkY0b90MAG15LS5FnHn44BH2DsoWiAe48+cdAv2CGNR5GABhoeEsn7mal8EvadlJncap2Uq8gwcPZvTo0Zw6dYrTp0/rx2kHDcrcUVDqrAUzMzMKFCigv21nZ6d6K3N7R3vCQ8Lx9fGjcEkXAp4HEvgiCJfiylXPN7TWKQCbf97Cr0eOsXz1knRjiUpTs8XN0SenOPrkn0vSp9X5ngOPvbnk/yf1XGrQw9WdiRfnERyr/JGdIbX+sStgR/Va1bh0/gp163+E73M//F74U7JU7hQRz6qWnZqnS7ATvplM226t/x2zGtJyd3enRIkS7NmzR18kx8PDg6pVM3cdc40aNdBoNCQnJ1O9enV95+Lk5GQ0Go1i08nWz97E9Qs3CQ+NYM6IBVhYWjB7uyd9fujF0kkr0WiMECKZXt91x8FJmZMDYHitUwL8A5g/eyFFihah/5ffAGBqZsbGbesVj8WQWtykNbxaX8LiIxhbc7D+vokX5hKZqEwnB0PbLiMnjGDGpFksX7ACIyMjRk4YQSGnQnm+3uUzV/PHuWuEhYYxZegM8llZsGzXwjxfb1bluEhOQEAATk7ZSwhvq2aW3elkskhOxmSRnIzJIjkZk0VyMqZ4kZy0dDod8+fPZ+vWrcTHx3Ps2DHGjh1Lly5daNu2baaXc/fu3eysXpIk6V8tW4l37dq1rF27Fkg5Mi1atCiBgYH88ssvWUq8Pj4+TJs2jbt375KQkKC//01FeCRJkt4H2ZpOtnfvXkqXLk3r1q3191WsWJEHDx5kaTkjR44kKCiI0NBQkpKSiImJIS4uLjshSZIk/WtkK/EGBARQo0YNChYsqL8vX758We6T9vfff9O/f3+MjY3ZunUr7u7u2NnZZSckSZKkf41sJV4XFxeuXr2qT7SPHj3i999/p0iRIllaTnJyMq1bt8bIyAgrKyvGjx9PaKg6k6wlSZKUkq0x3jZt2rB48WIeP36svw3QvXv3LC3H2tqaly9f4uTkhLu7O+bm5pibm2cnJL1KBark6PW56UnUI7VD0LPJb6d2CHomGsO49BlgZdPXi6mo5XHk32qHoOeUz0XtEPTK2KhTYS0vZeuIt1+/fjRv3hwhhP5fw4YN+eqrrzK9DCEETZs2BeDbb79Fo9GQlJREp06dshOSJEnSv0aW5/EmJSXx4MED8uXLh7m5Ob6+vjg7O1O4cOEsrVgIQbt27Th48CAAfn5+REZGotW+oWJYJkUlql/PIJUhHfGWyq9+KbxURpps/b3PE2EJhjO0ZUjzVQ3piNfOTPnLjN/Ewjh3yrJm+TfA2NiYTp06sWLFCpydnalWrVqWky6kTENzcnIiJCSEgIAAhBBYW1vj6+ub5WVJkiT9m2RrjLdcuXJER+f8UsiwsDDq1auHqalpuksbb9y4keNlS5IkGapsn1xbsGAB8+fPp1atWukK29SsmfnCE0+fPqVHjx4UKFAgO2HkOjVb3Bhyy5JZ03/i1MnT+Pn6sX3XVtXayaR64vOECWMn8vJlGPnzWzPFcwplyyl/ebaa7ZnWzdvIH2f+R5B/MD/97ElJbUoBmkEewzAxM8HMPOV30qN3e+o2/UiRmNIypNZVhrK/pJWtxDt3bkrxjVWrVrFq1Sr9/RqNhr/++ivTy3FycmLChAnp7lP7MmK1WtwYcsuSps2b0ufLz/miV19V1v+qqZM96fRpJzp4tOe3X39j4riJbN2pbEcMtdszfdSoFh16tmFi/6mvPfbd1G/1iVgNam+bVxnC/vKqbJ3lKFy4MC4uLq/9c3Z2ztTro6KiiIqKIjw8nA0bNhASEqK/74cf1CtaktriJnXY44PKlfD1VeaER8Wq5Sno+HoFtLQtS5RuP5Sqeo1qODmrUxntVSEhofx16y/atEu5arJp86b4+wXw9MmbO0LkFTXbM1Wo6oZDBvuLoTCU1lWGtL+kleXf5IsXL9KsWTMcHBzo2rUrtrZZL7KcWgpSCMGMGTOYMWNGukaahsIQWtwYWssStQX4+1OwUEFMTFJ2XY1Gg3NhZ/z8/CleorhicRhye6YlU1YghKBshTL0GNgVmwLK1uQ1pG1jKPvLq7KUeI8dO8awYcP0CXLPnj0cPHhQ/6Yya+zYsfz8888EBgbi6Oiovz9//vz07NkzS8vKK6ktbpZPWqZaDKktS6atMJxJ/lKKtO2ZPqxemTu37jJ22ATW/7IGuwLKdnxIa/Ly8RR0LohOp2P7yl0snbqSMfNGKhqDoW4bQ5KljLlmzRqSk5MpV64cgYGBPHnyhGPHjqUrlpMZn3/+OZ9//jlLly7NdNcKJRlKixtDbFmiNidnZ4KDgtHpdJiYmCCEwN/XHxeXzA1z5RZDbc9U0DmlfoqJiQlturZgaFdlky4Y1rYxlP3lVVka43306BH16tXjwIEDbN68GSEEjx5l/yIBDw8PfH19X/unptQWN8tUbnEDKS1L1h1awcp9S1i5bwnaimX5ZnS//2zSBXBwsKd8BTcOHTgMgPcxb5ycHRX/2pi2PRNgEO2Z4mLjiI78Z5rnud8uUEqFk2yGtG0MZX95VZauXHNzc+Pzzz9nzJgxGd7Oqo8++kg/thsfH09cXBx2dnZcuHAhW8uDnF25FuAfQOum7ShStAhW/3/2NSctbrJy5VraliX5bfJn2LIkJ72icnLl2rQfPTlz+iwhwSHY2tliZWmJ19H92V5eTq9c83nsw4SxEwkLC8fa2oopnpMpp83e+8vJlWveR06wee1WfXumHl92p1nrJtleXlauXFs1cy3/O/8nYaHh5LexxsLSgvELRzN3zEKSk5MRQuBUxJE+3/XC0SXrLXdyeuVabm6bnF65lpv7S25duZblxFunTh1atmwJwKRJk9LdBujatWu2gzl27Bh3795lyJAh2V6GvGQ4Y/KS4YzJS4YzJi8ZzphqifddzfOy0yctrY4dO7Jnz55sv14m3ozJxJsxmXgzJhNvxnIr8Wbp5Fp2ajK8TWo3YUgpvnP9+vVcuRRZkiTJkGUp8Z44cSJXV57a5l0IgbGxMSVKlGDcuHG5ug5JkiRDo86lUP9P7cuDJUmS1KD6YNutW7fYt28fABEREQQGBqobkCRJUh5TNfFu2bKFsWPHsmTJEiClTOSIESPUDEmSJCnPqTrUsHPnTnbu3Em3bt0AKF68OC9fvszRMhOS43MjtFxxwf+i2iHo5TdV9nr9tyloYRgFdwB8Ih+qHYJeaRvDmXly8sVxtUPQa1fCQ+0Qcp2qR7xmZmZYWKS/JNfY2FilaCRJkpSh+BFvr1699HOBnz59SpcuXXjy5Am9e/cmKCiIEiXUqyMqSZKkBMUTb9++KcW0L126RFxcHCEhISQmJnLt2jUsLS3TFVaXJEl6HymeeBs2bAjAsmXL2Lp1K0ZGRjx+/JiEhAQmTpxIsWLqFRmRJElSgmon18LDw/VDDtbW1iQkJBAaGoqvr2+uXyGXFR4tP8XM1BRzC3MAevftSdOW2S98klm6BB0H5hwm5HkoJmYmWNpa0mxAYwq42OF335/ja34nKTEJXaKOSo0rUrtj3pXXW/rTCi6cvkiAXyDLty6mrGtKf6rnT18we9I8wsPCsbK2YuSPwylZRtmhITX7v21esJ1r564T7B/ClHUTKFEu5SDh+oWb7F69DyEESUnJtO7enI9b1VUsLoDwsHCGff3PjKC42Hj8XvjidXIvNrZ5e2I1MUHHLzP3EPQ0GBNzE6xsrWg3uBUOhe1ZN2ojYYHhWFilnMup0qQydT1q52k8aRlav8BUiife1HoPQggqVKjw2uOdOnXKUXWy3DB19mS0bsqfYf6w+QeUql4SjUbD/w79ya9LfqOb56f8usybjz+rQ9laZYiNjGPd4J8pU7MUBYvlTeuX+k3r0eXzznzX9/t09y/0XExrj5a0aN+M095nmf3jPJZuWviGpeQNNfu/1WhYjdaftcBz0E/6+4QQrJy6ltGLvqd42aIE+QUzpudEqjeoRj5L5Wo529rZsn7nGv3tbT/v4M+r1/M86aaq0aoa5WqUQaPRcOnAFfYvPMiXs3oD0Kpfc8rXVSfhGVq/wFSKz2r43//+x9WrVxk8eDDm5ubUqFGDTz75hPr16zNkyBDVk65aTMxMKF2jlP5bQGFXF8IDI4CUdiVx0SnT5BLjEjEyMcbCOu9+qStX+4BCTgXT3fcyNIz7dx7QtHVjAOo3qUdQQDAvnilbP1nN/m9uVbTYO2bQEVujISYqBoDY6DisbKxV64+X6tDew7TxyFqDguwyNTNBW7Osft8t6lqEsADDKFZlSP0C01J877C0TKnuc+LECVxdXdmy5Z9unx07djSIjhRTxk1DCKjwQXkGDu1PAXvl28//ceAaZWunfMVv+W0z9s04wNktF4iNiKHZN02wLmClaDxBAUHYF7TH2CRlup9Go8HRuRCBfoEUKabe0JDaNBoNgyb3Y/H45ZhbmBMdGcMQzwGYqJh4b/55i8iISOp+UkeV9V/cfxm3j7T6279tOMHxTb/jWLwQTfs0wt5F+d8nQ6Pa3hEdHU3ZsmXp06cPxYsXR6fT8fTpU6KiorC2tlYrLJavX4KzixO6RB0rl6xm6vjpzFs2W9EYLv5ymZf+YXQd2AmAy3uuUr9nPSo0cCPMP5zt437BuaxTng01SJmXpEvC6+fDfDvtG9yqaHl0x4cFo5fg+fMk8tup08Hk0N7DtGjXHBMT5efEn9pxlhC/l/T5tg0Anb7vgG0hW4QQXD54lS0/7uDblQMUj8vQqHYBhZOTE97e3ly4cIEdO3awe/duIiMjqVkz690VcpOzS8rXEhNTE7r2/JTr/7uu6Pov7/uD+xf/pvMEd0zNTYmJiOXBxYdUaOAGgJ2zLS6uLry4o+xX/EJOhQgNDiVJlwSkjG0G+gfh6OL4jle+357+/YyXwWG4VUk5witdviT2jgV48uCZKvHExMRy4tjvtHFXZpghrbO7L3Dn3D16TemOmUVKR2HbQinNLTUaDbXb1eSl/0tiImIUj83QqJZ4X758yf79+3F0dGT8+PFs3LiRjz/+OMeF1HMiNiaWyIhI/e3fjnijddO+5RW568r+/3H39D26TO6oH8O1sDLH1MKEJzdSfpFjImLxu+9PweLKHu0WsLejrFtZvA+nlAY9c/wcBR0d/tPDDAD2jvaEh4Tj65NSxDzgeSCBL4JwKa7OuOKJoycoqy1DiVLK9hQ7t+ciN0/d5nPPz8j3//tuUlIyUS//qbl9++wdrOyssLTJnWLi/2aqDTWYmZlhb2+Pubk53bp1Q6PREBAQoFY4AISGvmTM8PEkJyUhBBQpWpiJnsrUB44MjuT39aexdbJl+/hdAJiYGtNzdnfajWzDqQ1nSE5OJlmXTPV2VSnilncJb4HnYi6dvUxoyEvGDJ6ApWU+ft6/lmFjv2X2j/PYtn4HllaWjJz0XZ7F8CZp+78N7D84x/3fsmL97E1cv3CT8NAI5oxYgIWlBbO3e9Lnh14snbQSjcYIIZLp9V13HJzUGQY6tO8IbTu2UXSd4cER/LrGmwLOdqwfsxkAYxNjvpjZk82TdqBL1KEx0mBpY0mPSdlvDZYdau4vb5Ol1j+5qU2bNkRHRxMREcEnn3yCkZERp0+f5urVqzlabmi84ZSV3PNot9oh6DUvZjidiQ2pSM6N0D/UDkFPFsnJmCEVybE0yZ3zT6oNNRgbG7Nz506KFi2KVqslMTERBwd5skiSpPefakMNL1680Fcm27VrF0IIEhIS1ApHkiRJMaod8SYnJ5OQkEBUVBRNmjRhypQpstGlJEn/Caod8To4ONCvXz8CAwM5d+4cR48excnJcMb+JEmS8opqR7wffPABtra22NjYMGzYMHbt2kW+fPnUCkeSJEkxqs1qePLkCXfu3KFixYoEBgZy6dIlypYtS/PmOTv7HpYQkksR5pyZkbnaIejpRKLaIejpkg0nltD4YLVD0HPM56J2CAbpXthttUPQq1no41xZjmpHvMOGDaNRo0aYmZkxfPhw/v77bw4ePKhWOJIkSYpRteeaubk5v//+O127dmXevHn4+PioGY4kSZIiVEu8CQkJJCQkcO7cOWrXVq4wsiRJktpUS7xt2rShXr16+Pn5Ua1aNQIDA+XJNUmS/hNUO7kGEBERgbW1NUZGRkRHRxMVFZXjKWW5cXLtwN6DTJs4nZ8WzKBBkwbZXk5OTq7ldsuSnJ5cG9xvCCHBIWiMjLCysuT7McOzHVNOT67lZnumrJxcWzFnDZdOXybQL4hFm+dRxrUUCfEJzBo3l6ePn2Fubo5tAVsGje5P4WJZP1GW05NrufkZ5VRuxpKVk2sbF2zlf2f/JNg/BM/1kyhRrjiR4VHMGDpH/5yEuAQC/YJYdmA+1jZZuwQ4t06uKT6P18fHh5IlS3L37t0MH1d7Lq/vCz/27/aiUuWKqsZhaC1LZsz1JL9NSn3Zk96/M3ncVLbu2axaPGq0Z/q4cR0693JnZL+x6e5v6dGcGnWrodFoOLDzMIumLWXmymmKxgaG9RmpFUuthtVp81lLpg6cqb8vv6010zf8qL99aOtR7v55P8tJNzcpnnhnzJjBypUrGThw4GuPaTQajh9XrzhHcnIy0yfNYMSY4Syas1i1OCClZYkhSf0lAoiKioL/b/PyX1Kp2ut/jM3MzahZr7r+tlslLXs271Mwqn8Y0mekVixuVd59VH3q0Fm69O+oQDRvpnjiXblyJUIItm3bpvrR7au2btxO5aqVKV/RTe1QDNKkMZO5ejmlmtfC5fNUjcUQ2jNlZP/2g3z0SS3V1m9In5EhxZLq/s2/iY6MpmrdD1WNQ7WTa337GsZX6FQPHzzk5G8n+fLrPmqHYrAmz5jEoeNefDOkP4vnL1UtjuXrl7B598/8vGMtdna2TB0/XbVY0tqxfhd+z/34fHAv1WIwlM/I0GJJdergGT5uWVffO1AtqiRejUaDk5MToaGhaqw+Q3/+7zp+vv50btsF9xYduXXjNjOmzGL3jj1qh2Zw2nZowx+X/0dYmDqdZNVuz5SR3Zv2cf7kRSYvnIiFhfpXLKr9GRliLHExcVw6cYUGbXLnBFlOqFYkx8rKCnd3dxo0aKDvPAwwZswYVeLp1LUjnbr+M+7zzReD6NazS45mNbwvIiMiiYuLo5BjIQB+P34KWzsbbG1tFI8lNiYWnU6nH0NUuj1TRvZu2c+pY2fwXDoZ6/zKdn9OZUifkSHFktbFE1coXrYYhUuof2m2aolXq9Wi1ar7C2PIDKllSVRUFKOHjyM+Ph4jjQY7ezvmLZ2LRoWTN2q2Z1o8fTlXzv3By5CXTBwymXyW+Zi5YiprFmzAuYgTYwZMAMDUzJT5G35SJKZUhvQZqRnL2p828ueFG4SHhjNr+HwsLC2Yt2MGkDLM0LDdJ3keQ2aoOo83L8giORmTRXIyJovkGL73sUiOake84eHhzJ07lwsXLqDRaKhbty7fffcdtra2aoUkSZKkCNVmNYwePRpjY2MWL17MokWLMDY2ZvTo0WqFI0mSpBjVjnh9fHxYvny5/vaECRNo1aqVWuFIkiQpRrUjXkdHx3TTyUJDQw3uggpJkqS8oNoRr62tLe3ataNBg5TpWqdPn6Z69erMmJFyBlKtaWWSJEl5TbVZDUuWLHnr44MHD1YoEkmSJGWpcsSblJSEjY0NvXv3VmP1kiRJqlJljNfY2Jh9+/apsWpJkiTVqXZyrW7dunh5eam1ekmSJNWoNsZbs2ZNIiMjMTU1JV++fAgh0Gg0XL58WY1wJEmSFKNa4n3x4kWG9xcpUkThSCRJkpT13tVqkCRJMnSqzeNt3LhxhtWK1Gz9I0mSpATVEu/KlSv1P8fHx7N//37s7OzUCkeSJEkxBjXU0K1bN7Zv3652GJIkSXlKtelkr3r58iVBQUFqhyFJkpTnVBtqcHd314/x6nQ6/Pz8DK4BpiRJUl5Qbagh7XxdI6OUA+8aNWqoEYokSZKiVEu8ffv2Zf78+RgbG9OuXTsAOnTowNChQ9UIR5IkSTGqjfGGhIRgY2PDqVOnaNy4MUePHsXb21utcAyWq6srERERubrM2rVr8/z5c/r168ejR49yddnvsnjxYo4ePUqrVq3o0KED9+7dy5XlhoWF0a1bNzp06JCuwP6bYvD09NTfzottrLYNGzbk6JxJXm+TkydP0qtXrzxb/rvk5r4fHR2Nq6trll6j2hivTqcD4OrVq9SvXx8zMzOMjY3VCifLdDodJiaqbb5csXr1asXXuWTJEurUqcOgQYNo27ZtrixTp9Nx/vx5rKys5KyY/7dx40Zq165NoUKFXnssOTkZ+GeI798uK7+Lqe9djX0/LdUyR7ly5fjqq6949OgRI0eOJDY2NtfXMWLECB4/fkxiYiIuLi54enoSHx+Pu7s7vXv35vfffycyMpLx48frC7J7e3szd+5cTE1NqV+/Prt27WL37t0ULVqUxo0b06pVKy5dukSJEiWIioqibdu2+qGSs2fPsnDhQn755Zc3xnTt2jV++uknoqOjEUIwdOhQ/vjjDy5fvoxOp8Pa2pqpU6dSunTp117buHFj2rVrx6VLl/Dz82PAgAGYmZmxc+dOgoKCGDFiBG3atHntdcePH2fOnDmYmJhQv379dMtbunQp5cuXZ9myZRw4cAAzMzMAli1bRpEiRd65PVJfD9CxY0dGjRpF7dq1M1xe6tztixcvcvXqVdavX8+kSZOYM2cOUVFRJCcn079/f1q1aoVOp6N///68fPmS+Ph43NzcmDp1KpaWlly6dIkpU6bw4Ycfcvv2bQYMGMBPP/1EZGQkHTp0YNSoUXh5eeHm5kafPn0AmDVrFpaWlnz77bcZfi5bt27F29ub0NBQBg0aRKdOnfSve9Nn4+rqyoABAzh16hSxsbEMGjSI9u3bv/WxtWvX4uPjw9SpUwGIiIigZs2afPPNN5w9e/a19fv4+DB9+nRCQkJISEiga9eu9OzZU7+OK1euYGNjA6R8k9m9ezf79u0jMDCQYcOGYWFhwcyZM/H29ub+/fvExMTg5+fH+vXr2bBhQ6b2u4zExcUxevRo7t+/j4mJCQULFmTWrFkMHz6c6Oho4uPjqV27NuPHj8fIyIjExEQ8PT05f/48NjY26c7nZLSs/v37M336dPbv3w/A/fv3GTBgACdOnOD58+e4u7vTtWtXzp8/T4cOHYiMjOT+/fuEh4cTGBhIyZIlmTFjBgUKFGDx4sWvvffu3bu/c9+/ceNGhvsmwPbt21m3bh2WlpY0a9YsU9ssHaGSuLg48dtvv4mnT58KIYTw9/cXp06dytV1hISE6H9euXKlmDBhgnj27JnQarXi6NGjQgghTp06JZo3by6EECI4OFjUqlVL/P3330IIIXbt2iW0Wq149uyZEEKIRo0aibFjx4rk5GQhhBBnz54VXbt21a9jwIABYu/evW+M5+XLl6JOnTriypUrQgghkpKSxMuXL9PFefDgQfHll1/qb2u1WhEeHq5fv6enpxBCCB8fH/HBBx+IpUuXCiGEuH79uqhdu/Zr60x9Tw8ePBBCCLF9+3b9e2rUqJH466+/RFhYmKhevbqIjY0VQggRExMj4uLiMrU9/vrrL/26PDw8xMWLF9+4vNT3061bN/Hbb7+J8PBw0aFDBxEQEKD/vBo0aCD8/f1FcnKyCA0NFUIIkZycLCZOnChWrlwphBDi4sWLwtXVVVy6dEm/7t27d4tvvvlGf3vUqFFi/fr1+tszZ84UixYtEkIIsWjRIjFt2rR023jt2rVCCCH+/vtvUaVKFZGYmKiP6W2fzfz584UQQjx9+lTUrFlTv23e9Fh4eLj46KOP9J/p+vXr37h+nU4nPDw89Ns/JiZGtG3bVly/fl2/jtTlCCFErVq13vjZLFq0SNSrV08EBQXp78vsfpeRY8eOpXv+y5cvRVxcnIiKihJCCKHT6cTXX38tDh48KIQQYvPmzaJ3794iPj5exMfHi549e4qePXu+cVkXL14U7du3199379490ahRIyGE0P8Op/1dW7Rokfjoo49EYGCgEEKISZMmifHjx7/xvb9r33/bvnnv3j1Rt25d/WNz584VWq32jdsqI6od8Zqbm9O0aVP9bScnp1zvuXbgwAH2799PQkIC8fHxFChQQL/u5s2bA1C1alWePXsGwJ9//olWq6VMmTIAeHh4MGnSpHTL9PDw0E+Dq1evHtOnT+evv/7C1taWmzdvsnDhwjfG8+eff1KqVCn9X3sjIyPs7Ow4cOAAmzdvJjo6muTkZMLDw9+4jNatWwNQokQJzM3NadGiBQAffPAB4eHhRERE6I+A0r6nsmXLAtC5c2f90VYqa2trSpQowciRI6lXrx4NGzbE2dmZs2fPvnN7ZORNy3vVtWvXePbsGf369Ut3/6NHjyhUqBAbNmzg1KlTJCUlERkZSdWqVfXPKVasGLVq1XpnLJmV+q2lTJkymJiYEBwcjLOzM+fOnXvrZ/Ppp5/q46lRowZXr16laNGib3zM3d2dFi1asHv3bvr06cO2bdveuP6oqCj+/vtvhg8frl9fdHQ0Dx8+pHLlyll+jw0aNKBgwYL62+96b2/j5ubGw4cP+fHHH6lVqxaffPIJycnJzJkzhz/++AMhBKGhoZQrV442bdpw4cIF3N3d9UeVnTp1Yvfu3W9c1ruYmprqv12katiwoX5opWvXrum62Lz63lO9aV+9ePHiG/fNBw8e8Mknn+Do6AhA9+7d012Jmxn/7kHKt7h69SqbNm1ix44dODg4cPz4cRYtWgSAmZmZPnkaGRmRlJSU6eVaWVmlu92rVy82bdpEwYIF6dSpk37HyixfX1+mTp3Krl27KF68OHfv3tV/lcyIubm5/mcjIyP9bY1Gg0aj0Y+dv0lG9TGMjY3ZuXMn165d49KlS3Tp0oV58+a9M3ZjY+N02y4hIeGty3t1uqAQgnLlymU4Lrt//34uXbrE5s2bsba2ZuPGjVy8eFH/uKWl5TtjSx3Pg5TL0t/2mle3q06ny/Jnk1m9evVi4MCBlC5dmgIFCuDj45Ph+oUQ2Nra6r9uZ/QeM9r+b5L2/ef0vRUrVoxDhw5x8eJFLly4wOzZs+ncuTMhISH88ssvmJubM2PGDOLj4zN8fdr9MKNlzZ49+7XPLy0LC4t3jlGnXcebPvs37atv2zcfPHjwxvVk1vsxup6BiIgIrKyssLOzIyEhgR07drzzNVWqVOH+/fv6s51eXl4kJia+9TUdOnTg7Nmz7Nmzh27dur31uVWrVuXJkydcvXoVSBnof/78OSYmJhQqVAghBFu2bMnkO8ycqlWrcv/+fR4+fAjA7t27X3tPUVFRBAcHU6NGDQYNGkT16tX566+/3rk9ihcvzvXr1wG4ceMGjx8/fuvyIOUPV2qyqFq1Ks+fP+f8+fP6Zd65c4eEhAQiIiKws7PD2tqaqKgo9u7dm6X3XaJECW7cuAGkXBV56tSpLL0eIDIy8p2fzZ49ewB4/vw5f/zxR7o/Lm96rEyZMhQtWpSJEye+NdmVKlUKa2tr/ZEhwJMnTwgLCwNStn/qezx27BgxMTH651lZWREZGZmj9/Y2/v7+aDQamjRpwg8//IAQgr/++otChQphbm5OUFAQR48e1T+/Tp06+v0nISFBv23etCwhBC9evNB3In/TH5+0Tp06RXBwMAC//PILdevWfedr3rSvvm3f/Oijjzhz5ox+1kjqt5aseG+PeOvXr4+XlxctW7bEzs6OunXrEhAQ8NbXODg4MG3aNAYNGoSZmRl169bF0tIy3Vf3V+XLl4/mzZsTGBiIi4vLW5dva2vLkiVLmDlzJtHR0RgZGTF06FBat25NmzZtsLOzSzf8khvs7e3x9PRk8ODB+hNkrxYjioqKYsiQIfoTnCVLlsTDw4P8+fO/dXsMGzaM0aNHs2PHDqpUqaIfznjT8gC+/PJLVq1axfTp0/nll19YuXIls2bNYubMmeh0OlxcXFi2bBnu7u4cP36cFi1aYG9vT/Xq1fH19c30++7SpQtDhgyhVatWFCtWjCpVqmR527m6ur7zs0lKSsLd3Z3Y2FjGjRunH2Z412NdunRh6tSptGjRghEjRmS4fhMTE1auXMn06dPZsGEDycnJFChQgLlz5wIpnbg9PT1ZsGABDRo0SPe59u7dmwkTJuhPrmXnvb3NvXv39EeGSUlJdOjQga5duzJkyBDatGmDo6NjusTXpUsXHjx4QJs2bfQn127fvv3GZdWsWZOvvvqKzp07U7BgwUwNP9SoUYMRI0YQEBCgP7n2Lm/b99+0b2q1WgYPHkyPHj3+fSfXDFVkZKT+599++020bNnyrc/X6XSiffv2+hNm75usbo//kredgHrXyanJkyeLJUuW5FVo/zmvnjA1dO/tEW92bd68mcOHD5OcnIy1tTVz5sx543OPHz+Op6cn9evXf28vd87K9pDeLSAggM8//xw7OzvWrFmjdjiSSgyqLKQkSdJ/wXt7ck2SJMlQycQrSZKkMJl4JUmSFCYTryRJksJk4pUkSVKYnE4m/es1btyYFy9evPHxwYMHv7EqmSSpQSZe6V+vY8eO+gIvR44cISgoiA8//JAPP/wQQP9/qsTERExNTRWPU5JSyXm80nulS5cuXL9+XX+U26tXLy5fvkz//v35448/uH79OgsWLMDb25u9e/fi4eGhv6Q29cg5tYh4fHw8a9as4eDBg/j5+eHk5ISHhwd9+/aViVvKETnGK/0nrFq1CnNzczp06PBarYo3GTlyJIsWLUIIQdu2bTEyMmL+/PmZqtwmSW8jE6/0n9C6dWvWrVuHp6dnpi7v9vPz49dffwWgevXq5MuXDzc3NyClGlXakoWSlFVyjFf6T8hM0fS0tYzTVkLbtWtXuufFxsYSGBiYYXF3ScoMmXil/4RXC9SnFsZOPSkXGhpKSEiI/vG0SfXw4cP6LhwAz549k0lXyhGZeKX/pIoVKwIpDUpnzpypb/qYqkiRIjRq1IiTJ0/Su3dvGjZsSFxcHLdu3cLR0ZFNmzapFbr0HpBjvNJ/Uvv27enYsSPm5uYcO3aMJk2aULhw4XTPmT9/Pt9++y02NjYcOHCAc+fO4ejoqO+lJknZJaeTSZIkKUwe8UqSJClMJl5JkiSFycQrSZKkMJl4JUmSFCYTryRJksJk4pUkSVKYTLySJEkKk4lXkiRJYTLxSpIkKUwmXkmSJIXJxCtJkqQwmXglSZIU9n+U6S1hjgBVMwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 354.331x236.22 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=default_style.SHORT_HALFSIZE_FIGURE)\n",
    "cf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cf, annot=True, cmap='Greens', fmt=\".4g\", cbar=False, ax=ax)\n",
    "ax.set_xlabel('True')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('LinearSVC confusion matrix')\n",
    "\n",
    "ticks = np.unique(y)\n",
    "ax.set_xticks(ticks + 0.5, labels=label_enc.inverse_transform(ticks))\n",
    "ax.set_yticks(ticks + 0.5, labels=label_enc.inverse_transform(ticks))\n",
    "\n",
    "plt.savefig(\"images/LinearSVC_conf_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354.331x236.22 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boundaries_on_embedding(reducer, lin_svc, embedding=embedding, \n",
    "                        title=\"Linear SVM on PCA\", \n",
    "                        cmap=\"inferno\",\n",
    "                       n_pts=80)\n",
    "plt.savefig(\"images/linear_SVM_UMAP.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_function = lin_svc.decision_function(X_train)\n",
    "# support_vector_indices = np.where((2 * y_train - 1) * decision_function <= 1)[0]\n",
    "# support_vectors = X_train[support_vector_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots the decision function for diferent values of C\n",
    "for i, C in enumerate([0.02, 300]):\n",
    "    plt.figure(figsize=default_style.SHORT_HALFSIZE_FIGURE)\n",
    "\n",
    "    cmap=\"viridis\"\n",
    "    clf = LinearSVC(C=C, loss=\"hinge\", penalty='l2', random_state=42).fit(X_test, y_test)\n",
    "\n",
    "    # Genearate a grid in embedding\n",
    "    xx = np.linspace(np.min(embedding[:,0]), np.max(embedding[:,0]), 30)\n",
    "    yy = np.linspace(np.min(embedding[:,1]), np.max(embedding[:,1]), 30)\n",
    "\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    points_in_embedding = np.array(np.meshgrid(xx, yy)).T\n",
    "    old_shape = points_in_embedding.shape[:-1]\n",
    "    \n",
    "    # Maps them back in the big space\n",
    "    points_in_embedding = points_in_embedding.reshape(-1,2)\n",
    "    points_in_gigaspace = reducer.inverse_transform(points_in_embedding)\n",
    "\n",
    "    # Gets results\n",
    "    results = clf.decision_function(points_in_gigaspace)\n",
    "    mappable=plt.contourf(XX, YY, results.reshape(old_shape).T, cmap=\"viridis\", alpha=0.6, levels=15)\n",
    "    plt.scatter(*embedding[test_mask].T, c=y_test, marker=\"o\", edgecolor=\"k\", s=5,cmap=cmap, label=\"test\")\n",
    "    plt.scatter(*embedding[~test_mask].T, c=y[~test_mask], marker=\"+\",  s=5, cmap=cmap, label=\"train+val\")\n",
    "    \n",
    "    # Plot support\n",
    "    results = clf.decision_function(X_train)\n",
    "    support_vector_indices = np.where((2 * y_train - 1) * results <= 1)[0]\n",
    "    plt.scatter(*(embedding[~test_mask][support_vector_indices]).T, s=10, color=\"r\")\n",
    "    plt.axis(\"off\")\n",
    "#     decision_function = clf.decision_function(X_res_t)\n",
    "# #     we can also calculate the decision function manually\n",
    "# #     decision_function = np.dot(X_pca, clf.coef_[0]) + clf.intercept_[0]\n",
    "#     support_vector_indices = np.where((2 * y_res_t - 1) * decision_function <= 1)[0]\n",
    "#     support_vectors = X_res_t[support_vector_indices]\n",
    "#     support_vectors_pca = pca.transform(support_vectors)\n",
    "\n",
    "#     plt.subplot(1, 2, i + 1)\n",
    "#     plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_res_t, s=30, cmap=plt.cm.Paired)\n",
    "#     ax = plt.gca()\n",
    "#     xlim = ax.get_xlim()\n",
    "#     ylim = ax.get_ylim()\n",
    "#     xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
    "#                          np.linspace(ylim[0], ylim[1], 50))\n",
    "#     clf = LinearSVC(C=C, loss=\"hinge\", random_state=42).fit(X_pca, y_res_t)\n",
    "#     Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "#                 linestyles=['--', '-', '--'])\n",
    "#     plt.scatter(support_vectors_pca[:, 0], support_vectors_pca[:, 1], s=100,\n",
    "#                 linewidth=1, facecolors='none', edgecolors='k')\n",
    "#     plt.title(\"C=\" + str(C))\n",
    "#     #plt.scatter(clf.coef_[:, 0] + clf.intercept_, clf.coef_[:, 1] + clf.intercept_)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "    plt.savefig(f\"images/linear_svm_UMAP_decision_function_C_{C}.pdf\")\n",
    "    plt.colorbar(mappable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for SVM is find hyperplane that maximizes the margin, furthermore C is the inverse of regularization strength therefore smaller value of C correspond a stronger regularization namely greater margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:58:55,353] A new study created in memory with name: no-name-bea899fb-8097-4fbb-8a5e-30f2878ee17d\n",
      "[I 2023-07-08 13:58:56,341] Trial 2 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 37.68714479288209, 'gamma': 0.23855585814096647}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:56,352] Trial 0 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 58.12690707686507, 'gamma': 0.7709585424049129}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:56,524] Trial 1 finished with value: 0.42699724517906334 and parameters: {'kernel': 'sigmoid', 'C': 6.275029475769968, 'gamma': 0.0045922863901682355}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:56,647] Trial 3 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 70.51230461980771, 'gamma': 0.6318992791161503}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:57,407] Trial 4 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 62.4975545676563, 'gamma': 0.9821063901449765}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:57,636] Trial 6 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 68.02770857803915, 'gamma': 0.37705704218726827}. Best is trial 2 with value: 0.6446280991735537.\n",
      "[I 2023-07-08 13:58:58,359] Trial 7 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 7.89367048057067, 'gamma': 0.2621379875818561}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:58:58,483] Trial 5 finished with value: 0.6556473829201102 and parameters: {'kernel': 'rbf', 'C': 41.09460746447622, 'gamma': 0.7812097098151025}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:58:58,734] Trial 8 finished with value: 0.6831955922865014 and parameters: {'kernel': 'rbf', 'C': 30.765519616667355, 'gamma': 0.08752999592718569}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:58:58,999] Trial 9 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 42.19935666389352, 'gamma': 0.8701523812656595}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:58:59,545] Trial 10 finished with value: 0.6528925619834711 and parameters: {'kernel': 'rbf', 'C': 28.130128321923056, 'gamma': 0.03521172411428477}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:58:59,727] Trial 11 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 19.54272031960135, 'gamma': 0.5167548100851173}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:59:00,219] Trial 12 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 67.25931704995989, 'gamma': 0.5248583991166033}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:59:00,869] Trial 13 finished with value: 0.4765840220385675 and parameters: {'kernel': 'rbf', 'C': 0.28716754019393953, 'gamma': 0.46136626519794643}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:59:01,187] Trial 14 finished with value: 0.6473829201101928 and parameters: {'kernel': 'rbf', 'C': 2.283378141127372, 'gamma': 0.25310029093880815}. Best is trial 7 with value: 0.6942148760330579.\n",
      "[I 2023-07-08 13:59:01,471] Trial 15 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 13.407102873021664, 'gamma': 0.20925005754245052}. Best is trial 15 with value: 0.696969696969697.\n",
      "[I 2023-07-08 13:59:01,812] Trial 16 finished with value: 0.6418732782369146 and parameters: {'kernel': 'rbf', 'C': 2.915697661373952, 'gamma': 0.1903460017570412}. Best is trial 15 with value: 0.696969696969697.\n",
      "[I 2023-07-08 13:59:02,678] Trial 17 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 13.293169635385532, 'gamma': 0.2021629573193385}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:02,801] Trial 18 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 14.558394085745235, 'gamma': 0.1617729216215693}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:03,347] Trial 19 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 13.185170318862763, 'gamma': 0.24601930680033535}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:04,124] Trial 20 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 13.30697559871139, 'gamma': 0.33918602087368177}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:04,882] Trial 21 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 15.673251658322544, 'gamma': 0.13410423148708894}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:05,565] Trial 22 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 15.283880037830679, 'gamma': 0.34545126942202053}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:05,945] Trial 23 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 21.759921242174606, 'gamma': 0.3388483004355717}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:06,268] Trial 24 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 21.459624569241697, 'gamma': 0.1512073250479153}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:07,667] Trial 25 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 24.24400090849997, 'gamma': 0.3282289172239585}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:07,752] Trial 26 finished with value: 0.6391184573002755 and parameters: {'kernel': 'rbf', 'C': 8.225365951925365, 'gamma': 0.07570264442648877}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:08,094] Trial 27 finished with value: 0.6639118457300276 and parameters: {'kernel': 'rbf', 'C': 7.7616060222333445, 'gamma': 0.09511289703699277}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:08,365] Trial 28 finished with value: 0.6666666666666666 and parameters: {'kernel': 'rbf', 'C': 7.8530304032935305, 'gamma': 0.09730243468665016}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:09,381] Trial 29 finished with value: 0.628099173553719 and parameters: {'kernel': 'rbf', 'C': 6.925838386636915, 'gamma': 0.08010334093295063}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:10,188] Trial 30 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 9.676819360101437, 'gamma': 0.24489966057681362}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:10,580] Trial 31 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 9.514683350902104, 'gamma': 0.22033810524487007}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:11,020] Trial 32 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 77.14794150019358, 'gamma': 0.19488296446507902}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:11,683] Trial 33 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 53.8742408483184, 'gamma': 0.22945989221042357}. Best is trial 17 with value: 0.699724517906336.\n",
      "[I 2023-07-08 13:59:12,393] Trial 34 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.90982807395262, 'gamma': 0.1965402929355844}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:12,549] Trial 35 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 17.669570416469526, 'gamma': 0.16181484705394522}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:12,796] Trial 37 finished with value: 0.5068870523415978 and parameters: {'kernel': 'poly', 'C': 16.977488429914132, 'gamma': 0.010784311708406408}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:13,167] Trial 36 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 18.181683380896416, 'gamma': 0.14518846238063715}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:59:13,652] Trial 39 finished with value: 0.5895316804407713 and parameters: {'kernel': 'poly', 'C': 3.718131647174685, 'gamma': 0.02945094531431905}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:13,904] Trial 38 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 18.908768657876884, 'gamma': 0.28273618094932956}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:14,375] Trial 40 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 3.8463862945182488, 'gamma': 0.28547734480330944}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:14,603] Trial 41 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 26.43891083980179, 'gamma': 0.2814553561302254}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:15,823] Trial 42 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 25.55314446769473, 'gamma': 0.2853450803750299}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:16,330] Trial 43 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 26.226171650656703, 'gamma': 0.27820758803933276}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:17,224] Trial 45 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 12.565427128189764, 'gamma': 0.18028653234883713}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:17,433] Trial 44 finished with value: 0.6831955922865014 and parameters: {'kernel': 'rbf', 'C': 11.502600138018487, 'gamma': 0.4082562503146925}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:18,320] Trial 46 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 12.11461855943266, 'gamma': 0.19099699971232298}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:18,442] Trial 47 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 11.553964321227596, 'gamma': 0.18940829692212968}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:19,287] Trial 49 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 11.531410651750651, 'gamma': 0.18136543744549388}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:19,818] Trial 50 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 12.360862491008394, 'gamma': 0.18413887210490576}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:19,899] Trial 48 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 10.937211945977669, 'gamma': 0.4174575736969016}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:20,439] Trial 51 finished with value: 0.6804407713498623 and parameters: {'kernel': 'rbf', 'C': 31.338026913507335, 'gamma': 0.12249402903212066}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:21,127] Trial 52 finished with value: 0.4931129476584022 and parameters: {'kernel': 'rbf', 'C': 0.5004654218177933, 'gamma': 0.1279301573281759}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:21,236] Trial 53 finished with value: 0.6639118457300276 and parameters: {'kernel': 'rbf', 'C': 6.195495071708671, 'gamma': 0.11941051372468549}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:21,775] Trial 54 finished with value: 0.4820936639118457 and parameters: {'kernel': 'rbf', 'C': 0.38170367637907354, 'gamma': 0.126837313623733}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:22,496] Trial 55 finished with value: 0.4820936639118457 and parameters: {'kernel': 'rbf', 'C': 0.2155230682435949, 'gamma': 0.2124012402627451}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:22,783] Trial 56 finished with value: 0.6033057851239669 and parameters: {'kernel': 'rbf', 'C': 6.134431639666929, 'gamma': 0.054797278330506155}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:23,280] Trial 58 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.312999458630326, 'gamma': 0.21461708758271805}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:23,432] Trial 57 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 15.199221713603146, 'gamma': 0.2126257725479122}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:24,509] Trial 59 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 14.852003212424194, 'gamma': 0.18315455350554227}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:24,732] Trial 60 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 15.335832058649709, 'gamma': 0.17400718532850704}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:24,877] Trial 61 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 20.993093590240953, 'gamma': 0.21518151475285596}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:25,258] Trial 62 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 21.77545275789374, 'gamma': 0.3204695816145853}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:26,226] Trial 63 finished with value: 0.31955922865013775 and parameters: {'kernel': 'sigmoid', 'C': 18.96674253987942, 'gamma': 0.05450007056913936}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:26,426] Trial 64 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 20.14089394696753, 'gamma': 0.24534628042557452}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:26,921] Trial 65 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 20.048854094163385, 'gamma': 0.3163440283642274}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:27,069] Trial 66 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 10.698268713191656, 'gamma': 0.2492853830695884}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:28,065] Trial 67 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 11.424935647465091, 'gamma': 0.24605207327298403}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:28,071] Trial 68 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 10.208864235284423, 'gamma': 0.1527054075876104}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:28,422] Trial 69 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 11.204709106915734, 'gamma': 0.16481233001195383}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:28,887] Trial 70 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 13.38763810190456, 'gamma': 0.1558490666177802}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:29,429] Trial 71 finished with value: 0.6694214876033058 and parameters: {'kernel': 'rbf', 'C': 14.27436424445385, 'gamma': 0.0940361103569306}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:29,759] Trial 73 finished with value: 0.6694214876033058 and parameters: {'kernel': 'rbf', 'C': 13.512394179262404, 'gamma': 0.09309913150772665}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:29,875] Trial 72 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 14.389984719516825, 'gamma': 0.15939068876876328}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:30,737] Trial 74 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 14.210387324692972, 'gamma': 0.20100013202760755}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:31,218] Trial 75 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 16.766186165885358, 'gamma': 0.19912049928280506}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:59:31,837] Trial 77 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 16.882779655153804, 'gamma': 0.19868340711739949}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:31,890] Trial 76 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 16.649270083908238, 'gamma': 0.1949909873210412}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:32,657] Trial 78 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.585420484629825, 'gamma': 0.2049863177609148}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:33,533] Trial 79 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 16.959538593884524, 'gamma': 0.20993642517952596}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:34,185] Trial 81 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 23.062697678287563, 'gamma': 0.227264534597763}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:34,524] Trial 80 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.3756315686425, 'gamma': 0.23011192841370412}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:35,393] Trial 82 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 17.588953191201636, 'gamma': 0.2653071265246261}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:35,495] Trial 83 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 22.661743490237722, 'gamma': 0.26713746324761306}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:35,863] Trial 84 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 23.790010258374814, 'gamma': 0.2587584149191484}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:36,389] Trial 85 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 17.67651791509131, 'gamma': 0.25524691175954645}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:36,927] Trial 88 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 17.26675844724108, 'gamma': 0.23391426113743857}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:37,061] Trial 86 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 23.16987130242347, 'gamma': 0.23748606515883752}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:37,398] Trial 87 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 22.930853925887654, 'gamma': 0.2352567012553452}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:37,541] Trial 89 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 19.582345149232985, 'gamma': 0.2248665951032899}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:38,748] Trial 90 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.79234865720904, 'gamma': 0.3016831525283055}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:38,811] Trial 92 finished with value: 0.6831955922865014 and parameters: {'kernel': 'rbf', 'C': 20.035836204617066, 'gamma': 0.11445554363930102}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:39,079] Trial 91 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.905773820428752, 'gamma': 0.30708355827878336}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:39,500] Trial 93 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 27.78611641603541, 'gamma': 0.2980868405534371}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:40,613] Trial 95 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.546450942972772, 'gamma': 0.2961002558917253}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:40,716] Trial 94 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 21.315271074223446, 'gamma': 0.2938876243259928}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:40,947] Trial 97 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 16.64194934979136, 'gamma': 0.14138214989111458}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:41,120] Trial 96 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.47049904988145, 'gamma': 0.29786768024885973}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:42,224] Trial 98 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 21.99295970121217, 'gamma': 0.14302893233747319}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:42,718] Trial 101 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 8.718513790517665, 'gamma': 0.19012545883416493}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:42,793] Trial 99 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 16.37264215895522, 'gamma': 0.3543419326788825}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:43,071] Trial 100 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 22.992400970110452, 'gamma': 0.3598204220254846}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:44,129] Trial 102 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 18.317854202425153, 'gamma': 0.1932205601453328}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:44,618] Trial 104 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.38547708189882, 'gamma': 0.22670414553600393}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:44,862] Trial 103 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 18.523146398479152, 'gamma': 0.35088889869484463}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:44,919] Trial 105 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 18.286068569489533, 'gamma': 0.2236475235615066}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:46,206] Trial 106 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.09117605562779, 'gamma': 0.2328788606630048}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:46,398] Trial 107 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 18.383217021676344, 'gamma': 0.171798438821964}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:46,854] Trial 109 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 16.306531805625305, 'gamma': 0.16562805681022297}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:47,301] Trial 108 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 21.155542844110386, 'gamma': 0.32267711718859293}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:48,125] Trial 110 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 15.777901521063658, 'gamma': 0.1677199626305439}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:48,442] Trial 111 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 15.87305057191153, 'gamma': 0.19909299517122853}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:48,971] Trial 112 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 20.780217825279088, 'gamma': 0.2695884069434674}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:49,568] Trial 113 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 25.9132882131696, 'gamma': 0.2704737713087587}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:50,970] Trial 114 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 30.069011320040364, 'gamma': 0.268271477881221}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 13:59:51,184] Trial 115 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 26.574737641217958, 'gamma': 0.3072193731161135}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:51,618] Trial 116 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 28.375477237719906, 'gamma': 0.31388348040451536}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:51,931] Trial 117 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 28.34273641470564, 'gamma': 0.30674873247875045}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:52,811] Trial 118 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 22.735292682052094, 'gamma': 0.3059599566607387}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:53,133] Trial 119 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 13.137875190198567, 'gamma': 0.2008979473420982}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:53,156] Trial 120 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 22.22759543432649, 'gamma': 0.20475947383092036}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:53,580] Trial 121 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 23.068859529367995, 'gamma': 0.20420648287699836}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:54,573] Trial 122 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 22.079553901711634, 'gamma': 0.208277254337265}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:54,832] Trial 123 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 24.452954275909235, 'gamma': 0.23503440839054707}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:55,130] Trial 124 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 24.953592730144738, 'gamma': 0.29105984035435944}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:55,474] Trial 125 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 25.102340887265008, 'gamma': 0.28518389982720604}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:56,319] Trial 126 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 20.322238725237433, 'gamma': 0.28332597159182166}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:56,413] Trial 128 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 12.577738119887343, 'gamma': 0.2528377495946948}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:56,709] Trial 129 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 20.76000643651277, 'gamma': 0.2500533782543061}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:56,796] Trial 127 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 20.704555252859826, 'gamma': 0.28670666572152337}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:57,867] Trial 130 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 12.445345274700472, 'gamma': 0.25338262074328666}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:58,745] Trial 132 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 15.109551379816594, 'gamma': 0.17879580893363717}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:59,077] Trial 131 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 14.823205151042819, 'gamma': 0.329870868078164}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 13:59:59,680] Trial 133 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 14.67613620676867, 'gamma': 0.37760560025406925}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:00,303] Trial 134 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 25.06463884450197, 'gamma': 0.33643213380701564}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:01,053] Trial 135 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 23.993904213047607, 'gamma': 0.37132407766880227}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:01,547] Trial 136 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 16.945483239781165, 'gamma': 0.2277884412817628}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:01,814] Trial 137 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.339492552946353, 'gamma': 0.22608559974590833}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:02,638] Trial 138 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.255434040099065, 'gamma': 0.22591109526566078}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:03,038] Trial 139 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 27.26361072562741, 'gamma': 0.22292106568967984}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:03,447] Trial 141 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 19.29820681788942, 'gamma': 0.13858400653862987}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:04,105] Trial 140 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 27.514016436180025, 'gamma': 0.30252420834502786}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:04,807] Trial 143 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 18.904441957084874, 'gamma': 0.1413049946379661}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:04,873] Trial 142 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 18.480433389363718, 'gamma': 0.17697155093386632}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:05,394] Trial 144 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 17.288638774225873, 'gamma': 0.17380188911906605}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:06,500] Trial 145 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 18.531286585089575, 'gamma': 0.18290805716183353}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:07,384] Trial 147 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 17.986644218341635, 'gamma': 0.18442789482427802}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:07,570] Trial 146 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.949173960891248, 'gamma': 0.19263107643885855}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:07,987] Trial 148 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.39592739968792, 'gamma': 0.19283780274916137}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:09,058] Trial 149 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 16.71493628601771, 'gamma': 0.26232936979452937}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:09,063] Trial 150 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 9.401680351473374, 'gamma': 0.26377989644801003}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:09,729] Trial 151 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 9.91817903011124, 'gamma': 0.2691451202160891}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:10,370] Trial 152 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 33.10737802636656, 'gamma': 0.2578046159457176}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:00:11,152] Trial 154 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 19.935975668575736, 'gamma': 0.21166597952555283}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:11,221] Trial 153 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 19.876055428380194, 'gamma': 0.24330803585793007}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:11,405] Trial 155 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 19.929959246997214, 'gamma': 0.23948626684787144}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:12,098] Trial 156 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 20.441629027935164, 'gamma': 0.24238355132210027}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:13,387] Trial 158 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 13.677789254390962, 'gamma': 0.23106216390669698}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:13,842] Trial 157 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 22.713883681820114, 'gamma': 0.24444266947153226}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:13,897] Trial 159 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 21.74518950275705, 'gamma': 0.32809977671528284}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:14,213] Trial 160 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 22.437844753021285, 'gamma': 0.15532707288628197}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:15,766] Trial 162 finished with value: 0.6804407713498623 and parameters: {'kernel': 'rbf', 'C': 21.615827355084793, 'gamma': 0.1190312042059841}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:15,879] Trial 161 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 22.280543106609752, 'gamma': 0.15972520802264695}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:16,444] Trial 163 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 15.76281124417421, 'gamma': 0.12046206554243269}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:16,877] Trial 164 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.998984066947013, 'gamma': 0.20997400744844272}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:18,507] Trial 165 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 16.824207773893388, 'gamma': 0.21162440731801543}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:18,787] Trial 166 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.573539786017196, 'gamma': 0.20952239083096563}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:19,366] Trial 167 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 16.498455850741017, 'gamma': 0.20845262028984662}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:19,502] Trial 168 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 18.709729376114563, 'gamma': 0.2993140534393209}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:21,300] Trial 169 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 25.36249568953776, 'gamma': 0.29500923109265764}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:21,820] Trial 171 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 26.213943150427244, 'gamma': 0.29757821150314157}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:21,890] Trial 170 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.016747682187944, 'gamma': 0.29923146846495496}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:21,976] Trial 172 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 12.866604331161383, 'gamma': 0.19009062566661192}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:24,018] Trial 173 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 23.347779775312414, 'gamma': 0.2764207919476158}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:24,450] Trial 174 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.362018383681026, 'gamma': 0.2259370616440187}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:24,529] Trial 175 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 20.589429201355728, 'gamma': 0.22447149010816936}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:24,555] Trial 176 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 23.529239896241233, 'gamma': 0.23015634281853703}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:25,583] Trial 177 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 21.32982039121327, 'gamma': 0.2279433761483272}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:26,034] Trial 179 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 14.512268475935247, 'gamma': 0.16726509335225184}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:26,243] Trial 178 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 20.940213888583735, 'gamma': 0.23663017104689998}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:26,302] Trial 180 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 17.615295320486457, 'gamma': 0.16762120483220197}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:27,141] Trial 181 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 18.591451559985654, 'gamma': 0.17065376731026205}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:27,840] Trial 183 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 18.505658655615, 'gamma': 0.19502519533935045}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:27,855] Trial 182 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 18.625409395755383, 'gamma': 0.19444947934362364}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:27,937] Trial 184 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 18.918376764769437, 'gamma': 0.1912873131179046}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:28,600] Trial 185 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 14.018400166231059, 'gamma': 0.19387198698906016}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:29,766] Trial 186 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 14.145095418209143, 'gamma': 0.2750626602776426}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:29,804] Trial 187 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 14.286865860156245, 'gamma': 0.2764451126136457}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:29,944] Trial 188 finished with value: 0.6694214876033058 and parameters: {'kernel': 'rbf', 'C': 14.53759177000354, 'gamma': 0.4983517054335285}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:30,454] Trial 189 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 11.505390756011757, 'gamma': 0.26557963330031453}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:31,144] Trial 192 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 11.757337977239029, 'gamma': 0.2500105261913431}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:00:31,232] Trial 191 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 16.509000269578646, 'gamma': 0.4718985899175692}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:31,594] Trial 190 finished with value: 0.14325068870523416 and parameters: {'kernel': 'sigmoid', 'C': 15.999231333941736, 'gamma': 0.25386734365909164}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:31,658] Trial 193 finished with value: 0.6446280991735537 and parameters: {'kernel': 'poly', 'C': 16.040135883107503, 'gamma': 0.14477444880153462}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:33,194] Trial 194 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 26.59353421978613, 'gamma': 0.32677857597518056}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:33,346] Trial 195 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 26.920575635459404, 'gamma': 0.3244643441997999}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:33,561] Trial 196 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 26.580307921494207, 'gamma': 0.3169369583210722}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:33,960] Trial 197 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 26.92462670251435, 'gamma': 0.3289490766623491}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:35,451] Trial 198 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 28.33810837857114, 'gamma': 0.31386415071910245}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:35,573] Trial 199 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 24.01177251788667, 'gamma': 0.34672823539504943}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:35,729] Trial 200 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 24.206595310523973, 'gamma': 0.34089763938008355}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:35,894] Trial 201 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 24.228078153561057, 'gamma': 0.20654141894226302}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:37,422] Trial 202 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 24.145953920080302, 'gamma': 0.21033068748962444}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:37,646] Trial 204 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 29.73188827669779, 'gamma': 0.2164572131420145}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:37,792] Trial 203 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 20.305283570647507, 'gamma': 0.21621837414319728}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:38,363] Trial 205 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 28.964340257346, 'gamma': 0.3862929208376902}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:40,089] Trial 206 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 43.752426164538726, 'gamma': 0.3895336537949807}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:40,241] Trial 207 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 32.27632122649449, 'gamma': 0.38426027205784113}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:40,313] Trial 208 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 21.539914203642297, 'gamma': 0.300779611568965}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:40,869] Trial 209 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 33.82286642533335, 'gamma': 0.27989049488713424}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:42,371] Trial 211 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 21.40013510106657, 'gamma': 0.2890862747345066}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:42,435] Trial 210 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 21.645808248513163, 'gamma': 0.3039466617436175}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:42,591] Trial 212 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 17.752556390941955, 'gamma': 0.2923524522887612}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:43,230] Trial 213 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 17.088525051792722, 'gamma': 0.24823005558989283}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:44,325] Trial 215 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 17.75212645916642, 'gamma': 0.31012639179522206}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:44,328] Trial 214 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 25.86588476486248, 'gamma': 0.30796565873558623}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:44,496] Trial 216 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 25.26813895292456, 'gamma': 0.24613193293513289}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:45,326] Trial 217 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 25.75145921144821, 'gamma': 0.35283243240978845}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:46,249] Trial 218 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 22.663824690643168, 'gamma': 0.23873732238562898}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:46,631] Trial 219 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 22.624081930344367, 'gamma': 0.35342164225817324}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:46,740] Trial 220 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 22.834563461585567, 'gamma': 0.34402376946414387}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:47,356] Trial 221 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 22.849275187433538, 'gamma': 0.3627219091365718}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:47,896] Trial 222 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 28.656780157170232, 'gamma': 0.17697623270343718}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:48,470] Trial 224 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.245658154121582, 'gamma': 0.19512551751559012}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:48,678] Trial 223 finished with value: 0.6887052341597796 and parameters: {'kernel': 'rbf', 'C': 19.677531526213105, 'gamma': 0.2676846044054833}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:49,220] Trial 225 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 19.4871916989777, 'gamma': 0.18495752907642898}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:49,684] Trial 226 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 19.016533059878217, 'gamma': 0.19647050680830716}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:50,375] Trial 227 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 20.10699295676881, 'gamma': 0.22761503612008724}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:50,981] Trial 228 finished with value: 0.6666666666666666 and parameters: {'kernel': 'rbf', 'C': 20.280384688485093, 'gamma': 0.5741677934218073}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:00:51,437] Trial 229 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 20.580238606793145, 'gamma': 0.42713676729420164}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:51,740] Trial 230 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 30.6284501603213, 'gamma': 0.2275642901259896}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:52,325] Trial 231 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.798103197189718, 'gamma': 0.219694490980251}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:52,732] Trial 232 finished with value: 0.6859504132231405 and parameters: {'kernel': 'rbf', 'C': 6.861975617661389, 'gamma': 0.15325405292480573}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:53,388] Trial 233 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.694619888800952, 'gamma': 0.21951639667849493}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:53,885] Trial 234 finished with value: 0.699724517906336 and parameters: {'kernel': 'rbf', 'C': 15.812920076518552, 'gamma': 0.2153139224793596}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:54,306] Trial 235 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.106801644618553, 'gamma': 0.20369105262326084}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:54,823] Trial 236 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 17.545811398636506, 'gamma': 0.21231164458260238}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:55,275] Trial 237 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 17.131794969544757, 'gamma': 0.20425373650302253}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:56,530] Trial 238 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 17.41714841675397, 'gamma': 0.25944508644349973}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:56,992] Trial 239 finished with value: 0.6942148760330579 and parameters: {'kernel': 'rbf', 'C': 17.86511130104782, 'gamma': 0.2564180587696502}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:57,504] Trial 240 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 27.612541542903887, 'gamma': 0.2561375718151427}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:57,733] Trial 241 finished with value: 0.6914600550964187 and parameters: {'kernel': 'rbf', 'C': 21.964259048792442, 'gamma': 0.25688834630139806}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:58,232] Trial 242 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 21.76008089203731, 'gamma': 0.17274460675438666}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:58,539] Trial 243 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 21.736057427317082, 'gamma': 0.1780862725982845}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:58,996] Trial 244 finished with value: 0.696969696969697 and parameters: {'kernel': 'rbf', 'C': 24.666700006771045, 'gamma': 0.1744835472595598}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:59,588] Trial 245 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.271261303700584, 'gamma': 0.23433713358633915}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:00:59,994] Trial 246 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.62769516897027, 'gamma': 0.23474828729727598}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:01:00,201] Trial 247 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 23.902968729520385, 'gamma': 0.2305061380675545}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:01:00,439] Trial 248 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 24.14137511512472, 'gamma': 0.2286993377320774}. Best is trial 34 with value: 0.7024793388429752.\n",
      "[I 2023-07-08 14:01:00,700] Trial 249 finished with value: 0.7024793388429752 and parameters: {'kernel': 'rbf', 'C': 23.43888780479068, 'gamma': 0.23447452358453566}. Best is trial 34 with value: 0.7024793388429752.\n"
     ]
    }
   ],
   "source": [
    "# objective function to be minimized\n",
    "def objective_fun(trial):\n",
    "\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])\n",
    "    C = trial.suggest_float('C', 0.1,80)\n",
    "    gamma = trial.suggest_float('gamma', 0.001, 1)\n",
    "    \n",
    "\n",
    "    lin_svc = SVC(kernel=kernel, gamma=gamma, C=C)\n",
    "\n",
    "    lin_svc.fit(X_train, y_train)\n",
    "    y_pred = lin_svc.predict(X_val)\n",
    "\n",
    "    error = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 250, n_jobs = -1, catch=(ValueError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 17.90982807395262, 'gamma': 0.1965402929355844}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.72      0.57        96\n",
      "           1       0.54      0.45      0.49        94\n",
      "           2       0.50      0.51      0.51        45\n",
      "           3       0.49      0.39      0.43        95\n",
      "           4       0.39      0.48      0.43        96\n",
      "           5       0.38      0.38      0.38        48\n",
      "           6       0.37      0.27      0.32        95\n",
      "           7       0.42      0.33      0.37        48\n",
      "\n",
      "    accuracy                           0.45       617\n",
      "   macro avg       0.45      0.44      0.44       617\n",
      "weighted avg       0.45      0.45      0.44       617\n",
      "\n",
      "Accuracy 0.44894651539708263\n",
      "F1-score [0.575      0.48837209 0.50549451 0.43274854 0.43192488 0.375\n",
      " 0.31515152 0.37209302]\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "svc = SVC(**best_params)\n",
    "svc.fit(np.concatenate((X_train, X_val)),np.concatenate((y_train, y_val)))\n",
    "\n",
    "y_pred_test = svc.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1662, 231)\n",
      "(2429, 231)\n"
     ]
    }
   ],
   "source": [
    "print(svc.support_vectors_.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354.331x236.22 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=default_style.SHORT_HALFSIZE_FIGURE)\n",
    "cf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cf, annot=True, cmap='Greens', cbar=False, fmt=\".4g\", ax=ax)\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Non-linear SVM confusion matrix')\n",
    "\n",
    "ax.set_xticks([0.5,1.5], labels=label_enc.inverse_transform([[0],[1]]))\n",
    "ax.set_yticks([0.5,1.5], labels=label_enc.inverse_transform([[0],[1]]))\n",
    "\n",
    "plt.savefig(\"images/nonlinear_svm_conf_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support_vectors_pca = pca.transform(svc.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_res_t, \n",
    "#             cmap=plt.cm.prism, edgecolor='none', alpha=0.7, s=10)\n",
    "# plt.scatter(support_vectors_pca[:100, 0], support_vectors_pca[:100, 1], s=10,\n",
    "#                 linewidth=1, facecolors='none', edgecolors='k')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_function = svc.decision_function(X_res_t)\n",
    "# support_vector_indices = np.where((2 * y_res_t - 1) * decision_function <= 1)[0]\n",
    "# support_vectors = X_res_t[support_vector_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support_vectors_pca = pca.transform(support_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for kernel in ('linear', 'rbf', 'poly'):\n",
    "#     clf = SVC(kernel=kernel, gamma=0.1, C=26)\n",
    "#     clf.fit(X_res_t, y_res_t)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.clf()\n",
    "#     plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_res_t, zorder=10, cmap=plt.cm.Paired,\n",
    "#                 edgecolor='k', s=20)\n",
    "\n",
    "#     # Circle out the test data\n",
    "# #     plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], s=80, facecolors='none',\n",
    "# #                 zorder=10, edgecolor='k')\n",
    "\n",
    "#     plt.axis('tight')\n",
    "#     x_min = X_pca[:, 0].min()\n",
    "#     x_max = X_pca[:, 0].max()\n",
    "#     y_min = X_pca[:, 1].min()\n",
    "#     y_max = X_pca[:, 1].max()\n",
    "\n",
    "#     XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "#     clf.fit(X_pca, y_res_t)\n",
    "#     Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "#     # Put the result into a color plot\n",
    "#     Z = Z.reshape(XX.shape)\n",
    "#     plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "#     plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "#                 linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "\n",
    "#     plt.title(kernel)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 354.331x236.22 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boundaries_on_embedding(reducer, svc, embedding=embedding, \n",
    "                        title=\"non-linear SVM on PCA\", \n",
    "                        cmap=\"inferno\",\n",
    "                        n_pts=80)\n",
    "# plt.scatter(*(reducer.transform(svc.support_vectors_).T), color=\"r\", s=10 , marker=\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the different boundary for different kernel:\n",
    "-Linear: just see previuos section\n",
    "-Radial: Basis Function:The gamma parameters can be seen as the inverse of the radius of influence of   samples selected\n",
    "-poly : the polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 14:02:04.664684: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 14:02:05.362675: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 14:02:05.364894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-08 14:02:07.306713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  build the model for neural networks\n",
    "def build_model(meta, hidden_layer_sizes, activation, optimizer):\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(n_features_in_,)))\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, activation=activation))\n",
    "    model.add(keras.layers.Dense(n_classes_, activation=\"softmax\"))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Sklearn wrapper\n",
    "clf = KerasClassifier(\n",
    "    model = build_model,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:06:51,475] A new study created in memory with name: no-name-20fdebf5-5b1f-49c6-9c34-689477b381e9\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "46/46 [==============================] - 4s 13ms/step - loss: 2.0547 - accuracy: 0.1622\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0410 - accuracy: 0.1863\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0292 - accuracy: 0.1974\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0202 - accuracy: 0.1925\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0131 - accuracy: 0.2188\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 7s 12ms/step - loss: 2.0771 - accuracy: 0.1291\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.9916 - accuracy: 0.1925Epoch 2/50\n",
      "46/46 [==============================] - 7s 11ms/step - loss: 1.9901 - accuracy: 0.1912\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0057 - accuracy: 0.2381\n",
      "Epoch 2/50\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 2.0557 - accuracy: 0.1762Epoch 7/50\n",
      "46/46 [==============================] - 7s 14ms/step - loss: 2.0523 - accuracy: 0.1712\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0466 - accuracy: 0.1532\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8461 - accuracy: 0.2588\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9982 - accuracy: 0.2181\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 2.0329 - accuracy: 0.1641Epoch 8/50\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9775 - accuracy: 0.2478\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0382 - accuracy: 0.1601\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.9879 - accuracy: 0.2383Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9902 - accuracy: 0.2360\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7329 - accuracy: 0.2954\n",
      "Epoch 9/50\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 2.0239 - accuracy: 0.1389Epoch 4/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8931 - accuracy: 0.2843\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 2.0249 - accuracy: 0.1785Epoch 4/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0257 - accuracy: 0.1746\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6616 - accuracy: 0.3188\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.8361 - accuracy: 0.2794Epoch 5/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9819 - accuracy: 0.2491\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8414 - accuracy: 0.2885\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 2.0051 - accuracy: 0.1678Epoch 5/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0048 - accuracy: 0.1822\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.9734 - accuracy: 0.2492Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6053 - accuracy: 0.3602\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0507 - accuracy: 0.2188Epoch 6/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9732 - accuracy: 0.2567\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.8282 - accuracy: 0.2872Epoch 11/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8212 - accuracy: 0.2767\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.9741 - accuracy: 0.2542Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9715 - accuracy: 0.2595\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.7787 - accuracy: 0.3068Epoch 7/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5732 - accuracy: 0.3665\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9633 - accuracy: 0.2609\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.9474 - accuracy: 0.2734Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7687 - accuracy: 0.3009\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.5184 - accuracy: 0.3884Epoch 7/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.9224 - accuracy: 0.2567\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.9528 - accuracy: 0.2671\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5199 - accuracy: 0.3851\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7202 - accuracy: 0.3071\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8628 - accuracy: 0.2864\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9415 - accuracy: 0.2650\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5072 - accuracy: 0.4030\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.8131 - accuracy: 0.3063Epoch 9/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6936 - accuracy: 0.3126\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8057 - accuracy: 0.3009\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.6444 - accuracy: 0.2875Epoch 10/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9301 - accuracy: 0.2588\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.6605 - accuracy: 0.2969Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4740 - accuracy: 0.4203\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.6533 - accuracy: 0.3125Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6407 - accuracy: 0.3230\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.9112 - accuracy: 0.2711Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7556 - accuracy: 0.3071\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9177 - accuracy: 0.2657\n",
      "Epoch 11/50\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4347 - accuracy: 0.4320\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6007 - accuracy: 0.3471\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9033 - accuracy: 0.2698\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7251 - accuracy: 0.3085\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.8953 - accuracy: 0.2500Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4269 - accuracy: 0.4341\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.7020 - accuracy: 0.3125Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5629 - accuracy: 0.3589\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8891 - accuracy: 0.2705\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6846 - accuracy: 0.3188\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3731 - accuracy: 0.4603\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.5139 - accuracy: 0.3560Epoch 13/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8745 - accuracy: 0.2795\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.6538 - accuracy: 0.3205Epoch 19/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5250 - accuracy: 0.3671\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6586 - accuracy: 0.3188\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.8459 - accuracy: 0.3015Epoch 14/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3512 - accuracy: 0.4776\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.6659 - accuracy: 0.3086Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8591 - accuracy: 0.2774\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.5159 - accuracy: 0.3757Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5152 - accuracy: 0.3768\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.6363 - accuracy: 0.3281Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6453 - accuracy: 0.3264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3482 - accuracy: 0.4762\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8434 - accuracy: 0.2878\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.6255 - accuracy: 0.3259Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4855 - accuracy: 0.3934\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3234 - accuracy: 0.4884Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6240 - accuracy: 0.3340\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.8334 - accuracy: 0.2930Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3287 - accuracy: 0.4817\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.4292 - accuracy: 0.4050Epoch 16/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8304 - accuracy: 0.2905\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.2093 - accuracy: 0.5813Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4488 - accuracy: 0.4072\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.2807 - accuracy: 0.5041Epoch 16/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6151 - accuracy: 0.3313\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3128 - accuracy: 0.4734\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.4269 - accuracy: 0.4141Epoch 17/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.8122 - accuracy: 0.2899\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.4380 - accuracy: 0.4120\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.6096 - accuracy: 0.3474Epoch 17/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6005 - accuracy: 0.3471\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2962 - accuracy: 0.4796\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7991 - accuracy: 0.2899\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.5935 - accuracy: 0.3151Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4364 - accuracy: 0.4168\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5828 - accuracy: 0.3416\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.7892 - accuracy: 0.2943Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7837 - accuracy: 0.2947\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2617 - accuracy: 0.4955\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4329 - accuracy: 0.4203\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.5754 - accuracy: 0.3527\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3272 - accuracy: 0.4769Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7719 - accuracy: 0.2954\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2933 - accuracy: 0.4921\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.5566 - accuracy: 0.3438Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4271 - accuracy: 0.4272\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.2043 - accuracy: 0.5404Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5626 - accuracy: 0.3520\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.3717 - accuracy: 0.4531Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7569 - accuracy: 0.3023\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2296 - accuracy: 0.5238\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4252 - accuracy: 0.4375\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.2245 - accuracy: 0.5326Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5608 - accuracy: 0.3568\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.2246 - accuracy: 0.5282Epoch 22/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7452 - accuracy: 0.2968\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2298 - accuracy: 0.5245\n",
      "Epoch 28/50\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3631 - accuracy: 0.4300\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5600 - accuracy: 0.3561\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7326 - accuracy: 0.3016\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.5359 - accuracy: 0.3229Epoch 29/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2512 - accuracy: 0.5176\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.5582 - accuracy: 0.3187Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3593 - accuracy: 0.4527\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.5403 - accuracy: 0.3491Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5419 - accuracy: 0.3492\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.7100 - accuracy: 0.3154Epoch 24/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2079 - accuracy: 0.5314\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.7203 - accuracy: 0.3050\n",
      "Epoch 24/50\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3565 - accuracy: 0.4486\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.5407 - accuracy: 0.3521Epoch 24/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5345 - accuracy: 0.3582\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7119 - accuracy: 0.3085\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1998 - accuracy: 0.5349\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3280 - accuracy: 0.4363Epoch 25/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3312 - accuracy: 0.4396\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.1499 - accuracy: 0.5625Epoch 25/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5242 - accuracy: 0.3692\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6975 - accuracy: 0.3112\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2497 - accuracy: 0.4750Epoch 32/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1738 - accuracy: 0.5411\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.7494 - accuracy: 0.2708Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3046 - accuracy: 0.4707\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5142 - accuracy: 0.3665\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.1892 - accuracy: 0.5333Epoch 27/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6907 - accuracy: 0.3050\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.1894 - accuracy: 0.5288Epoch 33/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1768 - accuracy: 0.5362\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.3217 - accuracy: 0.4665Epoch 27/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3086 - accuracy: 0.4714\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.1784 - accuracy: 0.5384Epoch 27/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5066 - accuracy: 0.3727\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.3410 - accuracy: 0.4688Epoch 28/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6788 - accuracy: 0.3092\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1683 - accuracy: 0.5390\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.3136 - accuracy: 0.4675Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2925 - accuracy: 0.4803\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5005 - accuracy: 0.3754\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6730 - accuracy: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.1550 - accuracy: 0.5492Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1541 - accuracy: 0.5438\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.5033 - accuracy: 0.3535Epoch 29/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2563 - accuracy: 0.5121\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4972 - accuracy: 0.3754\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6657 - accuracy: 0.3147\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.3677 - accuracy: 0.5938Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1256 - accuracy: 0.5583\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.4867 - accuracy: 0.4007Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6613 - accuracy: 0.3133\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2868 - accuracy: 0.4928\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.1645 - accuracy: 0.5412Epoch 30/50\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5047 - accuracy: 0.3927\n",
      " 1/46 [..............................] - ETA: 2s - loss: 1.3221 - accuracy: 0.5312Epoch 31/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1260 - accuracy: 0.5611\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6557 - accuracy: 0.3223\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.1263 - accuracy: 0.5508Epoch 38/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4849 - accuracy: 0.3948\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2365 - accuracy: 0.5079\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.6349 - accuracy: 0.3203Epoch 31/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1135 - accuracy: 0.5687\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.6463 - accuracy: 0.3074Epoch 32/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6441 - accuracy: 0.3147\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4782 - accuracy: 0.3879\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.6609 - accuracy: 0.3194Epoch 33/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2352 - accuracy: 0.5052\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6380 - accuracy: 0.3292\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1263 - accuracy: 0.5728\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.6025 - accuracy: 0.3125Epoch 33/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4581 - accuracy: 0.4030\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.2168 - accuracy: 0.5122Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2191 - accuracy: 0.5059\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.4885 - accuracy: 0.3870Epoch 33/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6350 - accuracy: 0.3244\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.4805 - accuracy: 0.3912Epoch 41/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1242 - accuracy: 0.5542\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.4575 - accuracy: 0.4029Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4546 - accuracy: 0.4113\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2286 - accuracy: 0.5148\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.6292 - accuracy: 0.3277Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6322 - accuracy: 0.3251\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0889 - accuracy: 0.5749\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.6192 - accuracy: 0.3207Epoch 35/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4496 - accuracy: 0.4017\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.0146 - accuracy: 0.5781Epoch 36/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6244 - accuracy: 0.3216\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.4797 - accuracy: 0.4336Epoch 43/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2651 - accuracy: 0.4893\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.5044 - accuracy: 0.4036Epoch 35/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1051 - accuracy: 0.5638\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.1772 - accuracy: 0.5451Epoch 36/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6208 - accuracy: 0.3264\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.1975 - accuracy: 0.5248Epoch 44/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4456 - accuracy: 0.4106\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.1986 - accuracy: 0.5253Epoch 37/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2024 - accuracy: 0.5217\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.0553 - accuracy: 0.5767Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6152 - accuracy: 0.3395\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0828 - accuracy: 0.5680\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.2101 - accuracy: 0.5365Epoch 45/50\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.4360 - accuracy: 0.4093Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4293 - accuracy: 0.4155\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.1980 - accuracy: 0.5331Epoch 38/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1944 - accuracy: 0.5307\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6089 - accuracy: 0.3451\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0539 - accuracy: 0.5908\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.7386 - accuracy: 0.3438Epoch 38/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4267 - accuracy: 0.4148\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.0657 - accuracy: 0.5645Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1686 - accuracy: 0.5404\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.6076 - accuracy: 0.3415Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6070 - accuracy: 0.3409\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.1747 - accuracy: 0.5729Epoch 47/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0572 - accuracy: 0.5818\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4148 - accuracy: 0.4327\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1692 - accuracy: 0.5459\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6053 - accuracy: 0.3361\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.4322 - accuracy: 0.4284Epoch 48/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4188 - accuracy: 0.4327\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0561 - accuracy: 0.5894\n",
      "Epoch 40/50\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1497 - accuracy: 0.5500\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.5953 - accuracy: 0.3396Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5971 - accuracy: 0.3430\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.0474 - accuracy: 0.5785Epoch 49/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0350 - accuracy: 0.5845\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.5580 - accuracy: 0.3724Epoch 41/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4165 - accuracy: 0.4175\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1283 - accuracy: 0.5493\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5921 - accuracy: 0.3409\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.1174 - accuracy: 0.5500Epoch 50/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0393 - accuracy: 0.5990\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.6358 - accuracy: 0.3269Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4011 - accuracy: 0.4362\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1108 - accuracy: 0.5493\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5926 - accuracy: 0.3458\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0325 - accuracy: 0.5928\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.1435 - accuracy: 0.5508Epoch 43/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3911 - accuracy: 0.4389\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1415 - accuracy: 0.5528\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 10ms/stepss: 1.3766 - accuracy: 0.43\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0686 - accuracy: 0.5680\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3818 - accuracy: 0.4348\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:07:26,911] Trial 3 finished with value: 0.3746556473829201 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'sgd'}. Best is trial 3 with value: 0.3746556473829201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/46 [..............................] - ETA: 0s - loss: 1.0155 - accuracy: 0.6562Epoch 45/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.1115 - accuracy: 0.5567"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1200 - accuracy: 0.5535\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.0349 - accuracy: 0.5855Epoch 44/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.0238 - accuracy: 0.5990\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.4008 - accuracy: 0.4405Epoch 45/50\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 1.3954 - accuracy: 0.4382\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.0457 - accuracy: 0.5938Epoch 46/50\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.1342 - accuracy: 0.5437Epoch 1/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.1303 - accuracy: 0.5452\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.0313 - accuracy: 0.5942\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3761 - accuracy: 0.4438\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1195 - accuracy: 0.5480\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.9912 - accuracy: 0.6052\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.3619 - accuracy: 0.4489Epoch 47/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3752 - accuracy: 0.4410\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0993 - accuracy: 0.5694\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.9918 - accuracy: 0.6073\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.3489 - accuracy: 0.4539Epoch 48/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3658 - accuracy: 0.4458\n",
      " 5/46 [==>...........................] - ETA: 1s - loss: 0.8929 - accuracy: 0.6438Epoch 49/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.0921 - accuracy: 0.5735\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 0.9749 - accuracy: 0.6193Epoch 48/50\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 1.0045 - accuracy: 0.6059\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.0701 - accuracy: 0.5771Epoch 49/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0676 - accuracy: 0.5797\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.3602 - accuracy: 0.4389\n",
      " 1/46 [..............................] - ETA: 2s - loss: 1.0468 - accuracy: 0.4688Epoch 50/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9822 - accuracy: 0.6149\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0497 - accuracy: 0.5880\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3518 - accuracy: 0.4472\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9784 - accuracy: 0.6197\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0504 - accuracy: 0.5901\n",
      "12/12 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:07:32,592] Trial 1 finished with value: 0.4462809917355372 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (50, 50, 50), 'optimizer': 'adam'}. Best is trial 1 with value: 0.4462809917355372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/12 [=>............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 11ms/step\n",
      " 5/12 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:07:33,016] Trial 0 finished with value: 0.512396694214876 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (50, 50, 50), 'optimizer': 'adam'}. Best is trial 0 with value: 0.512396694214876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "[I 2023-07-08 14:07:33,209] Trial 2 finished with value: 0.4738292011019284 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 50, 10), 'optimizer': 'adam'}. Best is trial 0 with value: 0.512396694214876.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "46/46 [==============================] - 8s 14ms/step - loss: 2.0719 - accuracy: 0.1587\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0441 - accuracy: 0.1767\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0151 - accuracy: 0.1905\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 4s 13ms/step - loss: 2.0647 - accuracy: 0.1511\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.9476 - accuracy: 0.2402\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0489 - accuracy: 0.1739\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8544 - accuracy: 0.2878\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0390 - accuracy: 0.1794\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.7844 - accuracy: 0.2871\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 2.0863 - accuracy: 0.1198  Epoch 7/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0309 - accuracy: 0.1643\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 2.0703 - accuracy: 0.1471Epoch 5/50\n",
      "46/46 [==============================] - 6s 15ms/step - loss: 2.0448 - accuracy: 0.1746\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 2.0284 - accuracy: 0.1631Epoch 2/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7403 - accuracy: 0.2919\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0258 - accuracy: 0.1712\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.7104 - accuracy: 0.3250Epoch 6/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0078 - accuracy: 0.2484\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 7s 16ms/step - loss: 2.1710 - accuracy: 0.0807\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.9948 - accuracy: 0.2156Epoch 2/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.7051 - accuracy: 0.3009\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0205 - accuracy: 0.1746\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.9793 - accuracy: 0.2257\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0977 - accuracy: 0.1021\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 2.0103 - accuracy: 0.1984Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0125 - accuracy: 0.1981\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6959 - accuracy: 0.3195\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 2.0944 - accuracy: 0.1198    Epoch 8/50\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.9491 - accuracy: 0.2519\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 2.0027 - accuracy: 0.2188Epoch 5/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0663 - accuracy: 0.1532\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.9085 - accuracy: 0.2757Epoch 4/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0068 - accuracy: 0.2153\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6460 - accuracy: 0.3257\n",
      "Epoch 9/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0467 - accuracy: 0.2188Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9118 - accuracy: 0.2664\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 2.0543 - accuracy: 0.1470Epoch 6/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0520 - accuracy: 0.1477\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0002 - accuracy: 0.2326\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.8979 - accuracy: 0.2981Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6226 - accuracy: 0.3306\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 2.0531 - accuracy: 0.1648Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.8720 - accuracy: 0.2871\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.9922 - accuracy: 0.2566Epoch 7/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0460 - accuracy: 0.1484\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.9906 - accuracy: 0.2478\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 1.6104 - accuracy: 0.3320\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.8447 - accuracy: 0.2893Epoch 13/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.8329 - accuracy: 0.3002\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.9843 - accuracy: 0.2536Epoch 8/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0425 - accuracy: 0.1622\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.7968 - accuracy: 0.2930Epoch 7/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9813 - accuracy: 0.2560\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.6265 - accuracy: 0.3292\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.9738 - accuracy: 0.2679Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7979 - accuracy: 0.2912\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 2.0404 - accuracy: 0.1608\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.9711 - accuracy: 0.2622\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.6380 - accuracy: 0.3148Epoch 13/50\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5986 - accuracy: 0.3306\n",
      "27/46 [================>.............] - ETA: 0s - loss: 2.0285 - accuracy: 0.1678Epoch 15/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.7634 - accuracy: 0.2981\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.5584 - accuracy: 0.3466Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0392 - accuracy: 0.1601\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.5863 - accuracy: 0.3289Epoch 9/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9596 - accuracy: 0.2754\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.9681 - accuracy: 0.2500Epoch 14/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5710 - accuracy: 0.3478\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.7309 - accuracy: 0.3061Epoch 16/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0372 - accuracy: 0.1663\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.7327 - accuracy: 0.2995\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.6101 - accuracy: 0.3125Epoch 11/50\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.9455 - accuracy: 0.2719\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 2.0847 - accuracy: 0.1875Epoch 15/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5698 - accuracy: 0.3471\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7115 - accuracy: 0.3064\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0358 - accuracy: 0.1387\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9324 - accuracy: 0.2747\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5719 - accuracy: 0.3665\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6852 - accuracy: 0.3050\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.9200 - accuracy: 0.2684Epoch 13/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9155 - accuracy: 0.2733\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0325 - accuracy: 0.1573\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.6259 - accuracy: 0.3580Epoch 12/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.5375 - accuracy: 0.3665\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6681 - accuracy: 0.3119\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.4488 - accuracy: 0.3750Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8998 - accuracy: 0.2836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/46 [========================>.....] - ETA: 0s - loss: 2.0285 - accuracy: 0.1603Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0285 - accuracy: 0.1677\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8827 - accuracy: 0.2878\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5260 - accuracy: 0.3706\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.6603 - accuracy: 0.3125Epoch 19/50\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6576 - accuracy: 0.3133\n",
      "45/46 [============================>.] - ETA: 0s - loss: 2.0240 - accuracy: 0.2139Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0244 - accuracy: 0.2146\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8672 - accuracy: 0.3002\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 2.0239 - accuracy: 0.1976Epoch 20/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6369 - accuracy: 0.3209\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.5204 - accuracy: 0.3747Epoch 16/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0205 - accuracy: 0.2050\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5204 - accuracy: 0.3747\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.5260 - accuracy: 0.4375Epoch 21/50\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8542 - accuracy: 0.2919\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.6294 - accuracy: 0.3118Epoch 21/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6312 - accuracy: 0.3112\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0167 - accuracy: 0.2142Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0159 - accuracy: 0.2133\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.8546 - accuracy: 0.2734Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5181 - accuracy: 0.3789\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.8232 - accuracy: 0.3239Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8348 - accuracy: 0.2981\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 2.0092 - accuracy: 0.2331Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6033 - accuracy: 0.3326\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.5050 - accuracy: 0.3817Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0108 - accuracy: 0.2222\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.4924 - accuracy: 0.3830Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4924 - accuracy: 0.3830\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.9790 - accuracy: 0.2812Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.8177 - accuracy: 0.3071\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.4693 - accuracy: 0.4034Epoch 23/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5901 - accuracy: 0.3299\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0049 - accuracy: 0.2174\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.5402 - accuracy: 0.3750Epoch 18/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4730 - accuracy: 0.4106\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8037 - accuracy: 0.3140\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5771 - accuracy: 0.3402\n",
      "Epoch 20/50\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9966 - accuracy: 0.2160\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4670 - accuracy: 0.4051\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.5402 - accuracy: 0.3729Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5694 - accuracy: 0.3423\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7913 - accuracy: 0.3050\n",
      "Epoch 21/50\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4465 - accuracy: 0.4210\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.8037 - accuracy: 0.2885Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9886 - accuracy: 0.2112\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3118 - accuracy: 0.4688Epoch 20/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.7777 - accuracy: 0.3119\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.4177 - accuracy: 0.4301Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5511 - accuracy: 0.3430\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.7363 - accuracy: 0.3259Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9801 - accuracy: 0.2284\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4529 - accuracy: 0.4120\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7618 - accuracy: 0.3071\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.9725 - accuracy: 0.1920Epoch 27/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5469 - accuracy: 0.3665\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.8737 - accuracy: 0.1875Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9738 - accuracy: 0.2008\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4261 - accuracy: 0.4320\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7494 - accuracy: 0.3251\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5372 - accuracy: 0.3602\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9644 - accuracy: 0.2208\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4007 - accuracy: 0.4244\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.5207 - accuracy: 0.3604Epoch 29/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7341 - accuracy: 0.3285\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5193 - accuracy: 0.3602\n",
      "Epoch 29/50\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.9511 - accuracy: 0.2261Epoch 25/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9563 - accuracy: 0.2257\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.5215 - accuracy: 0.3640Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7222 - accuracy: 0.3278\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5145 - accuracy: 0.3713\n",
      "Epoch 26/50\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.9527 - accuracy: 0.1969Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3896 - accuracy: 0.4534\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.9517 - accuracy: 0.1992Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9479 - accuracy: 0.2257\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.4002 - accuracy: 0.4479Epoch 25/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5025 - accuracy: 0.3692\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.9313 - accuracy: 0.2227Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7137 - accuracy: 0.3333\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3134 - accuracy: 0.5000Epoch 31/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3847 - accuracy: 0.4396\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.5045 - accuracy: 0.3654Epoch 31/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9406 - accuracy: 0.2277\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.3474 - accuracy: 0.4531Epoch 26/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5010 - accuracy: 0.3789\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3572 - accuracy: 0.4479Epoch 28/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6997 - accuracy: 0.3361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/46 [..............................] - ETA: 0s - loss: 1.5963 - accuracy: 0.2812Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3706 - accuracy: 0.4451\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.5196 - accuracy: 0.3553Epoch 32/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4889 - accuracy: 0.3761\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9323 - accuracy: 0.2284\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.4594 - accuracy: 0.3125Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6849 - accuracy: 0.3464\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.5604 - accuracy: 0.3187Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3518 - accuracy: 0.4576\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.6760 - accuracy: 0.3413Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4785 - accuracy: 0.3830\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9238 - accuracy: 0.2436\n",
      "Epoch 28/50\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6770 - accuracy: 0.3437\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.5121 - accuracy: 0.3625Epoch 34/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3296 - accuracy: 0.4617\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.6780 - accuracy: 0.3365Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4821 - accuracy: 0.3858\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9148 - accuracy: 0.2657\n",
      "Epoch 31/50\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.2867 - accuracy: 0.4625Epoch 29/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6700 - accuracy: 0.3354\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3334 - accuracy: 0.4562\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.9046 - accuracy: 0.2587Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9066 - accuracy: 0.2581\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.4612 - accuracy: 0.3965Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6596 - accuracy: 0.3458\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4642 - accuracy: 0.3954\n",
      "Epoch 36/50\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.3393 - accuracy: 0.4563Epoch 32/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3041 - accuracy: 0.4762\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6479 - accuracy: 0.3589\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.8985 - accuracy: 0.2614Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8970 - accuracy: 0.2609\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4640 - accuracy: 0.3934\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.3340 - accuracy: 0.4625Epoch 31/50\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.6699 - accuracy: 0.2750Epoch 33/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3098 - accuracy: 0.4686\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.8839 - accuracy: 0.2803Epoch 37/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6449 - accuracy: 0.3540\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.8861 - accuracy: 0.2755Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8879 - accuracy: 0.2740\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4528 - accuracy: 0.3996\n",
      "Epoch 32/50\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6313 - accuracy: 0.3582\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3068 - accuracy: 0.4651\n",
      "Epoch 39/50\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.4442 - accuracy: 0.4118Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4412 - accuracy: 0.4113\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.6134 - accuracy: 0.3973Epoch 35/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.8797 - accuracy: 0.2678\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.2731 - accuracy: 0.5018Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6217 - accuracy: 0.3685\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.2767 - accuracy: 0.4852Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2760 - accuracy: 0.4900\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.4540 - accuracy: 0.3885Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4408 - accuracy: 0.3934\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8710 - accuracy: 0.2843\n",
      "Epoch 36/50\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6213 - accuracy: 0.3609\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2996 - accuracy: 0.4714\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8663 - accuracy: 0.2995\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.4292 - accuracy: 0.4085Epoch 35/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4307 - accuracy: 0.4086\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.2809 - accuracy: 0.4785Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6089 - accuracy: 0.3651\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.8505 - accuracy: 0.2905Epoch 42/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2720 - accuracy: 0.4859\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.5988 - accuracy: 0.3719Epoch 41/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8569 - accuracy: 0.2857\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4288 - accuracy: 0.4106\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.6073 - accuracy: 0.3692Epoch 38/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6010 - accuracy: 0.3789\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.2572 - accuracy: 0.4964Epoch 43/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8433 - accuracy: 0.3106\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.2725 - accuracy: 0.4900\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.5794 - accuracy: 0.3920Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4196 - accuracy: 0.4099\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.2680 - accuracy: 0.4886Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5939 - accuracy: 0.3741\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8335 - accuracy: 0.3147\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.4234 - accuracy: 0.4167Epoch 38/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2586 - accuracy: 0.4921\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.5833 - accuracy: 0.3774Epoch 43/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4192 - accuracy: 0.4196\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5843 - accuracy: 0.3851\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.2651 - accuracy: 0.4980Epoch 45/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8248 - accuracy: 0.3140\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.2613 - accuracy: 0.4840Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2493 - accuracy: 0.4907\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4080 - accuracy: 0.4306\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.5889 - accuracy: 0.3731Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5831 - accuracy: 0.3782\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.2263 - accuracy: 0.4911Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8171 - accuracy: 0.3313\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2307 - accuracy: 0.5169\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.7897 - accuracy: 0.3187Epoch 45/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4018 - accuracy: 0.4210\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.7917 - accuracy: 0.3553Epoch 42/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5742 - accuracy: 0.3789\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.3386 - accuracy: 0.4750Epoch 47/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8042 - accuracy: 0.3471\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3926 - accuracy: 0.4329Epoch 41/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2430 - accuracy: 0.5072\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.5614 - accuracy: 0.4033Epoch 46/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3956 - accuracy: 0.4306\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5651 - accuracy: 0.3885\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.3653 - accuracy: 0.4313Epoch 48/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7959 - accuracy: 0.3492\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.2188 - accuracy: 0.5043Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3966 - accuracy: 0.4306\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.2217 - accuracy: 0.5031\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.5620 - accuracy: 0.3910Epoch 47/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5587 - accuracy: 0.3954\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.2588 - accuracy: 0.5208Epoch 49/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7854 - accuracy: 0.3458\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.3665 - accuracy: 0.4364Epoch 43/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3925 - accuracy: 0.4279\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.2199 - accuracy: 0.5160Epoch 45/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2191 - accuracy: 0.5162\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5568 - accuracy: 0.3920\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.7834 - accuracy: 0.3324Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7767 - accuracy: 0.3389\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3837 - accuracy: 0.4421Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3919 - accuracy: 0.4403\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.5313 - accuracy: 0.4120Epoch 46/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2441 - accuracy: 0.4948\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5493 - accuracy: 0.3879\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7644 - accuracy: 0.3671\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3862 - accuracy: 0.4363Epoch 45/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3835 - accuracy: 0.4465\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2036 - accuracy: 0.5197\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.3921 - accuracy: 0.4207Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7523 - accuracy: 0.3706\n",
      "12/12 [==============================] - 0s 10ms/step\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.3915 - accuracy: 0.4290Epoch 46/50\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.1594 - accuracy: 0.5352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:09,356] Trial 5 finished with value: 0.3856749311294766 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'sgd'}. Best is trial 0 with value: 0.512396694214876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3794 - accuracy: 0.4355\n",
      "Epoch 48/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3441 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2142 - accuracy: 0.5190\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7444 - accuracy: 0.3658\n",
      "Epoch 47/50\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.7538 - accuracy: 0.3854Epoch 1/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3700 - accuracy: 0.4465\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.7381 - accuracy: 0.3706\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.3681 - accuracy: 0.4465\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.7245 - accuracy: 0.3720\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.3325 - accuracy: 0.4429Epoch 49/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.3576 - accuracy: 0.4451\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.7117 - accuracy: 0.3810\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 6ms/steposs: 1.6845 - accuracy: 0.\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.6889 - accuracy: 0.3830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:11,559] Trial 7 finished with value: 0.41597796143250687 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (10, 10), 'optimizer': 'adam'}. Best is trial 0 with value: 0.512396694214876.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 11ms/step - loss: 1.6999 - accuracy: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12/12 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:12,211] Trial 6 finished with value: 0.39118457300275483 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 0 with value: 0.512396694214876.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/46 [============>.................] - ETA: 0s - loss: 2.1046 - accuracy: 0.1781Epoch 1/50\n",
      "46/46 [==============================] - 3s 9ms/step - loss: 2.0565 - accuracy: 0.2001\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.9888 - accuracy: 0.2450\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.9542 - accuracy: 0.2616\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.9121 - accuracy: 0.2974\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.8756 - accuracy: 0.2774\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.8347 - accuracy: 0.2961\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 1.8007 - accuracy: 0.3043\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.7654 - accuracy: 0.3161\n",
      "46/46 [==============================] - 3s 7ms/step - loss: 2.0722 - accuracy: 0.1767\n",
      "Epoch 9/50\n",
      "20/46 [============>.................] - ETA: 0s - loss: 2.0925 - accuracy: 0.1328Epoch 2/50\n",
      "46/46 [==============================] - 4s 9ms/step - loss: 2.0728 - accuracy: 0.1470\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.7377 - accuracy: 0.3368\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0614 - accuracy: 0.1843\n",
      "Epoch 10/50\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0218 - accuracy: 0.1946\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0543 - accuracy: 0.1801\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7062 - accuracy: 0.3299\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0334 - accuracy: 0.3438Epoch 11/50\n",
      "12/12 [==============================] - 0s 12ms/stepss: 2.0481 - accuracy: 0.19\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9923 - accuracy: 0.2001\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 2.0500 - accuracy: 0.1875Epoch 4/50\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.6907 - accuracy: 0.3498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:16,936] Trial 4 finished with value: 0.5234159779614325 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'adam'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0480 - accuracy: 0.1863\n",
      "Epoch 5/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0967 - accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6837 - accuracy: 0.3506\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.9439 - accuracy: 0.2426Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.9450 - accuracy: 0.2319\n",
      "Epoch 5/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 2.0433 - accuracy: 0.1853Epoch 1/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0417 - accuracy: 0.1905\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6661 - accuracy: 0.3506\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9006 - accuracy: 0.2616\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.6421 - accuracy: 0.3585Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0350 - accuracy: 0.1836\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.6435 - accuracy: 0.3572Epoch 7/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.6450 - accuracy: 0.3589\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8533 - accuracy: 0.2609\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.6298 - accuracy: 0.3574Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6268 - accuracy: 0.3568\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0268 - accuracy: 0.1953\n",
      "Epoch 15/50\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.8142 - accuracy: 0.2761\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0194 - accuracy: 0.2029\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6086 - accuracy: 0.3720\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.7764 - accuracy: 0.2812Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7745 - accuracy: 0.2899\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.6160 - accuracy: 0.3662Epoch 9/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0104 - accuracy: 0.2202\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.5983 - accuracy: 0.3699Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5983 - accuracy: 0.3699\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7248 - accuracy: 0.2947\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 2.0075 - accuracy: 0.2196Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0009 - accuracy: 0.2271\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.5803 - accuracy: 0.3750Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5801 - accuracy: 0.3823\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6924 - accuracy: 0.3050\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.9908 - accuracy: 0.2230Epoch 11/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.9913 - accuracy: 0.2291\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.5885 - accuracy: 0.3796Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5699 - accuracy: 0.3782\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6619 - accuracy: 0.3175\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.5589 - accuracy: 0.3869Epoch 12/50\n",
      "46/46 [==============================] - 4s 12ms/step - loss: 2.5472 - accuracy: 0.0807\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9801 - accuracy: 0.2429\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.6260 - accuracy: 0.3229Epoch 13/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5553 - accuracy: 0.3844\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6447 - accuracy: 0.3195\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 2.3035 - accuracy: 0.0824Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.3040 - accuracy: 0.0807\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.5505 - accuracy: 0.3872Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9673 - accuracy: 0.2609\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.5507 - accuracy: 0.3864Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5490 - accuracy: 0.3913\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6206 - accuracy: 0.3333\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.2066 - accuracy: 0.0793Epoch 14/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.1992 - accuracy: 0.0807\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9537 - accuracy: 0.2767\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5358 - accuracy: 0.3892\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 2.1218 - accuracy: 0.0781Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6092 - accuracy: 0.3402\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.5163 - accuracy: 0.4062Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.1430 - accuracy: 0.0807\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9397 - accuracy: 0.2740\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.5321 - accuracy: 0.3990Epoch 5/50\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5258 - accuracy: 0.4003\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.5837 - accuracy: 0.3628Epoch 23/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5968 - accuracy: 0.3423\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.9237 - accuracy: 0.2870Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9226 - accuracy: 0.2864\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.1099 - accuracy: 0.1256\n",
      "Epoch 17/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0152 - accuracy: 0.2188Epoch 6/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5136 - accuracy: 0.4120\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 2.0875 - accuracy: 0.1328Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5820 - accuracy: 0.3520\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.4919 - accuracy: 0.4185Epoch 17/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.9045 - accuracy: 0.2954\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 2.0890 - accuracy: 0.1477Epoch 18/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0889 - accuracy: 0.1470\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5006 - accuracy: 0.4092\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 2.0754 - accuracy: 0.1562Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5766 - accuracy: 0.3602\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8855 - accuracy: 0.3050\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.5307 - accuracy: 0.3854Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0741 - accuracy: 0.1470\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5038 - accuracy: 0.4189\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5692 - accuracy: 0.3568\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8667 - accuracy: 0.2981\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0641 - accuracy: 0.1470\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.5485 - accuracy: 0.4062Epoch 9/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4853 - accuracy: 0.4293\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.5547 - accuracy: 0.3711Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5483 - accuracy: 0.3616\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.8489 - accuracy: 0.3009\n",
      "Epoch 20/50\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0573 - accuracy: 0.1470\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.7004 - accuracy: 0.3438Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4754 - accuracy: 0.4306\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.5382 - accuracy: 0.3822Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5453 - accuracy: 0.3630\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8290 - accuracy: 0.3023\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0528 - accuracy: 0.1470\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.8890 - accuracy: 0.3750Epoch 21/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.4418 - accuracy: 0.4375Epoch 11/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4657 - accuracy: 0.4355\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.5161 - accuracy: 0.3750Epoch 29/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8132 - accuracy: 0.3037\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.5327 - accuracy: 0.3658Epoch 23/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5327 - accuracy: 0.3658\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0495 - accuracy: 0.1470\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.6063 - accuracy: 0.2812Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4483 - accuracy: 0.4313\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.5482 - accuracy: 0.3569Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7996 - accuracy: 0.3002\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5199 - accuracy: 0.3713\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0716 - accuracy: 0.1875Epoch 23/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0470 - accuracy: 0.1470\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.8737 - accuracy: 0.2438Epoch 13/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4379 - accuracy: 0.4389\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 2.0525 - accuracy: 0.1469Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7836 - accuracy: 0.2974\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.4053 - accuracy: 0.4698Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5120 - accuracy: 0.3892\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.7156 - accuracy: 0.3562Epoch 24/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0454 - accuracy: 0.1339\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4324 - accuracy: 0.4479\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.7587 - accuracy: 0.3088Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7693 - accuracy: 0.2974\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.4243 - accuracy: 0.4607Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0440 - accuracy: 0.1360\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5128 - accuracy: 0.3810\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.4102 - accuracy: 0.4617Epoch 15/50\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4228 - accuracy: 0.4569\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7595 - accuracy: 0.3092\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0430 - accuracy: 0.1442\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5039 - accuracy: 0.3754\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4085 - accuracy: 0.4458\n",
      "Epoch 26/50\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.7178 - accuracy: 0.3235Epoch 34/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.7489 - accuracy: 0.3106\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.5033 - accuracy: 0.3594Epoch 28/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0423 - accuracy: 0.1546\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5116 - accuracy: 0.3741\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 2.0296 - accuracy: 0.1597Epoch 27/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4050 - accuracy: 0.4438\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7388 - accuracy: 0.3016\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.3787 - accuracy: 0.4743Epoch 29/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0419 - accuracy: 0.1553\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.3771 - accuracy: 0.4693Epoch 18/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3771 - accuracy: 0.4693\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.4907 - accuracy: 0.3815Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4895 - accuracy: 0.3816\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.7297 - accuracy: 0.3002\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3870 - accuracy: 0.4362\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4845 - accuracy: 0.3913\n",
      "Epoch 29/50\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0416 - accuracy: 0.1553\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.7224 - accuracy: 0.3184Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7219 - accuracy: 0.3119\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.4656 - accuracy: 0.4122Epoch 31/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4786 - accuracy: 0.3899\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 2.0422 - accuracy: 0.1534Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3763 - accuracy: 0.4562\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0414 - accuracy: 0.1553\n",
      "Epoch 20/50\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7167 - accuracy: 0.3085\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4763 - accuracy: 0.3906\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0412 - accuracy: 0.1553\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3722 - accuracy: 0.4548\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.7000 - accuracy: 0.3070Epoch 39/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7096 - accuracy: 0.2974\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.3424 - accuracy: 0.4863Epoch 33/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4703 - accuracy: 0.3975\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 2.0402 - accuracy: 0.1577Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0411 - accuracy: 0.1553\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.6973 - accuracy: 0.3320Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3568 - accuracy: 0.4665\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7011 - accuracy: 0.3140\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 2.0422 - accuracy: 0.1469Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4623 - accuracy: 0.4065\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3429 - accuracy: 0.4560Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0409 - accuracy: 0.1553\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.6881 - accuracy: 0.3125Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3490 - accuracy: 0.4576\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6965 - accuracy: 0.3154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/46 [============================>.] - ETA: 0s - loss: 1.4654 - accuracy: 0.4014Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4670 - accuracy: 0.4003\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0409 - accuracy: 0.1511\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.3510 - accuracy: 0.4603Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3361 - accuracy: 0.4720\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.4480 - accuracy: 0.4077Epoch 42/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6900 - accuracy: 0.3023\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4554 - accuracy: 0.3982\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.7803 - accuracy: 0.2734Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0408 - accuracy: 0.1484\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3407 - accuracy: 0.4727\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 2.0281 - accuracy: 0.1597Epoch 43/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4538 - accuracy: 0.3989\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.3282 - accuracy: 0.4762Epoch 36/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6945 - accuracy: 0.3078\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.5104 - accuracy: 0.3438Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0407 - accuracy: 0.1553\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.4789 - accuracy: 0.3913Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3256 - accuracy: 0.4720\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4672 - accuracy: 0.3941\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6768 - accuracy: 0.3023\n",
      "Epoch 37/50\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.3191 - accuracy: 0.4750Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0407 - accuracy: 0.1553\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3159 - accuracy: 0.4845\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.4400 - accuracy: 0.4291Epoch 27/50\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4437 - accuracy: 0.4168\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6787 - accuracy: 0.3099\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3087 - accuracy: 0.4796\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0406 - accuracy: 0.1553\n",
      "Epoch 28/50\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4388 - accuracy: 0.4086\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 2.0410 - accuracy: 0.1741Epoch 39/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.6706 - accuracy: 0.3161\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3167 - accuracy: 0.4962\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0406 - accuracy: 0.1553\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.4442 - accuracy: 0.4072Epoch 29/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.6655 - accuracy: 0.3057\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4357 - accuracy: 0.4092\n",
      "Epoch 41/50\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3098 - accuracy: 0.4776\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0406 - accuracy: 0.1553\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.4301 - accuracy: 0.4164Epoch 30/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.6622 - accuracy: 0.3195\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.3088 - accuracy: 0.4672Epoch 42/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4327 - accuracy: 0.4148\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.7599 - accuracy: 0.1875Epoch 41/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2988 - accuracy: 0.4803\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 2.0401 - accuracy: 0.1488Epoch 49/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0406 - accuracy: 0.1491\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4271 - accuracy: 0.4155\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6625 - accuracy: 0.3195\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 2.0342 - accuracy: 0.1415Epoch 43/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.8030 - accuracy: 0.2500Epoch 42/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2869 - accuracy: 0.4859\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0406 - accuracy: 0.1539\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4286 - accuracy: 0.4155\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6559 - accuracy: 0.3112\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.2715 - accuracy: 0.4941Epoch 43/50\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2877 - accuracy: 0.4893\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0405 - accuracy: 0.1553\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.4242 - accuracy: 0.4251\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.6646 - accuracy: 0.3085\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 2.0490 - accuracy: 0.1745Epoch 45/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0405 - accuracy: 0.1553\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.6648 - accuracy: 0.3045Epoch 34/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.4173 - accuracy: 0.4224\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.6531 - accuracy: 0.3112\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0405 - accuracy: 0.1553\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.4319 - accuracy: 0.4219Epoch 35/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.6474 - accuracy: 0.3119\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.4239 - accuracy: 0.4224\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 2.0332 - accuracy: 0.1500Epoch 46/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0405 - accuracy: 0.1511\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.4444 - accuracy: 0.4089Epoch 36/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.6395 - accuracy: 0.3313\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 2.0600 - accuracy: 0.1458Epoch 48/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.4197 - accuracy: 0.4182\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.6448 - accuracy: 0.3313Epoch 47/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0405 - accuracy: 0.1511\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.6513 - accuracy: 0.3154\n",
      "Epoch 37/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.4209 - accuracy: 0.4237Epoch 49/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.4217 - accuracy: 0.4251\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 2.0331 - accuracy: 0.1250Epoch 48/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.6372 - accuracy: 0.3168\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.4012 - accuracy: 0.4320Epoch 50/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.4012 - accuracy: 0.4320\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0404 - accuracy: 0.1360\n",
      "Epoch 49/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.2586 - accuracy: 0.5625Epoch 38/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.6370 - accuracy: 0.3202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 9ms/step - loss: 1.4004 - accuracy: 0.4265\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0405 - accuracy: 0.1408\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.4088 - accuracy: 0.4306\n",
      "12/12 [==============================] - 0s 7ms/steposs: 2.0431 - accuracy: 0.15\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0405 - accuracy: 0.1553\n",
      "Epoch 40/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0613 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:43,574] Trial 10 finished with value: 0.3333333333333333 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'sgd'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/46 [================>.............] - ETA: 0s - loss: 2.0462 - accuracy: 0.1481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0405 - accuracy: 0.1498\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 6ms/step\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.0942 - accuracy: 0.0312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:43,995] Trial 9 finished with value: 0.39944903581267216 and parameters: {'activation': 'relu', 'hidden_layer_sizes': (10, 10), 'optimizer': 'adam'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/46 [====>.........................] - ETA: 0s - loss: 2.0610 - accuracy: 0.1328Epoch 1/50\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 2.0306 - accuracy: 0.1637"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0404 - accuracy: 0.1532\n",
      "Epoch 42/50\n",
      "22/46 [=============>................] - ETA: 0s - loss: 2.0363 - accuracy: 0.1719Epoch 1/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0404 - accuracy: 0.1553\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0404 - accuracy: 0.1477\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0405 - accuracy: 0.1518\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 6ms/step - loss: 2.0404 - accuracy: 0.1456\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0404 - accuracy: 0.1511\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0404 - accuracy: 0.1539\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0404 - accuracy: 0.1394\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 3s 10ms/step - loss: 2.1095 - accuracy: 0.1546\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0404 - accuracy: 0.1491\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 2.0476 - accuracy: 0.1449\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 2.0403 - accuracy: 0.1498\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 2.0440 - accuracy: 0.1504\n",
      " 1/12 [=>............................] - ETA: 2sEpoch 4/50\n",
      "12/12 [==============================] - 0s 6ms/steposs: 2.0605 - accuracy: 0.\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 2.0441 - accuracy: 0.1741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:48,497] Trial 11 finished with value: 0.14325068870523416 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (100, 50, 10), 'optimizer': 'sgd'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 5s 8ms/step - loss: 1.9865 - accuracy: 0.1953\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0436 - accuracy: 0.1525\n",
      "Epoch 5/50\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 2.0335 - accuracy: 0.1420Epoch 1/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7914 - accuracy: 0.3009\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0437 - accuracy: 0.1470\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.7040 - accuracy: 0.3482Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6754 - accuracy: 0.3540\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0427 - accuracy: 0.1587\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.5907 - accuracy: 0.4003\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0442 - accuracy: 0.1429\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5276 - accuracy: 0.4168\n",
      "28/46 [=================>............] - ETA: 0s - loss: 2.0382 - accuracy: 0.1652Epoch 6/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0430 - accuracy: 0.1580\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.4809 - accuracy: 0.4441Epoch 9/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.4789 - accuracy: 0.4375\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0447 - accuracy: 0.1380\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.4348 - accuracy: 0.4583Epoch 10/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4437 - accuracy: 0.4472\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0440 - accuracy: 0.1456\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.4067 - accuracy: 0.4817\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0436 - accuracy: 0.1442\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3963 - accuracy: 0.4638\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0438 - accuracy: 0.1484\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.3796 - accuracy: 0.4714\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0433 - accuracy: 0.1470\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.3337 - accuracy: 0.5108Epoch 14/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.3383 - accuracy: 0.5059\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 2.0642 - accuracy: 0.1562Epoch 12/50\n",
      "46/46 [==============================] - 5s 12ms/step - loss: 1.8663 - accuracy: 0.2581\n",
      "12/12 [==============================] - 0s 8ms/step\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0438 - accuracy: 0.1491\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.7499 - accuracy: 0.3750Epoch 15/50\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.3157 - accuracy: 0.5112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:08:54,284] Trial 8 finished with value: 0.3691460055096419 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (50, 50, 50), 'optimizer': 'sgd'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3211 - accuracy: 0.5114\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 2.0389 - accuracy: 0.1607Epoch 13/50\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 2.0453 - accuracy: 0.1431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6331 - accuracy: 0.3527\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0434 - accuracy: 0.1401\n",
      "Epoch 16/50\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3266 - accuracy: 0.4831\n",
      "Epoch 14/50\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.5529 - accuracy: 0.3941Epoch 1/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0435 - accuracy: 0.1477\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.2886 - accuracy: 0.5033Epoch 17/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5121 - accuracy: 0.3989\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2764 - accuracy: 0.5155\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0439 - accuracy: 0.1580\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.2562 - accuracy: 0.5312Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4332 - accuracy: 0.4451\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2603 - accuracy: 0.5349\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.3897 - accuracy: 0.4542Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0439 - accuracy: 0.1415\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3978 - accuracy: 0.4582\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 2.0429 - accuracy: 0.1589Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3172 - accuracy: 0.4928\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0434 - accuracy: 0.1615\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3656 - accuracy: 0.4796\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2526 - accuracy: 0.5355\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.3078 - accuracy: 0.4931Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0431 - accuracy: 0.1491\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2986 - accuracy: 0.4948\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2362 - accuracy: 0.5280\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0439 - accuracy: 0.1539\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3060 - accuracy: 0.4852\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2373 - accuracy: 0.5238\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 2.0399 - accuracy: 0.1683Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0430 - accuracy: 0.1511\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.2068 - accuracy: 0.5524Epoch 23/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3012 - accuracy: 0.5079\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2087 - accuracy: 0.5493\n",
      "22/46 [=============>................] - ETA: 0s - loss: 2.0337 - accuracy: 0.1506Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0433 - accuracy: 0.1463\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2764 - accuracy: 0.4921\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 2.0341 - accuracy: 0.1604Epoch 11/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.1901 - accuracy: 0.5487\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.1945 - accuracy: 0.5491Epoch 22/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0431 - accuracy: 0.1401\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2230 - accuracy: 0.5245\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 2.0492 - accuracy: 0.1562Epoch 12/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1926 - accuracy: 0.5404\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0423 - accuracy: 0.1546\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2156 - accuracy: 0.5224\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.2052 - accuracy: 0.5517Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1996 - accuracy: 0.5597\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0436 - accuracy: 0.1532\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.1304 - accuracy: 0.5859Epoch 27/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2532 - accuracy: 0.5086\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 2.0693 - accuracy: 0.1553Epoch 14/50\n",
      "46/46 [==============================] - 7s 15ms/step - loss: 2.0606 - accuracy: 0.1525\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.1481 - accuracy: 0.5724Epoch 2/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1520 - accuracy: 0.5680\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0415 - accuracy: 0.1517Epoch 25/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0433 - accuracy: 0.1525\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.2259 - accuracy: 0.5337Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1831 - accuracy: 0.5493\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0457 - accuracy: 0.1705\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.1986 - accuracy: 0.5250Epoch 3/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1421 - accuracy: 0.5707\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.1775 - accuracy: 0.5362Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0438 - accuracy: 0.1380\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 2.0211 - accuracy: 0.1731Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1454 - accuracy: 0.5480\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0103 - accuracy: 0.2084\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.1386 - accuracy: 0.5641Epoch 4/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1227 - accuracy: 0.5735\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0436 - accuracy: 0.1463\n",
      "Epoch 27/50\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2019 - accuracy: 0.5314\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.9569 - accuracy: 0.2360Epoch 17/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9362 - accuracy: 0.2457\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0434 - accuracy: 0.1387\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1340 - accuracy: 0.5625\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.1465 - accuracy: 0.5479Epoch 28/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1371 - accuracy: 0.5528\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8533 - accuracy: 0.2650\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0431 - accuracy: 0.1570Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0429 - accuracy: 0.1587\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.1291 - accuracy: 0.5680\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1541 - accuracy: 0.5549\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7915 - accuracy: 0.2816\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.1089 - accuracy: 0.5562Epoch 7/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0428 - accuracy: 0.1477\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.1312 - accuracy: 0.5576Epoch 33/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1127 - accuracy: 0.5700\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.7519 - accuracy: 0.2906Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1546 - accuracy: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/46 [=====================>........] - ETA: 0s - loss: 1.1041 - accuracy: 0.5873Epoch 20/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.0902 - accuracy: 0.5873\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.7397 - accuracy: 0.3050Epoch 31/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7397 - accuracy: 0.3050\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0433 - accuracy: 0.1463\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1290 - accuracy: 0.5611\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.0922 - accuracy: 0.5777Epoch 21/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0723 - accuracy: 0.5852\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.7021 - accuracy: 0.2981\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.0844 - accuracy: 0.5740Epoch 9/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0418 - accuracy: 0.1587\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.6882 - accuracy: 0.3438Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0722 - accuracy: 0.5970\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0941 - accuracy: 0.5859\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.0774 - accuracy: 0.5772Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6701 - accuracy: 0.3209\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.1028 - accuracy: 0.5625Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0420 - accuracy: 0.1560\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.0621 - accuracy: 0.5972Epoch 36/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0705 - accuracy: 0.5769\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.6378 - accuracy: 0.3269Epoch 23/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0524 - accuracy: 0.6094\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.1596 - accuracy: 0.5527Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0424 - accuracy: 0.1539\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.0336 - accuracy: 0.6094Epoch 37/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.6464 - accuracy: 0.3188\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.1349 - accuracy: 0.5580Epoch 11/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1287 - accuracy: 0.5576\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0537 - accuracy: 0.6046\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 0.9868 - accuracy: 0.6354Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0438 - accuracy: 0.1573\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6299 - accuracy: 0.3299\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.0764 - accuracy: 0.5924Epoch 12/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0346 - accuracy: 0.6073\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.0390 - accuracy: 0.6068Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0463 - accuracy: 0.6101\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 2.0400 - accuracy: 0.1510Epoch 36/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0430 - accuracy: 0.1429\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.0525 - accuracy: 0.6118Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6143 - accuracy: 0.3264\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.0775 - accuracy: 0.5825Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0737 - accuracy: 0.5832\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.5653 - accuracy: 0.3611Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0354 - accuracy: 0.6135\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.6084 - accuracy: 0.3366Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0437 - accuracy: 0.1325\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.0498 - accuracy: 0.5882Epoch 40/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5926 - accuracy: 0.3430\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0364 - accuracy: 0.5935\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.0170 - accuracy: 0.6149Epoch 27/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0132 - accuracy: 0.6177\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0431 - accuracy: 0.1477\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 0.9819 - accuracy: 0.6198Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5796 - accuracy: 0.3520\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.9981 - accuracy: 0.6197Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9946 - accuracy: 0.6225\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 0.9976 - accuracy: 0.6233Epoch 28/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9988 - accuracy: 0.6218\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0422 - accuracy: 0.1477\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.9351 - accuracy: 0.6451Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5650 - accuracy: 0.3458\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9647 - accuracy: 0.6225\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.5270 - accuracy: 0.3665Epoch 29/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0406 - accuracy: 0.6011\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 0.9918 - accuracy: 0.6281Epoch 40/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0431 - accuracy: 0.1498\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.5373 - accuracy: 0.3564Epoch 43/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5455 - accuracy: 0.3616\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0009 - accuracy: 0.6094\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9812 - accuracy: 0.6425\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0429 - accuracy: 0.1442\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.5316 - accuracy: 0.3594Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0349 - accuracy: 0.5873\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5362 - accuracy: 0.3658\n",
      "Epoch 31/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.8384 - accuracy: 0.6875Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0060 - accuracy: 0.6322\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0432 - accuracy: 0.1463\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 0.9729 - accuracy: 0.6179Epoch 45/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9589 - accuracy: 0.6280\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5227 - accuracy: 0.3789\n",
      " 5/46 [==>...........................] - ETA: 1s - loss: 1.0457 - accuracy: 0.6125Epoch 19/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.0011 - accuracy: 0.6121\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 2.0356 - accuracy: 0.1538Epoch 43/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 2.0424 - accuracy: 0.1491\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.5101 - accuracy: 0.3750Epoch 46/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.9601 - accuracy: 0.6363\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9895 - accuracy: 0.6149\n",
      "Epoch 33/50\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5274 - accuracy: 0.3720\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0433 - accuracy: 0.1553\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.5167 - accuracy: 0.3635Epoch 47/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9527 - accuracy: 0.6487\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.9295 - accuracy: 0.6453\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 2.0491 - accuracy: 0.1287Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4953 - accuracy: 0.3844\n",
      "29/46 [=================>............] - ETA: 0s - loss: 2.0459 - accuracy: 0.1304Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0430 - accuracy: 0.1373\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.9878 - accuracy: 0.6301\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9263 - accuracy: 0.6480\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 2.0385 - accuracy: 0.1406Epoch 46/50\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4924 - accuracy: 0.4023\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.9270 - accuracy: 0.7500Epoch 22/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 2.0425 - accuracy: 0.1373\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.4516 - accuracy: 0.4103Epoch 49/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4574 - accuracy: 0.3948\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9662 - accuracy: 0.6287\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9260 - accuracy: 0.6570\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 2.0463 - accuracy: 0.1562Epoch 36/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0431 - accuracy: 0.1601\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.4697 - accuracy: 0.4052Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9388 - accuracy: 0.6591\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4608 - accuracy: 0.4120\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 2.0503 - accuracy: 0.1585Epoch 24/50\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.8885 - accuracy: 0.6570\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 0.8768 - accuracy: 0.6562Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 2.0424 - accuracy: 0.1477\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4286 - accuracy: 0.4375\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9254 - accuracy: 0.6487\n",
      "Epoch 25/50\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8943 - accuracy: 0.6646\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 0.9363 - accuracy: 0.6438Epoch 38/50\n",
      "12/12 [==============================] - 0s 12ms/stepss: 0.9178 - accuracy: 0.65\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 0.9070 - accuracy: 0.6689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:09:18,602] Trial 13 finished with value: 0.1790633608815427 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'sgd'}. Best is trial 4 with value: 0.5234159779614325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9150 - accuracy: 0.6653\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4092 - accuracy: 0.4472\n",
      "Epoch 50/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.0133 - accuracy: 0.5312Epoch 26/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9090 - accuracy: 0.6508\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.4981 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.9210 - accuracy: 0.6363Epoch 1/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9136 - accuracy: 0.6632\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9142 - accuracy: 0.6363\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4070 - accuracy: 0.4369\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.9031 - accuracy: 0.6570\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3821 - accuracy: 0.4424\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.8873 - accuracy: 0.6653\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.3656 - accuracy: 0.4617\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 0.8012 - accuracy: 0.7098Epoch 29/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.8545 - accuracy: 0.6853\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.3493 - accuracy: 0.4583Epoch 43/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.3458 - accuracy: 0.4596\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 0.7911 - accuracy: 0.7009Epoch 30/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.8577 - accuracy: 0.6687\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3384 - accuracy: 0.4589\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 0.8595 - accuracy: 0.6875Epoch 31/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.8616 - accuracy: 0.6839\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3349 - accuracy: 0.4693\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 0.7880 - accuracy: 0.7098Epoch 32/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.7746 - accuracy: 0.7074\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.3200 - accuracy: 0.4665\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.8177 - accuracy: 0.6922\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.3137 - accuracy: 0.4762\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.7674 - accuracy: 0.7184\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2990 - accuracy: 0.4824\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.7543 - accuracy: 0.7302\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2821 - accuracy: 0.4934\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.7265 - accuracy: 0.7502\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.2996 - accuracy: 0.4651\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.7472 - accuracy: 0.7226\n",
      "46/46 [==============================] - 6s 9ms/step - loss: 2.1276 - accuracy: 0.1463\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2727 - accuracy: 0.5052\n",
      " 1/46 [..............................] - ETA: 0s - loss: 2.1375 - accuracy: 0.0312Epoch 38/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0807 - accuracy: 0.1415\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 7ms/steposs: 1.9901 - accuracy: 0.\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2667 - accuracy: 0.4921\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 2.0642 - accuracy: 0.1750Epoch 39/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.2140 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:09:25,426] Trial 14 finished with value: 0.5840220385674931 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/46 [=================>............] - ETA: 0s - loss: 2.0733 - accuracy: 0.1552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0596 - accuracy: 0.1705\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.2613 - accuracy: 0.5046Epoch 4/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2649 - accuracy: 0.4983\n",
      "Epoch 40/50\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 2.0538 - accuracy: 0.1797Epoch 1/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 2.0486 - accuracy: 0.1539\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2339 - accuracy: 0.5024\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0423 - accuracy: 0.1394\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2466 - accuracy: 0.5079\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 2.0389 - accuracy: 0.1401\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2498 - accuracy: 0.5017\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0362 - accuracy: 0.1567\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2245 - accuracy: 0.5114\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0341 - accuracy: 0.1511\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2305 - accuracy: 0.5066\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 2.0282 - accuracy: 0.1589Epoch 45/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0302 - accuracy: 0.1608\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.2093 - accuracy: 0.4952Epoch 10/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2221 - accuracy: 0.5010\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 2.0223 - accuracy: 0.1966Epoch 46/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 2.0256 - accuracy: 0.1919\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.2051 - accuracy: 0.4805Epoch 11/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2191 - accuracy: 0.5059\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 2.0204 - accuracy: 0.1677\n",
      "Epoch 47/50\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 2.0130 - accuracy: 0.2333\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 8ms/steposs: 2.0119 - accuracy: 0.27\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2210 - accuracy: 0.5114\n",
      "Epoch 48/50\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.1746 - accuracy: 0.5496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:09:30,143] Trial 12 finished with value: 0.5371900826446281 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 50, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/46 [============================>.] - ETA: 0s - loss: 2.0041 - accuracy: 0.2743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 11ms/step - loss: 2.0038 - accuracy: 0.2733\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.1907 - accuracy: 0.5232Epoch 14/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1959 - accuracy: 0.5197\n",
      "Epoch 49/50\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 2.0646 - accuracy: 0.1683Epoch 1/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9920 - accuracy: 0.2919\n",
      "20/46 [============>.................] - ETA: 0s - loss: 2.0252 - accuracy: 0.1828Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1995 - accuracy: 0.5307\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.9486 - accuracy: 0.2228Epoch 50/50\n",
      "46/46 [==============================] - 5s 13ms/step - loss: 1.9196 - accuracy: 0.2360\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9773 - accuracy: 0.2850\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.2050 - accuracy: 0.5089Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2106 - accuracy: 0.5100\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6631 - accuracy: 0.3423\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.9588 - accuracy: 0.2940Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9608 - accuracy: 0.2830\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5764 - accuracy: 0.3651\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 12ms/stepss: 1.4739 - accuracy: 0.31\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.9462 - accuracy: 0.2863"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:09:32,720] Trial 15 finished with value: 0.4903581267217631 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9409 - accuracy: 0.2864\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.4726 - accuracy: 0.4080Epoch 18/50\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.4680 - accuracy: 0.4130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4783 - accuracy: 0.4092\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9208 - accuracy: 0.2809\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.3841 - accuracy: 0.4485Epoch 19/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.9023 - accuracy: 0.3438Epoch 1/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4067 - accuracy: 0.4327\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.9150 - accuracy: 0.2822Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9008 - accuracy: 0.2885\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3886 - accuracy: 0.4458\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8814 - accuracy: 0.2892\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3699 - accuracy: 0.4651\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8606 - accuracy: 0.2871\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.2781 - accuracy: 0.4928\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.8433 - accuracy: 0.2871\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2977 - accuracy: 0.4824\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8285 - accuracy: 0.2830\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2590 - accuracy: 0.5183\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8148 - accuracy: 0.2843\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2091 - accuracy: 0.5342\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8022 - accuracy: 0.2926\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 8s 16ms/step - loss: 2.0313 - accuracy: 0.2284\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.2166 - accuracy: 0.5148\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.7913 - accuracy: 0.2947\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9479 - accuracy: 0.2381\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.1735 - accuracy: 0.5516Epoch 3/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7810 - accuracy: 0.3030\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.9112 - accuracy: 0.2946Epoch 28/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1883 - accuracy: 0.5335\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.8974 - accuracy: 0.2864\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.7670 - accuracy: 0.2942Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7700 - accuracy: 0.2981\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1830 - accuracy: 0.5293\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8331 - accuracy: 0.2926\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.7561 - accuracy: 0.2993Epoch 5/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.7597 - accuracy: 0.3043\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1268 - accuracy: 0.5590\n",
      "27/46 [================>.............] - ETA: 0s - loss: 2.0647 - accuracy: 0.1424Epoch 16/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7637 - accuracy: 0.3389\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 8s 14ms/step - loss: 2.0602 - accuracy: 0.1346\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.7549 - accuracy: 0.3016Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7538 - accuracy: 0.3030\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.6891 - accuracy: 0.3728Epoch 31/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7009 - accuracy: 0.3527\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2200 - accuracy: 0.5128\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.7375 - accuracy: 0.3212Epoch 17/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9774 - accuracy: 0.2105\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.6571 - accuracy: 0.3650Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7513 - accuracy: 0.2981\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.6477 - accuracy: 0.3741Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6533 - accuracy: 0.3678\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.9113 - accuracy: 0.2688Epoch 8/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1553 - accuracy: 0.5376\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.5944 - accuracy: 0.4125Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7429 - accuracy: 0.3092\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9043 - accuracy: 0.2547\n",
      "Epoch 33/50\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6142 - accuracy: 0.3768\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1213 - accuracy: 0.5631\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.7187 - accuracy: 0.3191Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8444 - accuracy: 0.2816\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.1330 - accuracy: 0.5568Epoch 5/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7385 - accuracy: 0.3092\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.7471 - accuracy: 0.3750Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5673 - accuracy: 0.4072\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.0793 - accuracy: 0.5752Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0977 - accuracy: 0.5666\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.7359 - accuracy: 0.3234Epoch 20/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7319 - accuracy: 0.3216\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.5295 - accuracy: 0.4014Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7818 - accuracy: 0.3037\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5366 - accuracy: 0.4058\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0991 - accuracy: 0.5832\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7272 - accuracy: 0.3064\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7307 - accuracy: 0.3430\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4963 - accuracy: 0.4237\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.6953 - accuracy: 0.3125Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0807 - accuracy: 0.5797\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.6837 - accuracy: 0.3510Epoch 22/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7229 - accuracy: 0.3037\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.6862 - accuracy: 0.3561Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6803 - accuracy: 0.3623\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4718 - accuracy: 0.4355\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.6341 - accuracy: 0.3802Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0362 - accuracy: 0.5977\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.4166 - accuracy: 0.4505Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7208 - accuracy: 0.3133\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 0.9799 - accuracy: 0.6250Epoch 38/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6493 - accuracy: 0.3713\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4508 - accuracy: 0.4341\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 11ms/step - loss: 1.7128 - accuracy: 0.3230\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.0417 - accuracy: 0.5930Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0387 - accuracy: 0.5990\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6072 - accuracy: 0.3747\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.4821 - accuracy: 0.4125Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4635 - accuracy: 0.4217\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.7112 - accuracy: 0.3147\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.0707 - accuracy: 0.6042Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0733 - accuracy: 0.6011\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.6804 - accuracy: 0.3299Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5856 - accuracy: 0.3865\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4069 - accuracy: 0.4527\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.0161 - accuracy: 0.6055Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7081 - accuracy: 0.3161\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.4088 - accuracy: 0.4938Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5514 - accuracy: 0.3948\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0438 - accuracy: 0.5970\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.7202 - accuracy: 0.3125Epoch 26/50\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3989 - accuracy: 0.4486\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.5311 - accuracy: 0.3941Epoch 17/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.7032 - accuracy: 0.3126\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9964 - accuracy: 0.6032\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.5281 - accuracy: 0.4085Epoch 27/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5219 - accuracy: 0.4155\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 0.9467 - accuracy: 0.5938Epoch 13/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7006 - accuracy: 0.3237\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.3801 - accuracy: 0.4632Epoch 43/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3812 - accuracy: 0.4631\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0447 - accuracy: 0.6039\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.3326 - accuracy: 0.4625Epoch 28/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5101 - accuracy: 0.4203\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.0497 - accuracy: 0.5677Epoch 14/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3668 - accuracy: 0.4582\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.6995 - accuracy: 0.3208Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6998 - accuracy: 0.3202\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.3214 - accuracy: 0.5625Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0219 - accuracy: 0.5866\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.3691 - accuracy: 0.4766Epoch 29/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4999 - accuracy: 0.4182\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.7087 - accuracy: 0.3089Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3465 - accuracy: 0.4796\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6956 - accuracy: 0.3237\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.4641 - accuracy: 0.4253Epoch 45/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9762 - accuracy: 0.6253\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.3426 - accuracy: 0.4774Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4718 - accuracy: 0.4217\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.3231 - accuracy: 0.4779Epoch 16/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3298 - accuracy: 0.4783\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6906 - accuracy: 0.3278\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.4329 - accuracy: 0.4421Epoch 46/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.9619 - accuracy: 0.6259\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.6543 - accuracy: 0.3348Epoch 31/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4583 - accuracy: 0.4334\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.4043 - accuracy: 0.4423Epoch 17/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6918 - accuracy: 0.3188\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3683 - accuracy: 0.4548\n",
      "Epoch 47/50\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.9781 - accuracy: 0.6250Epoch 22/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.9641 - accuracy: 0.6322\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4470 - accuracy: 0.4320\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.3094 - accuracy: 0.4863Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3068 - accuracy: 0.4762\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.6855 - accuracy: 0.3212Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6867 - accuracy: 0.3202\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4470 - accuracy: 0.4396\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.0450 - accuracy: 0.5970Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0450 - accuracy: 0.5970\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.2764 - accuracy: 0.5025Epoch 33/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2989 - accuracy: 0.4879\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.6853 - accuracy: 0.3243Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6842 - accuracy: 0.3257\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.4455 - accuracy: 0.4326Epoch 49/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4269 - accuracy: 0.4410\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.6830 - accuracy: 0.3438Epoch 20/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9712 - accuracy: 0.6377\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2781 - accuracy: 0.4969\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6796 - accuracy: 0.3299\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.4314 - accuracy: 0.4515Epoch 50/50\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4289 - accuracy: 0.4465\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.6827 - accuracy: 0.3289Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9113 - accuracy: 0.6370\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.6794 - accuracy: 0.3327Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6783 - accuracy: 0.3251\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2926 - accuracy: 0.4983\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 0.9178 - accuracy: 0.6270Epoch 26/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4118 - accuracy: 0.4500\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.8962 - accuracy: 0.6443Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.8962 - accuracy: 0.6473\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2671 - accuracy: 0.4928\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 10ms/stepss: 0.9256 - accuracy: 0.\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4009 - accuracy: 0.4438\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.9150 - accuracy: 0.6517Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:09:54,691] Trial 16 finished with value: 0.38016528925619836 and parameters: {'activation': 'sigmoid', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9227 - accuracy: 0.6467\n",
      "Epoch 37/50\n",
      " 1/46 [..............................] - ETA: 1s - loss: 0.7227 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2711 - accuracy: 0.4941\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3988 - accuracy: 0.4548\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.8488 - accuracy: 0.6915Epoch 24/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.8699 - accuracy: 0.6825\n",
      "Epoch 38/50\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.4176 - accuracy: 0.4233Epoch 1/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2681 - accuracy: 0.4941\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 0.9800 - accuracy: 0.6317Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3772 - accuracy: 0.4658\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.2962 - accuracy: 0.4815Epoch 25/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.9338 - accuracy: 0.6487\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2839 - accuracy: 0.4865\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3912 - accuracy: 0.4589\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.8867 - accuracy: 0.6678Epoch 26/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.8778 - accuracy: 0.6646\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2663 - accuracy: 0.4845\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3826 - accuracy: 0.4624\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.8477 - accuracy: 0.6763\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2434 - accuracy: 0.5114\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3636 - accuracy: 0.4645\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.8658 - accuracy: 0.6708\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.3460 - accuracy: 0.4629Epoch 42/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2214 - accuracy: 0.5197\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3548 - accuracy: 0.4727\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8323 - accuracy: 0.6763\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.2321 - accuracy: 0.5135Epoch 43/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2338 - accuracy: 0.5128\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.3191 - accuracy: 0.4909Epoch 34/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3437 - accuracy: 0.4962\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.2121 - accuracy: 0.5119Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8463 - accuracy: 0.6722\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2320 - accuracy: 0.5079\n",
      "Epoch 44/50\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3307 - accuracy: 0.4803\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 0.7732 - accuracy: 0.7326Epoch 31/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2114 - accuracy: 0.5183\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.8179 - accuracy: 0.7019\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.2232 - accuracy: 0.5052Epoch 45/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3277 - accuracy: 0.4859\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 0.6987 - accuracy: 0.7285Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2119 - accuracy: 0.5155\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 0.7729 - accuracy: 0.6957\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3437 - accuracy: 0.4658\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1960 - accuracy: 0.5286\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.8555 - accuracy: 0.6719Epoch 38/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.8497 - accuracy: 0.6736\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.1988 - accuracy: 0.5104Epoch 47/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3143 - accuracy: 0.4962\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1859 - accuracy: 0.5259\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.8124 - accuracy: 0.7045Epoch 39/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.8122 - accuracy: 0.7032\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3091 - accuracy: 0.4934\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.1913 - accuracy: 0.5182Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2076 - accuracy: 0.5280\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 2.0794 - accuracy: 0.1448Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.8282 - accuracy: 0.6832\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 2.0715 - accuracy: 0.1414Epoch 49/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3149 - accuracy: 0.4997\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 8s 13ms/step - loss: 2.0698 - accuracy: 0.1387\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.2291 - accuracy: 0.5035Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2272 - accuracy: 0.5066\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3500 - accuracy: 0.4815Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.8015 - accuracy: 0.6936\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.3462 - accuracy: 0.4832Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3388 - accuracy: 0.4865\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.2226 - accuracy: 0.5087Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 2.0233 - accuracy: 0.1608\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.1365 - accuracy: 0.5312Epoch 3/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1807 - accuracy: 0.5300\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.7681 - accuracy: 0.7012\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.9721 - accuracy: 0.2126\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.1335 - accuracy: 0.5469Epoch 4/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3068 - accuracy: 0.4852\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1739 - accuracy: 0.5245\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.9172 - accuracy: 0.2567Epoch 43/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9132 - accuracy: 0.2622\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2884 - accuracy: 0.5052\n",
      "Epoch 5/50\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 8ms/steposs: 1.1779 - accuracy: 0.55\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.1561 - accuracy: 0.5571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:05,086] Trial 17 finished with value: 0.5702479338842975 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1646 - accuracy: 0.5493\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.8526 - accuracy: 0.3018Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8524 - accuracy: 0.2974\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.1738 - accuracy: 0.5529Epoch 6/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2758 - accuracy: 0.5162\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.1826 - accuracy: 0.5460Epoch 40/50\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.1846 - accuracy: 0.5360Epoch 1/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1648 - accuracy: 0.5445\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8055 - accuracy: 0.2919\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.2617 - accuracy: 0.5128Epoch 7/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2602 - accuracy: 0.5135\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.1102 - accuracy: 0.5754Epoch 41/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1416 - accuracy: 0.5590\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7625 - accuracy: 0.3230\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2728 - accuracy: 0.5197\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1383 - accuracy: 0.5480\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.2594 - accuracy: 0.5160Epoch 47/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2586 - accuracy: 0.5148\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7254 - accuracy: 0.3340\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1464 - accuracy: 0.5514\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.7484 - accuracy: 0.3219Epoch 48/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2493 - accuracy: 0.5245\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7210 - accuracy: 0.3326\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1336 - accuracy: 0.5480\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.2399 - accuracy: 0.5298Epoch 49/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2415 - accuracy: 0.5286\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.6856 - accuracy: 0.3497Epoch 45/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6700 - accuracy: 0.3554\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1358 - accuracy: 0.5652\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2469 - accuracy: 0.5300\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6455 - accuracy: 0.3692\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1544 - accuracy: 0.5528\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2316 - accuracy: 0.5397\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6318 - accuracy: 0.3699\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 1s 11ms/stepss: 1.2266 - accuracy: 0.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2265 - accuracy: 0.5369\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.6288 - accuracy: 0.3615Epoch 48/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6265 - accuracy: 0.3630\n",
      "Epoch 14/50\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.1805 - accuracy: 0.5402"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:10,552] Trial 18 finished with value: 0.509641873278237 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/46 [========>.....................] - ETA: 0s - loss: 1.5776 - accuracy: 0.3884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2032 - accuracy: 0.5493\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.5780 - accuracy: 0.3882Epoch 49/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5804 - accuracy: 0.3858\n",
      "Epoch 15/50\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.2213 - accuracy: 0.5347Epoch 1/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5530 - accuracy: 0.4058\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2083 - accuracy: 0.5404\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5597 - accuracy: 0.4065\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2082 - accuracy: 0.5383\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5237 - accuracy: 0.4161\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 1s 12ms/stepss: 1.4849 - accuracy: 0.\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 2.0095 - accuracy: 0.2145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:13,747] Trial 19 finished with value: 0.4573002754820937 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 8s 20ms/step - loss: 2.0083 - accuracy: 0.2139\n",
      "Epoch 2/50\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.9346 - accuracy: 0.2062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 22ms/step - loss: 1.5003 - accuracy: 0.4293\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.9232 - accuracy: 0.2429\n",
      "Epoch 3/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.8581 - accuracy: 0.3125Epoch 1/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4801 - accuracy: 0.4182\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.8425 - accuracy: 0.2553\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 21ms/step - loss: 1.4657 - accuracy: 0.4389\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 20ms/step - loss: 1.7822 - accuracy: 0.3119\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.4456 - accuracy: 0.4424\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.7291 - accuracy: 0.3251\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 1.4299 - accuracy: 0.4472\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7004 - accuracy: 0.3326\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4300 - accuracy: 0.4362\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.6618 - accuracy: 0.3561\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.4122 - accuracy: 0.4534\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6317 - accuracy: 0.3602\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 2.1120 - accuracy: 0.1417Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3843 - accuracy: 0.4617\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 8s 14ms/step - loss: 2.0674 - accuracy: 0.1484\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6105 - accuracy: 0.3823\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 2.0077 - accuracy: 0.2261Epoch 10/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3640 - accuracy: 0.4776\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.6069 - accuracy: 0.3938Epoch 27/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9895 - accuracy: 0.2298\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.3627 - accuracy: 0.4932Epoch 3/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5849 - accuracy: 0.3885\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3635 - accuracy: 0.4700\n",
      "Epoch 11/50\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8993 - accuracy: 0.2802\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5618 - accuracy: 0.3885\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3354 - accuracy: 0.4914\n",
      "Epoch 12/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.4812 - accuracy: 0.4062Epoch 29/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.8226 - accuracy: 0.3002\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.3115 - accuracy: 0.5199Epoch 5/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.5418 - accuracy: 0.3996\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.7528 - accuracy: 0.3450Epoch 13/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.3294 - accuracy: 0.5052\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.6533 - accuracy: 0.2500Epoch 30/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7439 - accuracy: 0.3313\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5127 - accuracy: 0.4134\n",
      "27/46 [================>.............] - ETA: 0s - loss: 2.0455 - accuracy: 0.1771Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3149 - accuracy: 0.4934\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.4861 - accuracy: 0.4297Epoch 31/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6990 - accuracy: 0.3444\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.3398 - accuracy: 0.4948Epoch 7/50\n",
      "46/46 [==============================] - 8s 13ms/step - loss: 2.0378 - accuracy: 0.1836\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.3612 - accuracy: 0.4729Epoch 2/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4875 - accuracy: 0.4244\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3178 - accuracy: 0.4907\n",
      "Epoch 15/50\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6590 - accuracy: 0.3582\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2423 - accuracy: 0.5312Epoch 8/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9831 - accuracy: 0.2471\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4723 - accuracy: 0.4231\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.2888 - accuracy: 0.5097Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2932 - accuracy: 0.5086\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6313 - accuracy: 0.3554\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9016 - accuracy: 0.2747\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.4522 - accuracy: 0.4328Epoch 4/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4551 - accuracy: 0.4375\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.6075 - accuracy: 0.3683Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2855 - accuracy: 0.5038\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.8262 - accuracy: 0.2773Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6008 - accuracy: 0.3720\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.4002 - accuracy: 0.4141Epoch 10/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.8172 - accuracy: 0.2919\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4227 - accuracy: 0.4431\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.7605 - accuracy: 0.2823Epoch 18/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2729 - accuracy: 0.5107\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.7600 - accuracy: 0.2860Epoch 35/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5865 - accuracy: 0.3644\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.1713 - accuracy: 0.5938Epoch 11/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.7577 - accuracy: 0.2940\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.5589 - accuracy: 0.4000Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2559 - accuracy: 0.5162\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4123 - accuracy: 0.4562\n",
      "Epoch 36/50\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.7024 - accuracy: 0.3107Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5451 - accuracy: 0.3920\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.3604 - accuracy: 0.5703Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7073 - accuracy: 0.3168\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.3851 - accuracy: 0.4911Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2469 - accuracy: 0.5162\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.5514 - accuracy: 0.3767Epoch 37/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3948 - accuracy: 0.4596\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.6647 - accuracy: 0.3370Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5492 - accuracy: 0.3734\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.4406 - accuracy: 0.4219Epoch 13/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6624 - accuracy: 0.3368\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.7983 - accuracy: 0.4062Epoch 8/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2780 - accuracy: 0.5052\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3872 - accuracy: 0.4665\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6411 - accuracy: 0.3527\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.3114 - accuracy: 0.4639Epoch 9/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5288 - accuracy: 0.3982\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2626 - accuracy: 0.5059\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.6300 - accuracy: 0.3468Epoch 39/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6240 - accuracy: 0.3499\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3610 - accuracy: 0.4686\n",
      "Epoch 10/50\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.3328 - accuracy: 0.4609Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5067 - accuracy: 0.4196\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2639 - accuracy: 0.5086\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.6106 - accuracy: 0.3405Epoch 40/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3518 - accuracy: 0.4707\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6168 - accuracy: 0.3368\n",
      "Epoch 23/50\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4967 - accuracy: 0.4141\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2472 - accuracy: 0.5210\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3317 - accuracy: 0.4907\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.4610 - accuracy: 0.4304Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5893 - accuracy: 0.3734\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.1856 - accuracy: 0.5312Epoch 12/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4747 - accuracy: 0.4237\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3131 - accuracy: 0.4990\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5527 - accuracy: 0.3968\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2074 - accuracy: 0.5300\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.4893 - accuracy: 0.4089Epoch 13/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1870 - accuracy: 0.6250Epoch 42/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4771 - accuracy: 0.4155\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.5179 - accuracy: 0.4224\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.2036 - accuracy: 0.5327Epoch 14/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3178 - accuracy: 0.4831\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2034 - accuracy: 0.5335\n",
      "Epoch 43/50\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.4493 - accuracy: 0.4327\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5022 - accuracy: 0.4196\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.2064 - accuracy: 0.5278Epoch 15/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2084 - accuracy: 0.5273\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.2924 - accuracy: 0.5000Epoch 44/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3015 - accuracy: 0.4941\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.4896 - accuracy: 0.3646Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4418 - accuracy: 0.4472\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2146 - accuracy: 0.5286\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4744 - accuracy: 0.4279\n",
      "Epoch 45/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.2814 - accuracy: 0.4920Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2879 - accuracy: 0.4872\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.2744 - accuracy: 0.5069Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4666 - accuracy: 0.4431\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.4311 - accuracy: 0.4688Epoch 21/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2247 - accuracy: 0.5362\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.3077 - accuracy: 0.4944Epoch 46/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4520 - accuracy: 0.4465\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3052 - accuracy: 0.4955\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.4371 - accuracy: 0.4375Epoch 29/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4128 - accuracy: 0.4617\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.4615 - accuracy: 0.4435Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2270 - accuracy: 0.5190\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4610 - accuracy: 0.4313\n",
      "Epoch 47/50\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2711 - accuracy: 0.5010\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.3981 - accuracy: 0.4671Epoch 30/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3935 - accuracy: 0.4610\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1847 - accuracy: 0.5424\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4256 - accuracy: 0.4651\n",
      "Epoch 48/50\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2825 - accuracy: 0.5038\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.3836 - accuracy: 0.4719Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3939 - accuracy: 0.4700\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.3372 - accuracy: 0.4777Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1914 - accuracy: 0.5445\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.3710 - accuracy: 0.4676Epoch 49/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4123 - accuracy: 0.4472\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.3685 - accuracy: 0.4716Epoch 20/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2462 - accuracy: 0.5210\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.3657 - accuracy: 0.4420Epoch 32/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3848 - accuracy: 0.4679\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.2018 - accuracy: 0.5369Epoch 25/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1870 - accuracy: 0.5355\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.2389 - accuracy: 0.5134Epoch 50/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3828 - accuracy: 0.4645\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2417 - accuracy: 0.5162\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.1205 - accuracy: 0.5660Epoch 33/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3646 - accuracy: 0.4748\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1720 - accuracy: 0.5459\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3676 - accuracy: 0.4658\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2589 - accuracy: 0.4976\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3683 - accuracy: 0.4762\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3381 - accuracy: 0.4803\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.3413 - accuracy: 0.4777Epoch 23/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3403 - accuracy: 0.4755\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2682 - accuracy: 0.5100\n",
      "Epoch 28/50\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3366 - accuracy: 0.4831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/46 [====================>.........] - ETA: 0s - loss: 1.2156 - accuracy: 0.5265Epoch 24/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2168 - accuracy: 0.5321\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.3477 - accuracy: 0.4852\n",
      "Epoch 36/50\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3287 - accuracy: 0.4955\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.3014 - accuracy: 0.5201Epoch 25/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3347 - accuracy: 0.5059\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.2154 - accuracy: 0.5300Epoch 30/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2154 - accuracy: 0.5300\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1984 - accuracy: 0.4375Epoch 37/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.3189 - accuracy: 0.4865\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3287 - accuracy: 0.4969\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.2105 - accuracy: 0.5476Epoch 31/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2184 - accuracy: 0.5397\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2930 - accuracy: 0.4948\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.3223 - accuracy: 0.4942Epoch 27/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.3251 - accuracy: 0.5010\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2108 - accuracy: 0.5280\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2789 - accuracy: 0.5086\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.2983 - accuracy: 0.4997\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.1977 - accuracy: 0.5437Epoch 33/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2009 - accuracy: 0.5404\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2696 - accuracy: 0.5190\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2904 - accuracy: 0.4983\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1889 - accuracy: 0.5314\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2655 - accuracy: 0.5072\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.2689 - accuracy: 0.5192Epoch 30/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2824 - accuracy: 0.5107\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.1842 - accuracy: 0.5275Epoch 35/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1872 - accuracy: 0.5259\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2596 - accuracy: 0.4997\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2742 - accuracy: 0.5100\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1921 - accuracy: 0.5390\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 7ms/step - loss: 1.2804 - accuracy: 0.5045\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2805 - accuracy: 0.5141\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1754 - accuracy: 0.5355\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2500 - accuracy: 0.5176\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2843 - accuracy: 0.5066\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1816 - accuracy: 0.5397\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2256 - accuracy: 0.5307\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2505 - accuracy: 0.5273\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.1840 - accuracy: 0.5437Epoch 39/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1855 - accuracy: 0.5376\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2278 - accuracy: 0.5190\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2493 - accuracy: 0.5355\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1802 - accuracy: 0.5418\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2528 - accuracy: 0.4997\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.1502 - accuracy: 0.5708Epoch 36/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2333 - accuracy: 0.5390\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1684 - accuracy: 0.5424\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.2437 - accuracy: 0.5231Epoch 48/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2316 - accuracy: 0.5286\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.1617 - accuracy: 0.5422Epoch 37/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2439 - accuracy: 0.5307\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.1985 - accuracy: 0.5372Epoch 42/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1583 - accuracy: 0.5383\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.2147 - accuracy: 0.5273\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1579 - accuracy: 0.5459\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2683 - accuracy: 0.5155\n",
      "Epoch 43/50\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 1.1864 - accuracy: 0.5335\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1563 - accuracy: 0.5576\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2188 - accuracy: 0.5300\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.1933 - accuracy: 0.5576Epoch 44/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.1856 - accuracy: 0.5535\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2129 - accuracy: 0.5259\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 7ms/steposs: 1.1903 - accuracy: 0.\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.1926 - accuracy: 0.5521\n",
      "Epoch 41/50\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.1856 - accuracy: 0.5538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:43,107] Trial 21 finished with value: 0.509641873278237 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/46 [============>.................] - ETA: 0s - loss: 1.1576 - accuracy: 0.5375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2066 - accuracy: 0.5438\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1879 - accuracy: 0.5397\n",
      "Epoch 42/50\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.1971 - accuracy: 0.5487Epoch 1/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1973 - accuracy: 0.5487\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1716 - accuracy: 0.5487\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 1s 12ms/stepss: 1.1120 - accuracy: 0.63\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.1492 - accuracy: 0.5692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:44,372] Trial 20 finished with value: 0.48760330578512395 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2470 - accuracy: 0.5335\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.1703 - accuracy: 0.5567Epoch 48/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1578 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1719 - accuracy: 0.5500\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.1748 - accuracy: 0.5503Epoch 44/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1929 - accuracy: 0.5314\n",
      "Epoch 1/50\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1663 - accuracy: 0.5418\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2245 - accuracy: 0.5335\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1439 - accuracy: 0.5638\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1901 - accuracy: 0.5438\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1505 - accuracy: 0.5521\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 10ms/stepss: 1.1430 - accuracy: 0.\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.1332 - accuracy: 0.5489"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:47,506] Trial 22 finished with value: 0.4573002754820937 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1426 - accuracy: 0.5514\n",
      "Epoch 48/50\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.3153 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1471 - accuracy: 0.5487\n",
      "Epoch 49/50\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.0890 - accuracy: 0.5742Epoch 1/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1626 - accuracy: 0.5528\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1546 - accuracy: 0.5507\n",
      "12/12 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:10:50,334] Trial 23 finished with value: 0.4820936639118457 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/46 [========>.....................] - ETA: 0s - loss: 2.1018 - accuracy: 0.1292Epoch 1/50\n",
      "46/46 [==============================] - 8s 9ms/step - loss: 2.0565 - accuracy: 0.1511\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9874 - accuracy: 0.2326\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 7s 11ms/step - loss: 2.0463 - accuracy: 0.1746\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9137 - accuracy: 0.2692\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.9535 - accuracy: 0.2616\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8268 - accuracy: 0.2878\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.8578 - accuracy: 0.2774\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7723 - accuracy: 0.3154\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.7970 - accuracy: 0.2947\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.7428 - accuracy: 0.2911Epoch 5/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.7334 - accuracy: 0.3023\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7326 - accuracy: 0.3175\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.6928 - accuracy: 0.3163Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6965 - accuracy: 0.3154\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 2.0874 - accuracy: 0.1587Epoch 8/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6932 - accuracy: 0.3306\n",
      "46/46 [==============================] - 7s 13ms/step - loss: 2.0591 - accuracy: 0.1808\n",
      "Epoch 7/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.5916 - accuracy: 0.3750Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6650 - accuracy: 0.3375\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 2.0010 - accuracy: 0.2104Epoch 9/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6665 - accuracy: 0.3271\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.9807 - accuracy: 0.2119Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9807 - accuracy: 0.2119\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.8227 - accuracy: 0.2812Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6298 - accuracy: 0.3354\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.9105 - accuracy: 0.2668Epoch 10/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6398 - accuracy: 0.3471\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9008 - accuracy: 0.2622\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6195 - accuracy: 0.3547\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6236 - accuracy: 0.3506\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.8223 - accuracy: 0.2958Epoch 10/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8229 - accuracy: 0.2940\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.5837 - accuracy: 0.3782Epoch 5/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5840 - accuracy: 0.3671\n",
      " 1/46 [..............................] - ETA: 4:48 - loss: 2.1985 - accuracy: 0.1250Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6009 - accuracy: 0.3527\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.5910 - accuracy: 0.3727Epoch 11/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.7548 - accuracy: 0.3092\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.5827 - accuracy: 0.3823Epoch 6/50\n",
      "46/46 [==============================] - 7s 16ms/step - loss: 1.8633 - accuracy: 0.2650\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.5870 - accuracy: 0.3720Epoch 2/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5870 - accuracy: 0.3720\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.8817 - accuracy: 0.1562Epoch 13/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7102 - accuracy: 0.3257\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5736 - accuracy: 0.3768\n",
      "Epoch 7/50\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6088 - accuracy: 0.3623\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.5452 - accuracy: 0.3969Epoch 3/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5467 - accuracy: 0.3934\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.5377 - accuracy: 0.3800Epoch 14/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6773 - accuracy: 0.3368\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.5448 - accuracy: 0.4062Epoch 8/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5457 - accuracy: 0.3810\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.5198 - accuracy: 0.4141Epoch 13/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5078 - accuracy: 0.4217\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.6545 - accuracy: 0.3507Epoch 4/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.5536 - accuracy: 0.3941\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5463 - accuracy: 0.3768\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6600 - accuracy: 0.3368\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.4966 - accuracy: 0.4043Epoch 14/50\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4743 - accuracy: 0.4203\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5134 - accuracy: 0.4017\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.6374 - accuracy: 0.3394Epoch 5/50\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6324 - accuracy: 0.3485\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5108 - accuracy: 0.3961\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.4977 - accuracy: 0.4234Epoch 10/50\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5104 - accuracy: 0.4072\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 1.6022 - accuracy: 0.3546Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3643 - accuracy: 0.4651\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.4802 - accuracy: 0.4375Epoch 6/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6233 - accuracy: 0.3547\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4903 - accuracy: 0.4120\n",
      "Epoch 11/50\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4840 - accuracy: 0.4092\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3020 - accuracy: 0.4838\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.6336 - accuracy: 0.3262Epoch 7/50\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 1.4597 - accuracy: 0.4183Epoch 18/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4735 - accuracy: 0.4168\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6186 - accuracy: 0.3382\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.4728 - accuracy: 0.4076Epoch 12/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4760 - accuracy: 0.4148\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.5692 - accuracy: 0.3750Epoch 19/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.3182 - accuracy: 0.4810\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.4602 - accuracy: 0.3875Epoch 8/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4856 - accuracy: 0.4134\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.5816 - accuracy: 0.3640Epoch 18/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.5750 - accuracy: 0.3706\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.4496 - accuracy: 0.4102Epoch 13/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.4526 - accuracy: 0.4203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/46 [==============>...............] - ETA: 0s - loss: 1.5508 - accuracy: 0.3958Epoch 20/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.2543 - accuracy: 0.4990\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4314 - accuracy: 0.4327\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.4293 - accuracy: 0.4487Epoch 19/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5637 - accuracy: 0.3658\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.2636 - accuracy: 0.5022Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4305 - accuracy: 0.4396\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.4059 - accuracy: 0.4583Epoch 21/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2320 - accuracy: 0.5031\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.4044 - accuracy: 0.4599Epoch 10/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4173 - accuracy: 0.4486\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2419 - accuracy: 0.5188Epoch 20/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5397 - accuracy: 0.3837\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3527 - accuracy: 0.5000Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4293 - accuracy: 0.4355\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.5484 - accuracy: 0.3877Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2477 - accuracy: 0.4976\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.5227 - accuracy: 0.3970Epoch 11/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.5271 - accuracy: 0.3865\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.4115 - accuracy: 0.4521Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4111 - accuracy: 0.4513\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4182 - accuracy: 0.4431\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.3890 - accuracy: 0.4563Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2658 - accuracy: 0.5031\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5181 - accuracy: 0.3954\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.3917 - accuracy: 0.4553Epoch 17/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3947 - accuracy: 0.4534\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4290 - accuracy: 0.4293\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4950 - accuracy: 0.4099\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.2403 - accuracy: 0.5149Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2340 - accuracy: 0.5217\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3852 - accuracy: 0.4576\n",
      "Epoch 13/50\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.3899 - accuracy: 0.4256Epoch 23/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3791 - accuracy: 0.4534\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.2272 - accuracy: 0.5231Epoch 25/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4889 - accuracy: 0.4106\n",
      "12/46 [======>.......................] - ETA: 0s - loss: 1.3761 - accuracy: 0.4349Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2330 - accuracy: 0.5238\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3516 - accuracy: 0.4741\n",
      "Epoch 14/50\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.4761 - accuracy: 0.4375Epoch 24/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3831 - accuracy: 0.4493\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.3292 - accuracy: 0.4837Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4668 - accuracy: 0.4065\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.3843 - accuracy: 0.4729Epoch 20/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3341 - accuracy: 0.4776\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.5311 - accuracy: 0.3875Epoch 25/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2217 - accuracy: 0.5121\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.5159 - accuracy: 0.4034Epoch 15/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3714 - accuracy: 0.4576\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.4583 - accuracy: 0.4128Epoch 27/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.4582 - accuracy: 0.4189\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3236 - accuracy: 0.4845\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.3417 - accuracy: 0.4645Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2030 - accuracy: 0.5204\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.4030 - accuracy: 0.4668Epoch 16/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3522 - accuracy: 0.4603\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4255 - accuracy: 0.4451\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3056 - accuracy: 0.4886\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1563 - accuracy: 0.5645\n",
      "Epoch 27/50\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3636 - accuracy: 0.4617\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4094 - accuracy: 0.4424\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2966 - accuracy: 0.4859\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0861 - accuracy: 0.5714\n",
      "Epoch 28/50\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3535 - accuracy: 0.4576\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4162 - accuracy: 0.4258\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.3149 - accuracy: 0.4688Epoch 24/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1246 - accuracy: 0.5569\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2975 - accuracy: 0.4824\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.3240 - accuracy: 0.4768Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3246 - accuracy: 0.4651\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 1.2853 - accuracy: 0.4773Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3887 - accuracy: 0.4548\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 1.2869 - accuracy: 0.4934Epoch 25/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2810 - accuracy: 0.4969\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0815 - accuracy: 0.5728\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1273 - accuracy: 0.5625Epoch 20/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3261 - accuracy: 0.4672\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.0685 - accuracy: 0.5859Epoch 32/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.3570 - accuracy: 0.4672\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2953 - accuracy: 0.4872\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.3760 - accuracy: 0.4355Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0975 - accuracy: 0.5694\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3383 - accuracy: 0.5312Epoch 21/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3156 - accuracy: 0.4762\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3521 - accuracy: 0.4762\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.3172 - accuracy: 0.4708Epoch 27/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2600 - accuracy: 0.5031\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0430 - accuracy: 0.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3136 - accuracy: 0.4762\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.2285 - accuracy: 0.5063Epoch 34/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3377 - accuracy: 0.4741\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.0076 - accuracy: 0.6283Epoch 28/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2752 - accuracy: 0.4879\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.2945 - accuracy: 0.4865Epoch 33/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3056 - accuracy: 0.4838\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.0123 - accuracy: 0.6135Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0123 - accuracy: 0.6135\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3140 - accuracy: 0.4845\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.2711 - accuracy: 0.5046Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2430 - accuracy: 0.5072\n",
      " 7/46 [===>..........................] - ETA: 0s - loss: 1.3744 - accuracy: 0.4732Epoch 34/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2870 - accuracy: 0.4900\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0106 - accuracy: 0.6101\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.3131 - accuracy: 0.4929Epoch 24/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.3162 - accuracy: 0.4859\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2398 - accuracy: 0.5128\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2716 - accuracy: 0.5052\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.3009 - accuracy: 0.5000Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0194 - accuracy: 0.5845\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3116 - accuracy: 0.4865\n",
      "45/46 [============================>.] - ETA: 0s - loss: 1.2441 - accuracy: 0.5049Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2421 - accuracy: 0.5059\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2685 - accuracy: 0.5010\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.0519 - accuracy: 0.5859\n",
      "Epoch 38/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.2613 - accuracy: 0.5000Epoch 26/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2970 - accuracy: 0.4997\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2312 - accuracy: 0.5017\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.0181 - accuracy: 0.6055Epoch 37/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2515 - accuracy: 0.5072\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.0362 - accuracy: 0.5970Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0345 - accuracy: 0.6011\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2309 - accuracy: 0.5079\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.2776 - accuracy: 0.4984Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2855 - accuracy: 0.5031\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1192 - accuracy: 0.6562Epoch 33/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2796 - accuracy: 0.4948\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.0173 - accuracy: 0.5977Epoch 40/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0162 - accuracy: 0.6018\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2115 - accuracy: 0.5231\n",
      "21/46 [============>.................] - ETA: 0s - loss: 0.9563 - accuracy: 0.6280Epoch 39/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2742 - accuracy: 0.5197\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.2091 - accuracy: 0.5451Epoch 34/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2553 - accuracy: 0.5031\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9626 - accuracy: 0.6239\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.2083 - accuracy: 0.5226Epoch 29/50\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2553 - accuracy: 0.5148\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1982 - accuracy: 0.5314\n",
      "Epoch 42/50\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.2845 - accuracy: 0.5115Epoch 40/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2863 - accuracy: 0.5031\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.9782 - accuracy: 0.6297Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9821 - accuracy: 0.6211\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.2519 - accuracy: 0.5078Epoch 30/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2268 - accuracy: 0.5183\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2003 - accuracy: 0.5383\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2473 - accuracy: 0.5176\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.1679 - accuracy: 0.5156Epoch 36/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.0222 - accuracy: 0.6032\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1907 - accuracy: 0.5328\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2116 - accuracy: 0.5259\n",
      "Epoch 42/50\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2696 - accuracy: 0.5176\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.0812 - accuracy: 0.5938Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9471 - accuracy: 0.6315\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.2120 - accuracy: 0.5194Epoch 32/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1765 - accuracy: 0.5355\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.2406 - accuracy: 0.5169Epoch 43/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2406 - accuracy: 0.5169\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2012 - accuracy: 0.5231\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.4672 - accuracy: 0.3750Epoch 45/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9229 - accuracy: 0.6646\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1968 - accuracy: 0.5259\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2288 - accuracy: 0.5100\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1995 - accuracy: 0.5217\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.9608 - accuracy: 0.6301\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 1.1927 - accuracy: 0.5337Epoch 34/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1947 - accuracy: 0.5286\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2040 - accuracy: 0.5197\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.2539 - accuracy: 0.5063Epoch 45/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2279 - accuracy: 0.5190\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.9079 - accuracy: 0.6439\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 1.2244 - accuracy: 0.5196Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1880 - accuracy: 0.5397\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.1712 - accuracy: 0.5451Epoch 48/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1673 - accuracy: 0.5411\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.2174 - accuracy: 0.5233Epoch 46/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2186 - accuracy: 0.5217\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.2253 - accuracy: 0.5069Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1746 - accuracy: 0.5445\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8897 - accuracy: 0.6660\n",
      " 1/46 [..............................] - ETA: 1s - loss: 0.9812 - accuracy: 0.5938Epoch 36/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.1605 - accuracy: 0.5369\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.7731 - accuracy: 0.7188Epoch 47/50\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 1.2321 - accuracy: 0.5169\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 0.8963 - accuracy: 0.6750Epoch 42/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1625 - accuracy: 0.5445\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1616 - accuracy: 0.5404\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2199 - accuracy: 0.5252\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1672 - accuracy: 0.5938Epoch 48/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.9070 - accuracy: 0.6570\n",
      "Epoch 43/50\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3958 - accuracy: 0.4688Epoch 37/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1624 - accuracy: 0.5376\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2305 - accuracy: 0.5217\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.8978 - accuracy: 0.6542\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.1573 - accuracy: 0.5514Epoch 38/50\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1573 - accuracy: 0.5514\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2037 - accuracy: 0.5349\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1747 - accuracy: 0.5445\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.3069 - accuracy: 0.4375Epoch 50/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.8582 - accuracy: 0.6729\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 8ms/steposs: 1.1301 - accuracy: 0.54\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 1.2244 - accuracy: 0.5252\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:11:22,243] Trial 24 finished with value: 0.512396694214876 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 10ms/step - loss: 1.1514 - accuracy: 0.5438\n",
      "46/46 [==============================] - 0s 10ms/step - loss: 0.8551 - accuracy: 0.6715\n",
      "Epoch 40/50\n",
      "11/46 [======>.......................] - ETA: 0s - loss: 0.8388 - accuracy: 0.6932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1997 - accuracy: 0.5307\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.8406 - accuracy: 0.6874\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.0593 - accuracy: 0.4688Epoch 41/50\n",
      " 1/12 [=>............................] - ETA: 3s - loss: 0.8805 - accuracy: 0.6607Epoch 1/50\n",
      "12/12 [==============================] - 0s 11ms/stepss: 0.8214 - accuracy: 0.68\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 1.2088 - accuracy: 0.5450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:11:23,165] Trial 25 finished with value: 0.4628099173553719 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/46 [=========================>....] - ETA: 0s - loss: 1.1991 - accuracy: 0.5381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1912 - accuracy: 0.5418\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8056 - accuracy: 0.7067\n",
      "Epoch 48/50\n",
      "Epoch 42/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.1779 - accuracy: 0.5429Epoch 1/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.1776 - accuracy: 0.5452\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.8337 - accuracy: 0.6891Epoch 49/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.8412 - accuracy: 0.6853\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1776 - accuracy: 0.5459\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 0.8119 - accuracy: 0.6977\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1811 - accuracy: 0.5424\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.7843 - accuracy: 0.7081\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 10ms/stepss: 0.7647 - accuracy: 0.72\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.7775 - accuracy: 0.7130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:11:26,168] Trial 26 finished with value: 0.4765840220385675 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (10, 10, 10), 'optimizer': 'adam'}. Best is trial 14 with value: 0.5840220385674931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 14ms/step - loss: 0.7778 - accuracy: 0.7150\n",
      "Epoch 46/50\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 0.6399 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/46 [===========================>..] - ETA: 0s - loss: 0.7683 - accuracy: 0.7038Epoch 1/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 0.7610 - accuracy: 0.7074\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.7573 - accuracy: 0.7184\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 0.7311 - accuracy: 0.7122\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 0.7413 - accuracy: 0.7205\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 0.7070 - accuracy: 0.7371\n",
      "12/12 [==============================] - 0s 14ms/stepss: 2.0629 - accuracy: 0.\n",
      "28/46 [=================>............] - ETA: 0s - loss: 2.0575 - accuracy: 0.1451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-08 14:11:30,615] Trial 27 finished with value: 0.5922865013774105 and parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100, 100), 'optimizer': 'adam'}. Best is trial 27 with value: 0.5922865013774105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/46 [===========================>..] - ETA: 0s - loss: 2.0493 - accuracy: 0.1519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 100, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50, 50, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/djanloo/.local/share/virtualenvs/DM2-PeqFRmfa/lib/python3.10/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (10, 10) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 8s 15ms/step - loss: 2.0457 - accuracy: 0.1546\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9591 - accuracy: 0.2409\n",
      "Epoch 3/50\n",
      "Epoch 1/50\n",
      "46/46 [==============================] - 8s 15ms/step - loss: 2.1086 - accuracy: 0.1705\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8689 - accuracy: 0.3202\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.9803 - accuracy: 0.2774\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7634 - accuracy: 0.3257\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.8873 - accuracy: 0.2933\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.6947 - accuracy: 0.3326\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.8205 - accuracy: 0.2919\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6484 - accuracy: 0.3485\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.7678 - accuracy: 0.3202\n",
      "27/46 [================>.............] - ETA: 0s - loss: 2.0631 - accuracy: 0.1412Epoch 6/50\n",
      "46/46 [==============================] - 7s 13ms/step - loss: 2.0486 - accuracy: 0.1580\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.7373 - accuracy: 0.3125Epoch 2/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.6111 - accuracy: 0.3665\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.7360 - accuracy: 0.3168\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.9852 - accuracy: 0.2450\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.5943 - accuracy: 0.3658\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.6924 - accuracy: 0.3485\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.5776 - accuracy: 0.3667Epoch 8/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.9012 - accuracy: 0.2809\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.5512 - accuracy: 0.3815Epoch 4/50\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 1.5569 - accuracy: 0.3844\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 1.6580 - accuracy: 0.3368\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.8186 - accuracy: 0.3088Epoch 9/50\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 1.8195 - accuracy: 0.3009\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.5607 - accuracy: 0.3962Epoch 5/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.5486 - accuracy: 0.3865\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.7950 - accuracy: 0.2835Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6499 - accuracy: 0.3520\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 6s 15ms/step - loss: 1.9369 - accuracy: 0.2277\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.7625 - accuracy: 0.3057\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5296 - accuracy: 0.4003\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6309 - accuracy: 0.3623\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.6825 - accuracy: 0.3375\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.7341 - accuracy: 0.3410Epoch 3/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.6961 - accuracy: 0.3582\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.4946 - accuracy: 0.4175\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5914 - accuracy: 0.3699\n",
      "17/46 [==========>...................] - ETA: 0s - loss: 1.6563 - accuracy: 0.3676Epoch 12/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5424 - accuracy: 0.3927\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.6496 - accuracy: 0.3668Epoch 4/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.6587 - accuracy: 0.3499\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 1.5813 - accuracy: 0.3886Epoch 8/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.4603 - accuracy: 0.4286\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5828 - accuracy: 0.3734\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.4699 - accuracy: 0.4072\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.5935 - accuracy: 0.4000Epoch 5/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.6155 - accuracy: 0.3685\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.4494 - accuracy: 0.4334\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.5807 - accuracy: 0.4132Epoch 15/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5625 - accuracy: 0.3899\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.4252 - accuracy: 0.4485Epoch 14/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.4142 - accuracy: 0.4534\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.5077 - accuracy: 0.3906Epoch 6/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.5755 - accuracy: 0.3761\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.5017 - accuracy: 0.3988Epoch 10/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4299 - accuracy: 0.4493\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.3782 - accuracy: 0.4286Epoch 16/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5329 - accuracy: 0.3920\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.3641 - accuracy: 0.4902Epoch 15/50\n",
      "46/46 [==============================] - 1s 19ms/step - loss: 1.3281 - accuracy: 0.4810\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.4931 - accuracy: 0.4333Epoch 7/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.5504 - accuracy: 0.3975\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 1.4149 - accuracy: 0.4482Epoch 11/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4094 - accuracy: 0.4500\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.2955 - accuracy: 0.4868Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.5123 - accuracy: 0.4106\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.5095 - accuracy: 0.4271Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3296 - accuracy: 0.4693\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.4053 - accuracy: 0.4609Epoch 8/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.5175 - accuracy: 0.4196\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3988 - accuracy: 0.4520\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.5021 - accuracy: 0.4189\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.4757 - accuracy: 0.4253Epoch 17/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2922 - accuracy: 0.4962\n",
      "20/46 [============>.................] - ETA: 0s - loss: 1.4819 - accuracy: 0.4141Epoch 9/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.4720 - accuracy: 0.4417\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2962 - accuracy: 0.4688Epoch 13/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3862 - accuracy: 0.4596\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4863 - accuracy: 0.4113\n",
      "27/46 [================>.............] - ETA: 0s - loss: 1.2717 - accuracy: 0.5023Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2570 - accuracy: 0.5135\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.4582 - accuracy: 0.4319Epoch 10/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.4576 - accuracy: 0.4369\n",
      "21/46 [============>.................] - ETA: 0s - loss: 1.3157 - accuracy: 0.4866Epoch 14/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4709 - accuracy: 0.4244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3671 - accuracy: 0.4727\n",
      "Epoch 19/50\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2719 - accuracy: 0.5072\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4164 - accuracy: 0.4520\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3543 - accuracy: 0.4783\n",
      "Epoch 15/50\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 1.4620 - accuracy: 0.4208Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4600 - accuracy: 0.4224\n",
      " 3/46 [>.............................] - ETA: 1s - loss: 1.3275 - accuracy: 0.4896Epoch 20/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2324 - accuracy: 0.5252\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3306 - accuracy: 0.4921\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3929 - accuracy: 0.4610\n",
      "Epoch 22/50\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.4301 - accuracy: 0.4403Epoch 16/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4371 - accuracy: 0.4369\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2557 - accuracy: 0.5114\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.3864 - accuracy: 0.4752Epoch 13/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3827 - accuracy: 0.4727\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3163 - accuracy: 0.4872\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.2940 - accuracy: 0.5188Epoch 23/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.4472 - accuracy: 0.4362\n",
      "22/46 [=============>................] - ETA: 0s - loss: 1.2256 - accuracy: 0.5199Epoch 22/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2027 - accuracy: 0.5252\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 1.3068 - accuracy: 0.4865Epoch 14/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3036 - accuracy: 0.4907\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3462 - accuracy: 0.4783\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.1819 - accuracy: 0.5562Epoch 18/50\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4127 - accuracy: 0.4458\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.3827 - accuracy: 0.4250Epoch 23/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3459 - accuracy: 0.4831\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.2229 - accuracy: 0.5321\n",
      "Epoch 15/50\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3006 - accuracy: 0.5107\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.4021 - accuracy: 0.4527\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2890 - accuracy: 0.4688Epoch 24/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.3677 - accuracy: 0.4562\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1719 - accuracy: 0.5445\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.3163 - accuracy: 0.4992Epoch 20/50\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3185 - accuracy: 0.4983\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.4006 - accuracy: 0.4472\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.1353 - accuracy: 0.6094Epoch 25/50\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3070 - accuracy: 0.5052\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.3811 - accuracy: 0.4700\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.2761 - accuracy: 0.5045\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 1.1325 - accuracy: 0.5611Epoch 26/50\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.1423 - accuracy: 0.5590\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.4205 - accuracy: 0.4688Epoch 17/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2909 - accuracy: 0.4955\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 1.0655 - accuracy: 0.6024Epoch 22/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.3755 - accuracy: 0.4776\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.2548 - accuracy: 0.5141\n",
      "Epoch 27/50\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.0921 - accuracy: 0.5825\n",
      "13/46 [=======>......................] - ETA: 0s - loss: 1.2793 - accuracy: 0.5120Epoch 18/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2999 - accuracy: 0.4900\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.1090 - accuracy: 0.5713Epoch 23/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3446 - accuracy: 0.4810\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2465 - accuracy: 0.5259\n",
      "Epoch 28/50\n",
      " 8/46 [====>.........................] - ETA: 0s - loss: 1.3432 - accuracy: 0.4883Epoch 29/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1197 - accuracy: 0.5625\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2898 - accuracy: 0.5100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3485 - accuracy: 0.4727\n",
      "Epoch 24/50\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 1.2695 - accuracy: 0.5117Epoch 29/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2700 - accuracy: 0.5148\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.1528 - accuracy: 0.5569\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2678 - accuracy: 0.5072\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 1.3332 - accuracy: 0.4811Epoch 25/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.3275 - accuracy: 0.4803\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2697 - accuracy: 0.5072\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.2825 - accuracy: 0.5000Epoch 31/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0915 - accuracy: 0.5852\n",
      " 6/46 [==>...........................] - ETA: 0s - loss: 1.2037 - accuracy: 0.5417Epoch 21/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2569 - accuracy: 0.5066\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3282 - accuracy: 0.4769\n",
      "10/46 [=====>........................] - ETA: 0s - loss: 1.2599 - accuracy: 0.5219Epoch 31/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2187 - accuracy: 0.5390\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.1139 - accuracy: 0.5742\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.2656 - accuracy: 0.5145Epoch 22/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2689 - accuracy: 0.5072\n",
      "16/46 [=========>....................] - ETA: 0s - loss: 1.0193 - accuracy: 0.6133Epoch 27/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.3212 - accuracy: 0.4852\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 17ms/step - loss: 1.2175 - accuracy: 0.5224\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 1s 16ms/step - loss: 1.0655 - accuracy: 0.5928\n",
      " 1/46 [..............................] - ETA: 1s - loss: 1.1874 - accuracy: 0.6250Epoch 23/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2541 - accuracy: 0.5176\n",
      "28/46 [=================>............] - ETA: 0s - loss: 1.2983 - accuracy: 0.4978Epoch 28/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.3128 - accuracy: 0.4824\n",
      "29/46 [=================>............] - ETA: 0s - loss: 1.0856 - accuracy: 0.5765Epoch 33/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2181 - accuracy: 0.5307\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0684 - accuracy: 0.5901\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2520 - accuracy: 0.5210\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2855 - accuracy: 0.5121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/46 [==========>...................] - ETA: 0s - loss: 1.2561 - accuracy: 0.5174Epoch 34/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2203 - accuracy: 0.5259\n",
      "18/46 [==========>...................] - ETA: 0s - loss: 1.2402 - accuracy: 0.5243Epoch 35/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.0111 - accuracy: 0.6259\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.2694 - accuracy: 0.5082Epoch 25/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2689 - accuracy: 0.5079\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2807 - accuracy: 0.4976\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 1.1958 - accuracy: 0.5437Epoch 35/50\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 1.1989 - accuracy: 0.5383\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.0720 - accuracy: 0.5783\n",
      " 9/46 [====>.........................] - ETA: 0s - loss: 1.2194 - accuracy: 0.5278Epoch 26/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2295 - accuracy: 0.5224\n",
      "14/46 [========>.....................] - ETA: 0s - loss: 1.2179 - accuracy: 0.5156Epoch 31/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.2883 - accuracy: 0.5017\n",
      "15/46 [========>.....................] - ETA: 0s - loss: 1.1915 - accuracy: 0.5250Epoch 36/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2029 - accuracy: 0.5383\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 1.2102 - accuracy: 0.5202Epoch 37/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.0034 - accuracy: 0.6135\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.2180 - accuracy: 0.5148Epoch 27/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2180 - accuracy: 0.5148\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 1.2555 - accuracy: 0.5210\n",
      " 4/46 [=>............................] - ETA: 0s - loss: 1.3507 - accuracy: 0.4375Epoch 37/50\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 1.1854 - accuracy: 0.5493\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 1.2547 - accuracy: 0.5020Epoch 38/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2392 - accuracy: 0.5100\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 1.0255 - accuracy: 0.6004Epoch 33/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2484 - accuracy: 0.5059\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 1s 18ms/step - loss: 1.0207 - accuracy: 0.6011\n",
      " 5/46 [==>...........................] - ETA: 0s - loss: 1.2352 - accuracy: 0.5562Epoch 28/50\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 1.2028 - accuracy: 0.5418\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 1.2553 - accuracy: 0.5091Epoch 39/50\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 1.2249 - accuracy: 0.5383\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 0.9665 - accuracy: 0.6410Epoch 34/50\n",
      "19/46 [===========>..................] - ETA: 0s - loss: 1.1979 - accuracy: 0.5296"
     ]
    }
   ],
   "source": [
    "# objective function to be minimized\n",
    "import random\n",
    "import optuna\n",
    "def objective_fun(trial):\n",
    "    \n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes',[(100, 100, 100), (50, 50, 50), (10, 10, 10), \n",
    "                                  (100, 50, 10), (10, 10, 10), (10, 10)])\n",
    "#     learning_rate = trial.suggest_float('learning_rate', 0.001, 1)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "#     epochs = trial.suggest_int('epochs', 10, 200,10)\n",
    "    par = {'n_features_in_': X.shape[1], 'n_classes_': len(np.unique(y))}                                      \n",
    "                                          \n",
    "    net = KerasClassifier(model=build_model(par, hidden_layer_sizes, activation, optimizer ))\n",
    "    \n",
    "    net.fit(X_train, y_train, epochs=50)\n",
    "    y_pred = net.predict(X_val).astype(int)\n",
    "\n",
    "    error = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 50, n_jobs = -1, catch=(ValueError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "par = {'n_features_in_': X_shape[1], 'n_classes_': len(np.unique(y))}   \n",
    "\n",
    "net = KerasClassifier(model=build_model(par, **best_params), epochs=50)\n",
    "net.fit(X_train, y_train, \n",
    "        validation_split = 0.2)\n",
    "\n",
    "y_pred_test = net.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cf, annot=True, cmap='Greens')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('LinearSVC confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = net.history_\n",
    "plt.plot(history['loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_proba = net.predict_proba(X_test)\n",
    "plot_roc(y_test, y_test_pred_proba)\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()\n",
    "print(roc_auc_score(y_test, y_test_pred_proba[:, 1], multi_class='ovr', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries_on_embedding(reducer, net, embedding=embedding, \n",
    "                        n_pts=30,\n",
    "                       cmap=\"inferno\",\n",
    "                       titile=\"Neural network on PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fun(trial):\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 200, 2000, 10)\n",
    "    max_depth = trial.suggest_int('max_depth', 0, 100, 10)\n",
    "    max_features = trial.suggest_categorical('max_features',['auto', 'sqrt'])\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 1, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "#     epochs = trial.suggest_int('epochs', 10, 200,10)\n",
    "    par = {'n_features_in_': 231, 'n_classes_': 2}                                      \n",
    "                                          \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth , max_features=max_features, \\\n",
    "                               min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \\\n",
    "                               bootstrap=bootstrap, criterion=criterion)\n",
    "    rf.fit(X_res_t, y_res_t)\n",
    "    y_pred = rf.predict(X_res_v)\n",
    "    error = accuracy_score(y_res_v, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 50, n_jobs = -1, catch=(ValueError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "rf = RandomForestClassifier(**best_params)\n",
    "rf.fit(X_res_t, y_res_t)\n",
    "\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cf, annot=True, cmap='Greens')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('LinearSVC confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from scikitplot.metrics import plot_roc\n",
    "\n",
    "y_test_pred_proba = rf.predict_proba(X_test)\n",
    "plot_roc(y_test, y_test_pred_proba)\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()\n",
    "print(roc_auc_score(y_test, y_test_pred_proba[:, 1], multi_class='ovr', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_features = X_res_t.shape[1]\n",
    "\n",
    "tree_feature_importances = rf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()[::-1][:30]\n",
    "df2 = df.iloc[:, sorted_idx]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_idx))\n",
    "fig, ax = plt.subplots()\n",
    "plt.barh(y_ticks, tree_feature_importances[sorted_idx][::-1])\n",
    "plt.yticks(y_ticks, list(df2.columns)[::-1])\n",
    "plt.title(\"Random Forest Feature Importances (MDI)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = result.importances_mean.argsort()[::-1][:30]\n",
    "df2 = df.iloc[:, sorted_idx]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=list(df2.columns)[::-1])\n",
    "plt.title(\"Permutation Importances (test set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plot_tree(rf.estimators_[0], \n",
    "          feature_names=df.columns, \n",
    "          class_names=['speech', 'song'], \n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=12,\n",
    "          max_depth=2)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plot_tree(rf.estimators_[1], \n",
    "          feature_names=df.columns, \n",
    "          class_names=['speech', 'song'], \n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=12,\n",
    "          max_depth=2)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_fun(trial):\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 1, 20)\n",
    "    max_depth = trial.suggest_int('max_depth', 0, 100, 10)\n",
    "    max_features = trial.suggest_categorical('max_features',[\"log2\",\"sqrt\"])\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 1, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    criterion = trial.suggest_categorical('criterion', [\"friedman_mse\",  \"mae\"])\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1)\n",
    "    loss = trial.suggest_categorical('loss',['log_loss', 'deviance', 'exponential'])\n",
    "                                  \n",
    "                                          \n",
    "    gb = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth , max_features=max_features, \\\n",
    "                               min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \\\n",
    "                               learning_rate=learning_rate, criterion=criterion, subsample=subsample, loss=loss)\n",
    "    gb.fit(X_res_t, y_res_t)\n",
    "    y_pred = rf.predict(X_res_v)\n",
    "    error = accuracy_score(y_res_v, y_pred)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective_fun, n_trials = 100, n_jobs = -1, catch=(ValueError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "gb = GradientBoostingClassifier(**best_params)\n",
    "gb.fit(X_res_t, y_res_t)\n",
    "\n",
    "y_pred_test = gb.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred_test))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_proba = gb.predict_proba(X_test)\n",
    "plot_roc(y_test, y_test_pred_proba)\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()\n",
    "print(roc_auc_score(y_test, y_test_pred_proba[:, 1], multi_class='ovr', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "asdasda = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(asdasda, axis=0))\n",
    "print(np.max(asdasda, axis=0))\n",
    "\n",
    "u = pca.inverse_transform([[0.1, 0.1]])\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "dm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
